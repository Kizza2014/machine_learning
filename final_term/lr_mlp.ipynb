{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "HeartDiseaseorAttack",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "HighBP",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "HighChol",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "CholCheck",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "BMI",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Smoker",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Stroke",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Diabetes",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "PhysActivity",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Fruits",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Veggies",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "HvyAlcoholConsump",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "AnyHealthcare",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "NoDocbcCost",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "GenHlth",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "MentHlth",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "PhysHlth",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "DiffWalk",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Sex",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Age",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Education",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Income",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "150cc03c-ba77-449e-8315-78d478d6e218",
       "rows": [
        [
         "0",
         "0.0",
         "1.0",
         "1.0",
         "1.0",
         "40.0",
         "1.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "1.0",
         "0.0",
         "1.0",
         "0.0",
         "5.0",
         "18.0",
         "15.0",
         "1.0",
         "0.0",
         "9.0",
         "4.0",
         "3.0"
        ],
        [
         "1",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "25.0",
         "1.0",
         "0.0",
         "0.0",
         "1.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "1.0",
         "3.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "7.0",
         "6.0",
         "1.0"
        ],
        [
         "2",
         "0.0",
         "1.0",
         "1.0",
         "1.0",
         "28.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "1.0",
         "0.0",
         "0.0",
         "1.0",
         "1.0",
         "5.0",
         "30.0",
         "30.0",
         "1.0",
         "0.0",
         "9.0",
         "4.0",
         "8.0"
        ],
        [
         "3",
         "0.0",
         "1.0",
         "0.0",
         "1.0",
         "27.0",
         "0.0",
         "0.0",
         "0.0",
         "1.0",
         "1.0",
         "1.0",
         "0.0",
         "1.0",
         "0.0",
         "2.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "11.0",
         "3.0",
         "6.0"
        ],
        [
         "4",
         "0.0",
         "1.0",
         "1.0",
         "1.0",
         "24.0",
         "0.0",
         "0.0",
         "0.0",
         "1.0",
         "1.0",
         "1.0",
         "0.0",
         "1.0",
         "0.0",
         "2.0",
         "3.0",
         "0.0",
         "0.0",
         "0.0",
         "11.0",
         "5.0",
         "4.0"
        ],
        [
         "5",
         "0.0",
         "1.0",
         "1.0",
         "1.0",
         "25.0",
         "1.0",
         "0.0",
         "0.0",
         "1.0",
         "1.0",
         "1.0",
         "0.0",
         "1.0",
         "0.0",
         "2.0",
         "0.0",
         "2.0",
         "0.0",
         "1.0",
         "10.0",
         "6.0",
         "8.0"
        ],
        [
         "6",
         "0.0",
         "1.0",
         "0.0",
         "1.0",
         "30.0",
         "1.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "1.0",
         "0.0",
         "3.0",
         "0.0",
         "14.0",
         "0.0",
         "0.0",
         "9.0",
         "6.0",
         "7.0"
        ],
        [
         "7",
         "0.0",
         "1.0",
         "1.0",
         "1.0",
         "25.0",
         "1.0",
         "0.0",
         "0.0",
         "1.0",
         "0.0",
         "1.0",
         "0.0",
         "1.0",
         "0.0",
         "3.0",
         "0.0",
         "0.0",
         "1.0",
         "0.0",
         "11.0",
         "4.0",
         "4.0"
        ],
        [
         "8",
         "1.0",
         "1.0",
         "1.0",
         "1.0",
         "30.0",
         "1.0",
         "0.0",
         "2.0",
         "0.0",
         "1.0",
         "1.0",
         "0.0",
         "1.0",
         "0.0",
         "5.0",
         "30.0",
         "30.0",
         "1.0",
         "0.0",
         "9.0",
         "5.0",
         "1.0"
        ],
        [
         "9",
         "0.0",
         "0.0",
         "0.0",
         "1.0",
         "24.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "1.0",
         "0.0",
         "1.0",
         "0.0",
         "2.0",
         "0.0",
         "0.0",
         "0.0",
         "1.0",
         "8.0",
         "4.0",
         "3.0"
        ],
        [
         "10",
         "0.0",
         "0.0",
         "0.0",
         "1.0",
         "25.0",
         "1.0",
         "0.0",
         "2.0",
         "1.0",
         "1.0",
         "1.0",
         "0.0",
         "1.0",
         "0.0",
         "3.0",
         "0.0",
         "0.0",
         "0.0",
         "1.0",
         "13.0",
         "6.0",
         "8.0"
        ],
        [
         "11",
         "0.0",
         "1.0",
         "1.0",
         "1.0",
         "34.0",
         "1.0",
         "0.0",
         "0.0",
         "0.0",
         "1.0",
         "1.0",
         "0.0",
         "1.0",
         "0.0",
         "3.0",
         "0.0",
         "30.0",
         "1.0",
         "0.0",
         "10.0",
         "5.0",
         "1.0"
        ],
        [
         "12",
         "0.0",
         "0.0",
         "0.0",
         "1.0",
         "26.0",
         "1.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "1.0",
         "0.0",
         "1.0",
         "0.0",
         "3.0",
         "0.0",
         "15.0",
         "0.0",
         "0.0",
         "7.0",
         "5.0",
         "7.0"
        ],
        [
         "13",
         "0.0",
         "1.0",
         "1.0",
         "1.0",
         "28.0",
         "0.0",
         "0.0",
         "2.0",
         "0.0",
         "0.0",
         "1.0",
         "0.0",
         "1.0",
         "0.0",
         "4.0",
         "0.0",
         "0.0",
         "1.0",
         "0.0",
         "11.0",
         "4.0",
         "6.0"
        ],
        [
         "14",
         "0.0",
         "0.0",
         "1.0",
         "1.0",
         "33.0",
         "1.0",
         "1.0",
         "0.0",
         "1.0",
         "0.0",
         "1.0",
         "0.0",
         "1.0",
         "1.0",
         "4.0",
         "30.0",
         "28.0",
         "0.0",
         "0.0",
         "4.0",
         "6.0",
         "2.0"
        ],
        [
         "15",
         "0.0",
         "1.0",
         "0.0",
         "1.0",
         "33.0",
         "0.0",
         "0.0",
         "0.0",
         "1.0",
         "0.0",
         "0.0",
         "0.0",
         "1.0",
         "0.0",
         "2.0",
         "5.0",
         "0.0",
         "0.0",
         "0.0",
         "6.0",
         "6.0",
         "8.0"
        ],
        [
         "16",
         "0.0",
         "1.0",
         "1.0",
         "1.0",
         "21.0",
         "0.0",
         "0.0",
         "0.0",
         "1.0",
         "1.0",
         "1.0",
         "0.0",
         "1.0",
         "0.0",
         "3.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "10.0",
         "4.0",
         "3.0"
        ],
        [
         "17",
         "0.0",
         "0.0",
         "0.0",
         "1.0",
         "23.0",
         "1.0",
         "0.0",
         "2.0",
         "1.0",
         "0.0",
         "0.0",
         "0.0",
         "1.0",
         "0.0",
         "2.0",
         "0.0",
         "0.0",
         "0.0",
         "1.0",
         "7.0",
         "5.0",
         "6.0"
        ],
        [
         "18",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "23.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "1.0",
         "0.0",
         "1.0",
         "0.0",
         "2.0",
         "15.0",
         "0.0",
         "0.0",
         "0.0",
         "2.0",
         "6.0",
         "7.0"
        ],
        [
         "19",
         "0.0",
         "0.0",
         "1.0",
         "1.0",
         "28.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "1.0",
         "1.0",
         "0.0",
         "2.0",
         "10.0",
         "0.0",
         "0.0",
         "1.0",
         "4.0",
         "6.0",
         "8.0"
        ],
        [
         "20",
         "1.0",
         "1.0",
         "1.0",
         "1.0",
         "22.0",
         "0.0",
         "1.0",
         "0.0",
         "0.0",
         "1.0",
         "0.0",
         "0.0",
         "1.0",
         "0.0",
         "3.0",
         "30.0",
         "0.0",
         "1.0",
         "0.0",
         "12.0",
         "4.0",
         "4.0"
        ],
        [
         "21",
         "0.0",
         "1.0",
         "1.0",
         "1.0",
         "38.0",
         "1.0",
         "0.0",
         "0.0",
         "0.0",
         "1.0",
         "1.0",
         "0.0",
         "1.0",
         "0.0",
         "5.0",
         "15.0",
         "30.0",
         "1.0",
         "0.0",
         "13.0",
         "2.0",
         "3.0"
        ],
        [
         "22",
         "0.0",
         "0.0",
         "0.0",
         "1.0",
         "28.0",
         "1.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "1.0",
         "0.0",
         "1.0",
         "0.0",
         "3.0",
         "0.0",
         "7.0",
         "0.0",
         "1.0",
         "5.0",
         "5.0",
         "5.0"
        ],
        [
         "23",
         "0.0",
         "1.0",
         "0.0",
         "1.0",
         "27.0",
         "0.0",
         "0.0",
         "2.0",
         "1.0",
         "1.0",
         "1.0",
         "0.0",
         "1.0",
         "0.0",
         "1.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "13.0",
         "5.0",
         "4.0"
        ],
        [
         "24",
         "0.0",
         "1.0",
         "1.0",
         "1.0",
         "28.0",
         "1.0",
         "0.0",
         "0.0",
         "0.0",
         "1.0",
         "1.0",
         "0.0",
         "1.0",
         "0.0",
         "3.0",
         "6.0",
         "0.0",
         "1.0",
         "0.0",
         "9.0",
         "4.0",
         "6.0"
        ],
        [
         "25",
         "0.0",
         "0.0",
         "0.0",
         "1.0",
         "32.0",
         "0.0",
         "0.0",
         "0.0",
         "1.0",
         "1.0",
         "1.0",
         "0.0",
         "1.0",
         "0.0",
         "2.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "5.0",
         "6.0",
         "8.0"
        ],
        [
         "26",
         "1.0",
         "1.0",
         "1.0",
         "1.0",
         "37.0",
         "1.0",
         "1.0",
         "2.0",
         "0.0",
         "0.0",
         "1.0",
         "0.0",
         "1.0",
         "0.0",
         "5.0",
         "0.0",
         "0.0",
         "1.0",
         "1.0",
         "10.0",
         "6.0",
         "5.0"
        ],
        [
         "27",
         "1.0",
         "1.0",
         "1.0",
         "1.0",
         "28.0",
         "1.0",
         "0.0",
         "2.0",
         "0.0",
         "0.0",
         "1.0",
         "0.0",
         "1.0",
         "0.0",
         "4.0",
         "0.0",
         "0.0",
         "0.0",
         "1.0",
         "12.0",
         "2.0",
         "4.0"
        ],
        [
         "28",
         "0.0",
         "1.0",
         "1.0",
         "1.0",
         "27.0",
         "1.0",
         "0.0",
         "2.0",
         "0.0",
         "1.0",
         "1.0",
         "0.0",
         "1.0",
         "0.0",
         "4.0",
         "20.0",
         "20.0",
         "1.0",
         "0.0",
         "8.0",
         "4.0",
         "7.0"
        ],
        [
         "29",
         "0.0",
         "0.0",
         "1.0",
         "1.0",
         "31.0",
         "1.0",
         "0.0",
         "0.0",
         "1.0",
         "1.0",
         "1.0",
         "0.0",
         "1.0",
         "0.0",
         "1.0",
         "0.0",
         "0.0",
         "0.0",
         "1.0",
         "12.0",
         "6.0",
         "8.0"
        ],
        [
         "30",
         "0.0",
         "1.0",
         "1.0",
         "1.0",
         "34.0",
         "1.0",
         "1.0",
         "2.0",
         "1.0",
         "0.0",
         "0.0",
         "0.0",
         "1.0",
         "0.0",
         "4.0",
         "0.0",
         "7.0",
         "1.0",
         "0.0",
         "9.0",
         "5.0",
         "4.0"
        ],
        [
         "31",
         "0.0",
         "1.0",
         "0.0",
         "1.0",
         "33.0",
         "1.0",
         "0.0",
         "0.0",
         "1.0",
         "1.0",
         "1.0",
         "0.0",
         "0.0",
         "0.0",
         "1.0",
         "0.0",
         "0.0",
         "1.0",
         "1.0",
         "13.0",
         "3.0",
         "3.0"
        ],
        [
         "32",
         "0.0",
         "0.0",
         "0.0",
         "1.0",
         "23.0",
         "0.0",
         "0.0",
         "0.0",
         "1.0",
         "1.0",
         "1.0",
         "0.0",
         "1.0",
         "0.0",
         "1.0",
         "2.0",
         "0.0",
         "0.0",
         "0.0",
         "6.0",
         "4.0",
         "8.0"
        ],
        [
         "33",
         "0.0",
         "1.0",
         "0.0",
         "1.0",
         "31.0",
         "0.0",
         "0.0",
         "0.0",
         "1.0",
         "0.0",
         "0.0",
         "0.0",
         "1.0",
         "0.0",
         "3.0",
         "0.0",
         "0.0",
         "0.0",
         "1.0",
         "11.0",
         "6.0",
         "2.0"
        ],
        [
         "34",
         "0.0",
         "1.0",
         "1.0",
         "1.0",
         "24.0",
         "1.0",
         "0.0",
         "2.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "1.0",
         "0.0",
         "2.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "12.0",
         "3.0",
         "3.0"
        ],
        [
         "35",
         "0.0",
         "1.0",
         "0.0",
         "1.0",
         "26.0",
         "0.0",
         "0.0",
         "0.0",
         "1.0",
         "1.0",
         "1.0",
         "0.0",
         "1.0",
         "0.0",
         "2.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "9.0",
         "4.0",
         "4.0"
        ],
        [
         "36",
         "0.0",
         "1.0",
         "1.0",
         "1.0",
         "24.0",
         "1.0",
         "0.0",
         "0.0",
         "1.0",
         "0.0",
         "1.0",
         "0.0",
         "1.0",
         "0.0",
         "3.0",
         "5.0",
         "3.0",
         "1.0",
         "1.0",
         "8.0",
         "4.0",
         "3.0"
        ],
        [
         "37",
         "0.0",
         "0.0",
         "0.0",
         "1.0",
         "22.0",
         "0.0",
         "0.0",
         "0.0",
         "1.0",
         "1.0",
         "1.0",
         "0.0",
         "1.0",
         "0.0",
         "2.0",
         "0.0",
         "10.0",
         "0.0",
         "0.0",
         "12.0",
         "5.0",
         "7.0"
        ],
        [
         "38",
         "0.0",
         "0.0",
         "0.0",
         "1.0",
         "26.0",
         "1.0",
         "0.0",
         "0.0",
         "1.0",
         "1.0",
         "1.0",
         "0.0",
         "0.0",
         "0.0",
         "1.0",
         "0.0",
         "1.0",
         "0.0",
         "1.0",
         "4.0",
         "5.0",
         "3.0"
        ],
        [
         "39",
         "0.0",
         "1.0",
         "1.0",
         "1.0",
         "24.0",
         "1.0",
         "0.0",
         "0.0",
         "0.0",
         "1.0",
         "1.0",
         "0.0",
         "1.0",
         "0.0",
         "5.0",
         "0.0",
         "30.0",
         "0.0",
         "1.0",
         "9.0",
         "3.0",
         "1.0"
        ],
        [
         "40",
         "0.0",
         "0.0",
         "0.0",
         "1.0",
         "26.0",
         "0.0",
         "0.0",
         "0.0",
         "1.0",
         "1.0",
         "0.0",
         "0.0",
         "1.0",
         "0.0",
         "2.0",
         "0.0",
         "1.0",
         "0.0",
         "0.0",
         "4.0",
         "6.0",
         "7.0"
        ],
        [
         "41",
         "0.0",
         "1.0",
         "0.0",
         "1.0",
         "31.0",
         "0.0",
         "0.0",
         "2.0",
         "0.0",
         "1.0",
         "0.0",
         "0.0",
         "1.0",
         "0.0",
         "3.0",
         "0.0",
         "5.0",
         "0.0",
         "0.0",
         "13.0",
         "4.0",
         "4.0"
        ],
        [
         "42",
         "0.0",
         "0.0",
         "0.0",
         "1.0",
         "28.0",
         "1.0",
         "1.0",
         "0.0",
         "0.0",
         "1.0",
         "1.0",
         "0.0",
         "1.0",
         "1.0",
         "4.0",
         "15.0",
         "30.0",
         "1.0",
         "0.0",
         "7.0",
         "4.0",
         "3.0"
        ],
        [
         "43",
         "0.0",
         "1.0",
         "0.0",
         "1.0",
         "23.0",
         "1.0",
         "0.0",
         "0.0",
         "1.0",
         "0.0",
         "1.0",
         "0.0",
         "1.0",
         "0.0",
         "2.0",
         "25.0",
         "0.0",
         "0.0",
         "0.0",
         "9.0",
         "6.0",
         "8.0"
        ],
        [
         "44",
         "0.0",
         "0.0",
         "1.0",
         "1.0",
         "31.0",
         "1.0",
         "0.0",
         "0.0",
         "0.0",
         "1.0",
         "1.0",
         "0.0",
         "1.0",
         "0.0",
         "2.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "8.0",
         "5.0",
         "8.0"
        ],
        [
         "45",
         "0.0",
         "0.0",
         "1.0",
         "1.0",
         "21.0",
         "1.0",
         "0.0",
         "0.0",
         "1.0",
         "1.0",
         "1.0",
         "0.0",
         "0.0",
         "0.0",
         "3.0",
         "30.0",
         "0.0",
         "0.0",
         "0.0",
         "8.0",
         "6.0",
         "3.0"
        ],
        [
         "46",
         "0.0",
         "0.0",
         "0.0",
         "1.0",
         "31.0",
         "0.0",
         "0.0",
         "0.0",
         "1.0",
         "1.0",
         "1.0",
         "0.0",
         "0.0",
         "0.0",
         "2.0",
         "5.0",
         "0.0",
         "0.0",
         "0.0",
         "7.0",
         "5.0",
         "2.0"
        ],
        [
         "47",
         "1.0",
         "1.0",
         "1.0",
         "1.0",
         "25.0",
         "1.0",
         "0.0",
         "0.0",
         "0.0",
         "1.0",
         "1.0",
         "0.0",
         "1.0",
         "0.0",
         "2.0",
         "1.0",
         "0.0",
         "0.0",
         "1.0",
         "10.0",
         "4.0",
         "7.0"
        ],
        [
         "48",
         "0.0",
         "1.0",
         "1.0",
         "1.0",
         "37.0",
         "0.0",
         "0.0",
         "0.0",
         "1.0",
         "1.0",
         "1.0",
         "0.0",
         "1.0",
         "0.0",
         "3.0",
         "30.0",
         "2.0",
         "1.0",
         "1.0",
         "10.0",
         "5.0",
         "6.0"
        ],
        [
         "49",
         "0.0",
         "1.0",
         "1.0",
         "1.0",
         "33.0",
         "1.0",
         "0.0",
         "2.0",
         "0.0",
         "0.0",
         "1.0",
         "0.0",
         "1.0",
         "0.0",
         "3.0",
         "0.0",
         "30.0",
         "1.0",
         "0.0",
         "11.0",
         "4.0",
         "2.0"
        ]
       ],
       "shape": {
        "columns": 22,
        "rows": 253680
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HeartDiseaseorAttack</th>\n",
       "      <th>HighBP</th>\n",
       "      <th>HighChol</th>\n",
       "      <th>CholCheck</th>\n",
       "      <th>BMI</th>\n",
       "      <th>Smoker</th>\n",
       "      <th>Stroke</th>\n",
       "      <th>Diabetes</th>\n",
       "      <th>PhysActivity</th>\n",
       "      <th>Fruits</th>\n",
       "      <th>...</th>\n",
       "      <th>AnyHealthcare</th>\n",
       "      <th>NoDocbcCost</th>\n",
       "      <th>GenHlth</th>\n",
       "      <th>MentHlth</th>\n",
       "      <th>PhysHlth</th>\n",
       "      <th>DiffWalk</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Education</th>\n",
       "      <th>Income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253675</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253676</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253677</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253678</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253679</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>253680 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        HeartDiseaseorAttack  HighBP  HighChol  CholCheck   BMI  Smoker  \\\n",
       "0                        0.0     1.0       1.0        1.0  40.0     1.0   \n",
       "1                        0.0     0.0       0.0        0.0  25.0     1.0   \n",
       "2                        0.0     1.0       1.0        1.0  28.0     0.0   \n",
       "3                        0.0     1.0       0.0        1.0  27.0     0.0   \n",
       "4                        0.0     1.0       1.0        1.0  24.0     0.0   \n",
       "...                      ...     ...       ...        ...   ...     ...   \n",
       "253675                   0.0     1.0       1.0        1.0  45.0     0.0   \n",
       "253676                   0.0     1.0       1.0        1.0  18.0     0.0   \n",
       "253677                   0.0     0.0       0.0        1.0  28.0     0.0   \n",
       "253678                   0.0     1.0       0.0        1.0  23.0     0.0   \n",
       "253679                   1.0     1.0       1.0        1.0  25.0     0.0   \n",
       "\n",
       "        Stroke  Diabetes  PhysActivity  Fruits  ...  AnyHealthcare  \\\n",
       "0          0.0       0.0           0.0     0.0  ...            1.0   \n",
       "1          0.0       0.0           1.0     0.0  ...            0.0   \n",
       "2          0.0       0.0           0.0     1.0  ...            1.0   \n",
       "3          0.0       0.0           1.0     1.0  ...            1.0   \n",
       "4          0.0       0.0           1.0     1.0  ...            1.0   \n",
       "...        ...       ...           ...     ...  ...            ...   \n",
       "253675     0.0       0.0           0.0     1.0  ...            1.0   \n",
       "253676     0.0       2.0           0.0     0.0  ...            1.0   \n",
       "253677     0.0       0.0           1.0     1.0  ...            1.0   \n",
       "253678     0.0       0.0           0.0     1.0  ...            1.0   \n",
       "253679     0.0       2.0           1.0     1.0  ...            1.0   \n",
       "\n",
       "        NoDocbcCost  GenHlth  MentHlth  PhysHlth  DiffWalk  Sex   Age  \\\n",
       "0               0.0      5.0      18.0      15.0       1.0  0.0   9.0   \n",
       "1               1.0      3.0       0.0       0.0       0.0  0.0   7.0   \n",
       "2               1.0      5.0      30.0      30.0       1.0  0.0   9.0   \n",
       "3               0.0      2.0       0.0       0.0       0.0  0.0  11.0   \n",
       "4               0.0      2.0       3.0       0.0       0.0  0.0  11.0   \n",
       "...             ...      ...       ...       ...       ...  ...   ...   \n",
       "253675          0.0      3.0       0.0       5.0       0.0  1.0   5.0   \n",
       "253676          0.0      4.0       0.0       0.0       1.0  0.0  11.0   \n",
       "253677          0.0      1.0       0.0       0.0       0.0  0.0   2.0   \n",
       "253678          0.0      3.0       0.0       0.0       0.0  1.0   7.0   \n",
       "253679          0.0      2.0       0.0       0.0       0.0  0.0   9.0   \n",
       "\n",
       "        Education  Income  \n",
       "0             4.0     3.0  \n",
       "1             6.0     1.0  \n",
       "2             4.0     8.0  \n",
       "3             3.0     6.0  \n",
       "4             5.0     4.0  \n",
       "...           ...     ...  \n",
       "253675        6.0     7.0  \n",
       "253676        2.0     4.0  \n",
       "253677        5.0     2.0  \n",
       "253678        5.0     1.0  \n",
       "253679        6.0     2.0  \n",
       "\n",
       "[253680 rows x 22 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "df = pd.read_csv(\"heart_disease_health_indicators_BRFSS2015.csv\")\n",
    "df "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "X = df.drop('HeartDiseaseorAttack', axis=1)\n",
    "y = df['HeartDiseaseorAttack'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Without Demensional Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.75      0.85     45957\n",
      "           1       0.25      0.80      0.38      4779\n",
      "\n",
      "    accuracy                           0.75     50736\n",
      "   macro avg       0.61      0.77      0.61     50736\n",
      "weighted avg       0.90      0.75      0.80     50736\n",
      "\n",
      "[[34418 11539]\n",
      " [  969  3810]]\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "lrm = LogisticRegression(max_iter=10000, class_weight='balanced', random_state=42)\n",
    "lrm.fit(X_train, y_train)\n",
    "\n",
    "y_pred = lrm.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.75      0.85     68936\n",
      "           1       0.25      0.80      0.38      7168\n",
      "\n",
      "    accuracy                           0.75     76104\n",
      "   macro avg       0.61      0.77      0.61     76104\n",
      "weighted avg       0.90      0.75      0.80     76104\n",
      "\n",
      "[[51507 17429]\n",
      " [ 1461  5707]]\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "lrm = LogisticRegression(max_iter=10000, class_weight='balanced', random_state=42)\n",
    "lrm.fit(X_train, y_train)\n",
    "\n",
    "y_pred = lrm.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.75      0.84     91915\n",
      "           1       0.25      0.80      0.38      9557\n",
      "\n",
      "    accuracy                           0.75    101472\n",
      "   macro avg       0.61      0.77      0.61    101472\n",
      "weighted avg       0.90      0.75      0.80    101472\n",
      "\n",
      "[[68588 23327]\n",
      " [ 1956  7601]]\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42, stratify=y)\n",
    "\n",
    "lrm = LogisticRegression(max_iter=10000, class_weight='balanced', random_state=42)\n",
    "lrm.fit(X_train, y_train)\n",
    "\n",
    "y_pred = lrm.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensional Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.75      0.84     45957\n",
      "           1       0.24      0.78      0.37      4779\n",
      "\n",
      "    accuracy                           0.75     50736\n",
      "   macro avg       0.61      0.76      0.61     50736\n",
      "weighted avg       0.90      0.75      0.80     50736\n",
      "\n",
      "[[34358 11599]\n",
      " [ 1049  3730]]\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "std = StandardScaler()\n",
    "X_train_scaled = std.fit_transform(X_train)\n",
    "X_test_scaled = std.transform(X_test)\n",
    "\n",
    "pca = PCA(n_components=14)\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "\n",
    "lrm = LogisticRegression(max_iter=10000, class_weight='balanced')\n",
    "lrm.fit(X_train_pca, y_train)\n",
    "\n",
    "y_pred = lrm.predict(X_test_pca)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.75      0.84     68936\n",
      "           1       0.24      0.78      0.37      7168\n",
      "\n",
      "    accuracy                           0.75     76104\n",
      "   macro avg       0.61      0.76      0.61     76104\n",
      "weighted avg       0.90      0.75      0.80     76104\n",
      "\n",
      "[[51471 17465]\n",
      " [ 1578  5590]]\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "std = StandardScaler()\n",
    "X_train_scaled = std.fit_transform(X_train)\n",
    "X_test_scaled = std.transform(X_test)\n",
    "\n",
    "pca = PCA(n_components=14)\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "\n",
    "lrm = LogisticRegression(max_iter=10000, class_weight='balanced')\n",
    "lrm.fit(X_train_pca, y_train)\n",
    "\n",
    "y_pred = lrm.predict(X_test_pca)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.75      0.84     91915\n",
      "           1       0.24      0.78      0.37      9557\n",
      "\n",
      "    accuracy                           0.75    101472\n",
      "   macro avg       0.61      0.76      0.61    101472\n",
      "weighted avg       0.90      0.75      0.80    101472\n",
      "\n",
      "[[68478 23437]\n",
      " [ 2075  7482]]\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42, stratify=y)\n",
    "\n",
    "std = StandardScaler()\n",
    "X_train_scaled = std.fit_transform(X_train)\n",
    "X_test_scaled = std.transform(X_test)\n",
    "\n",
    "pca = PCA(n_components=14)\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "\n",
    "lrm = LogisticRegression(max_iter=10000, class_weight='balanced')\n",
    "lrm.fit(X_train_pca, y_train)\n",
    "\n",
    "y_pred = lrm.predict(X_test_pca)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Loading dataset...\n",
      "Dataset shape: (253680, 22)\n",
      "   HeartDiseaseorAttack  HighBP  HighChol  CholCheck   BMI  Smoker  Stroke  \\\n",
      "0                   0.0     1.0       1.0        1.0  40.0     1.0     0.0   \n",
      "1                   0.0     0.0       0.0        0.0  25.0     1.0     0.0   \n",
      "2                   0.0     1.0       1.0        1.0  28.0     0.0     0.0   \n",
      "3                   0.0     1.0       0.0        1.0  27.0     0.0     0.0   \n",
      "4                   0.0     1.0       1.0        1.0  24.0     0.0     0.0   \n",
      "\n",
      "   Diabetes  PhysActivity  Fruits  ...  AnyHealthcare  NoDocbcCost  GenHlth  \\\n",
      "0       0.0           0.0     0.0  ...            1.0          0.0      5.0   \n",
      "1       0.0           1.0     0.0  ...            0.0          1.0      3.0   \n",
      "2       0.0           0.0     1.0  ...            1.0          1.0      5.0   \n",
      "3       0.0           1.0     1.0  ...            1.0          0.0      2.0   \n",
      "4       0.0           1.0     1.0  ...            1.0          0.0      2.0   \n",
      "\n",
      "   MentHlth  PhysHlth  DiffWalk  Sex   Age  Education  Income  \n",
      "0      18.0      15.0       1.0  0.0   9.0        4.0     3.0  \n",
      "1       0.0       0.0       0.0  0.0   7.0        6.0     1.0  \n",
      "2      30.0      30.0       1.0  0.0   9.0        4.0     8.0  \n",
      "3       0.0       0.0       0.0  0.0  11.0        3.0     6.0  \n",
      "4       3.0       0.0       0.0  0.0  11.0        5.0     4.0  \n",
      "\n",
      "[5 rows x 22 columns]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import torchvision.ops as ops\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "import seaborn as sns\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "class HeartDiseaseDataset(Dataset):\n",
    "    def __init__(self, features, targets):\n",
    "        self.features = torch.tensor(features, dtype=torch.float32)\n",
    "        self.targets = torch.tensor(targets, dtype=torch.float32)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.targets[idx]\n",
    "\n",
    "print(\"Loading dataset...\")\n",
    "data = pd.read_csv('heart_disease_health_indicators_BRFSS2015.csv')\n",
    "print(f\"Dataset shape: {data.shape}\")\n",
    "print(data.head())\n",
    "\n",
    "X = data.drop('HeartDiseaseorAttack', axis=1).values\n",
    "y = data['HeartDiseaseorAttack'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Class distribution:\n",
      "Class 0.0: 229787 samples (90.58%)\n",
      "Class 1.0: 23893 samples (9.42%)\n"
     ]
    }
   ],
   "source": [
    "unique, counts = np.unique(y, return_counts=True)\n",
    "print(\"\\nClass distribution:\")\n",
    "for value, count in zip(unique, counts):\n",
    "    print(f\"Class {value}: {count} samples ({100 * count / len(y):.2f}%)\")\n",
    "\n",
    "total_size = len(X)\n",
    "train_size = int(0.8 * total_size)\n",
    "test_size = total_size - train_size\n",
    "\n",
    "dataset = HeartDiseaseDataset(X, y)\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1024, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "class HeartDiseaseMLPClassifier(nn.Module):\n",
    "    def __init__(self, input_size, num_classes=2, class_frequencies=None):\n",
    "        super(HeartDiseaseMLPClassifier, self).__init__()\n",
    "        \n",
    "        self.mlp = ops.MLP(\n",
    "            in_channels=input_size,\n",
    "            hidden_channels=[128, 64, 32],  # 3 hidden layers\n",
    "            norm_layer=nn.BatchNorm1d,      # batch normalization\n",
    "            activation_layer=nn.ReLU,       # ReLU activation\n",
    "            dropout=0.3,                    # 30% dropout for regularization\n",
    "            bias=True\n",
    "        )\n",
    "        \n",
    "        # Output layer for binary classification\n",
    "        self.classifier = nn.Linear(32, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        if class_frequencies:\n",
    "            self.weights = torch.tensor([1.0 / (freq + 1e-7) for freq in class_frequencies], device=device)\n",
    "        else:\n",
    "            self.weights = torch.ones(num_classes, device=device)\n",
    "                \n",
    "    def forward(self, x):\n",
    "        x = self.mlp(x)       \n",
    "        x = self.classifier(x) \n",
    "        x = self.sigmoid(x)    \n",
    "        return x.squeeze(-1)   \n",
    "    \n",
    "    def get_weighted_loss(self, outputs, labels):\n",
    "        # Ensure outputs and labels are on the same device\n",
    "        outputs = outputs.to(self.weights.device)\n",
    "        labels = labels.to(self.weights.device)\n",
    "        labels = labels.float()        \n",
    "        \n",
    "        # For binary classification with sigmoid outputs\n",
    "        criterion = nn.BCELoss(reduction='none')\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Apply class weights for binary classification\n",
    "        weighted_loss = torch.zeros_like(loss)\n",
    "        \n",
    "        # Class 1 (positive samples)\n",
    "        positive_mask = (labels == 1)\n",
    "        if positive_mask.any():\n",
    "            weighted_loss[positive_mask] = loss[positive_mask] * self.weights[1]\n",
    "        \n",
    "        # Class 0 (negative samples)\n",
    "        negative_mask = (labels == 0)\n",
    "        if negative_mask.any():\n",
    "            weighted_loss[negative_mask] = loss[negative_mask] * self.weights[0]\n",
    "        \n",
    "        return weighted_loss.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Without weight-balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HeartDiseaseMLPClassifier(\n",
      "  (mlp): MLP(\n",
      "    (0): Linear(in_features=21, out_features=128, bias=True)\n",
      "    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.3, inplace=False)\n",
      "    (4): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (5): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU()\n",
      "    (7): Dropout(p=0.3, inplace=False)\n",
      "    (8): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (9): Dropout(p=0.3, inplace=False)\n",
      "  )\n",
      "  (classifier): Linear(in_features=32, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "input_features = X.shape[1]  \n",
    "model = HeartDiseaseMLPClassifier(input_features).to(device)\n",
    "print(model)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training...\n",
      "Epoch: 1/20, Batch: 0/1586, Loss: 0.6899\n",
      "Epoch: 1/20, Batch: 50/1586, Loss: 0.2444\n",
      "Epoch: 1/20, Batch: 100/1586, Loss: 0.3659\n",
      "Epoch: 1/20, Batch: 150/1586, Loss: 0.2561\n",
      "Epoch: 1/20, Batch: 200/1586, Loss: 0.2546\n",
      "Epoch: 1/20, Batch: 250/1586, Loss: 0.2226\n",
      "Epoch: 1/20, Batch: 300/1586, Loss: 0.2958\n",
      "Epoch: 1/20, Batch: 350/1586, Loss: 0.3165\n",
      "Epoch: 1/20, Batch: 400/1586, Loss: 0.2802\n",
      "Epoch: 1/20, Batch: 450/1586, Loss: 0.2850\n",
      "Epoch: 1/20, Batch: 500/1586, Loss: 0.3040\n",
      "Epoch: 1/20, Batch: 550/1586, Loss: 0.3082\n",
      "Epoch: 1/20, Batch: 600/1586, Loss: 0.1681\n",
      "Epoch: 1/20, Batch: 650/1586, Loss: 0.2659\n",
      "Epoch: 1/20, Batch: 700/1586, Loss: 0.2841\n",
      "Epoch: 1/20, Batch: 750/1586, Loss: 0.2347\n",
      "Epoch: 1/20, Batch: 800/1586, Loss: 0.2712\n",
      "Epoch: 1/20, Batch: 850/1586, Loss: 0.1981\n",
      "Epoch: 1/20, Batch: 900/1586, Loss: 0.3458\n",
      "Epoch: 1/20, Batch: 950/1586, Loss: 0.1936\n",
      "Epoch: 1/20, Batch: 1000/1586, Loss: 0.2657\n",
      "Epoch: 1/20, Batch: 1050/1586, Loss: 0.1987\n",
      "Epoch: 1/20, Batch: 1100/1586, Loss: 0.2083\n",
      "Epoch: 1/20, Batch: 1150/1586, Loss: 0.2690\n",
      "Epoch: 1/20, Batch: 1200/1586, Loss: 0.2076\n",
      "Epoch: 1/20, Batch: 1250/1586, Loss: 0.2767\n",
      "Epoch: 1/20, Batch: 1300/1586, Loss: 0.3471\n",
      "Epoch: 1/20, Batch: 1350/1586, Loss: 0.2770\n",
      "Epoch: 1/20, Batch: 1400/1586, Loss: 0.2193\n",
      "Epoch: 1/20, Batch: 1450/1586, Loss: 0.3154\n",
      "Epoch: 1/20, Batch: 1500/1586, Loss: 0.2234\n",
      "Epoch: 1/20, Batch: 1550/1586, Loss: 0.2473\n",
      "Epoch: 1, Train Loss: 0.2569\n",
      "Epoch: 2/20, Batch: 0/1586, Loss: 0.2200\n",
      "Epoch: 2/20, Batch: 50/1586, Loss: 0.2214\n",
      "Epoch: 2/20, Batch: 100/1586, Loss: 0.2055\n",
      "Epoch: 2/20, Batch: 150/1586, Loss: 0.2165\n",
      "Epoch: 2/20, Batch: 200/1586, Loss: 0.2184\n",
      "Epoch: 2/20, Batch: 250/1586, Loss: 0.1811\n",
      "Epoch: 2/20, Batch: 300/1586, Loss: 0.2452\n",
      "Epoch: 2/20, Batch: 350/1586, Loss: 0.3153\n",
      "Epoch: 2/20, Batch: 400/1586, Loss: 0.2529\n",
      "Epoch: 2/20, Batch: 450/1586, Loss: 0.1819\n",
      "Epoch: 2/20, Batch: 500/1586, Loss: 0.2601\n",
      "Epoch: 2/20, Batch: 550/1586, Loss: 0.3119\n",
      "Epoch: 2/20, Batch: 600/1586, Loss: 0.2567\n",
      "Epoch: 2/20, Batch: 650/1586, Loss: 0.2121\n",
      "Epoch: 2/20, Batch: 700/1586, Loss: 0.2490\n",
      "Epoch: 2/20, Batch: 750/1586, Loss: 0.2070\n",
      "Epoch: 2/20, Batch: 800/1586, Loss: 0.1965\n",
      "Epoch: 2/20, Batch: 850/1586, Loss: 0.2879\n",
      "Epoch: 2/20, Batch: 900/1586, Loss: 0.2678\n",
      "Epoch: 2/20, Batch: 950/1586, Loss: 0.1974\n",
      "Epoch: 2/20, Batch: 1000/1586, Loss: 0.2515\n",
      "Epoch: 2/20, Batch: 1050/1586, Loss: 0.2690\n",
      "Epoch: 2/20, Batch: 1100/1586, Loss: 0.1874\n",
      "Epoch: 2/20, Batch: 1150/1586, Loss: 0.2534\n",
      "Epoch: 2/20, Batch: 1200/1586, Loss: 0.2450\n",
      "Epoch: 2/20, Batch: 1250/1586, Loss: 0.1919\n",
      "Epoch: 2/20, Batch: 1300/1586, Loss: 0.2845\n",
      "Epoch: 2/20, Batch: 1350/1586, Loss: 0.2435\n",
      "Epoch: 2/20, Batch: 1400/1586, Loss: 0.2877\n",
      "Epoch: 2/20, Batch: 1450/1586, Loss: 0.3183\n",
      "Epoch: 2/20, Batch: 1500/1586, Loss: 0.2601\n",
      "Epoch: 2/20, Batch: 1550/1586, Loss: 0.2353\n",
      "Epoch: 2, Train Loss: 0.2461\n",
      "Epoch: 3/20, Batch: 0/1586, Loss: 0.2979\n",
      "Epoch: 3/20, Batch: 50/1586, Loss: 0.1649\n",
      "Epoch: 3/20, Batch: 100/1586, Loss: 0.2953\n",
      "Epoch: 3/20, Batch: 150/1586, Loss: 0.1913\n",
      "Epoch: 3/20, Batch: 200/1586, Loss: 0.1552\n",
      "Epoch: 3/20, Batch: 250/1586, Loss: 0.3392\n",
      "Epoch: 3/20, Batch: 300/1586, Loss: 0.2098\n",
      "Epoch: 3/20, Batch: 350/1586, Loss: 0.2167\n",
      "Epoch: 3/20, Batch: 400/1586, Loss: 0.3110\n",
      "Epoch: 3/20, Batch: 450/1586, Loss: 0.1856\n",
      "Epoch: 3/20, Batch: 500/1586, Loss: 0.2612\n",
      "Epoch: 3/20, Batch: 550/1586, Loss: 0.1902\n",
      "Epoch: 3/20, Batch: 600/1586, Loss: 0.2334\n",
      "Epoch: 3/20, Batch: 650/1586, Loss: 0.2800\n",
      "Epoch: 3/20, Batch: 700/1586, Loss: 0.2420\n",
      "Epoch: 3/20, Batch: 750/1586, Loss: 0.1913\n",
      "Epoch: 3/20, Batch: 800/1586, Loss: 0.2413\n",
      "Epoch: 3/20, Batch: 850/1586, Loss: 0.2196\n",
      "Epoch: 3/20, Batch: 900/1586, Loss: 0.2809\n",
      "Epoch: 3/20, Batch: 950/1586, Loss: 0.2683\n",
      "Epoch: 3/20, Batch: 1000/1586, Loss: 0.1522\n",
      "Epoch: 3/20, Batch: 1050/1586, Loss: 0.2456\n",
      "Epoch: 3/20, Batch: 1100/1586, Loss: 0.2813\n",
      "Epoch: 3/20, Batch: 1150/1586, Loss: 0.3244\n",
      "Epoch: 3/20, Batch: 1200/1586, Loss: 0.3302\n",
      "Epoch: 3/20, Batch: 1250/1586, Loss: 0.3131\n",
      "Epoch: 3/20, Batch: 1300/1586, Loss: 0.2260\n",
      "Epoch: 3/20, Batch: 1350/1586, Loss: 0.2033\n",
      "Epoch: 3/20, Batch: 1400/1586, Loss: 0.3193\n",
      "Epoch: 3/20, Batch: 1450/1586, Loss: 0.1848\n",
      "Epoch: 3/20, Batch: 1500/1586, Loss: 0.1910\n",
      "Epoch: 3/20, Batch: 1550/1586, Loss: 0.3144\n",
      "Epoch: 3, Train Loss: 0.2446\n",
      "Epoch: 4/20, Batch: 0/1586, Loss: 0.2612\n",
      "Epoch: 4/20, Batch: 50/1586, Loss: 0.2290\n",
      "Epoch: 4/20, Batch: 100/1586, Loss: 0.2803\n",
      "Epoch: 4/20, Batch: 150/1586, Loss: 0.2401\n",
      "Epoch: 4/20, Batch: 200/1586, Loss: 0.1741\n",
      "Epoch: 4/20, Batch: 250/1586, Loss: 0.2347\n",
      "Epoch: 4/20, Batch: 300/1586, Loss: 0.2305\n",
      "Epoch: 4/20, Batch: 350/1586, Loss: 0.2388\n",
      "Epoch: 4/20, Batch: 400/1586, Loss: 0.2393\n",
      "Epoch: 4/20, Batch: 450/1586, Loss: 0.2695\n",
      "Epoch: 4/20, Batch: 500/1586, Loss: 0.2244\n",
      "Epoch: 4/20, Batch: 550/1586, Loss: 0.3789\n",
      "Epoch: 4/20, Batch: 600/1586, Loss: 0.2258\n",
      "Epoch: 4/20, Batch: 650/1586, Loss: 0.3302\n",
      "Epoch: 4/20, Batch: 700/1586, Loss: 0.2516\n",
      "Epoch: 4/20, Batch: 750/1586, Loss: 0.2728\n",
      "Epoch: 4/20, Batch: 800/1586, Loss: 0.2466\n",
      "Epoch: 4/20, Batch: 850/1586, Loss: 0.1911\n",
      "Epoch: 4/20, Batch: 900/1586, Loss: 0.2205\n",
      "Epoch: 4/20, Batch: 950/1586, Loss: 0.1996\n",
      "Epoch: 4/20, Batch: 1000/1586, Loss: 0.2036\n",
      "Epoch: 4/20, Batch: 1050/1586, Loss: 0.1910\n",
      "Epoch: 4/20, Batch: 1100/1586, Loss: 0.3116\n",
      "Epoch: 4/20, Batch: 1150/1586, Loss: 0.2318\n",
      "Epoch: 4/20, Batch: 1200/1586, Loss: 0.2622\n",
      "Epoch: 4/20, Batch: 1250/1586, Loss: 0.1955\n",
      "Epoch: 4/20, Batch: 1300/1586, Loss: 0.1795\n",
      "Epoch: 4/20, Batch: 1350/1586, Loss: 0.2201\n",
      "Epoch: 4/20, Batch: 1400/1586, Loss: 0.2444\n",
      "Epoch: 4/20, Batch: 1450/1586, Loss: 0.2220\n",
      "Epoch: 4/20, Batch: 1500/1586, Loss: 0.2243\n",
      "Epoch: 4/20, Batch: 1550/1586, Loss: 0.2246\n",
      "Epoch: 4, Train Loss: 0.2437\n",
      "Epoch: 5/20, Batch: 0/1586, Loss: 0.2690\n",
      "Epoch: 5/20, Batch: 50/1586, Loss: 0.2291\n",
      "Epoch: 5/20, Batch: 100/1586, Loss: 0.2669\n",
      "Epoch: 5/20, Batch: 150/1586, Loss: 0.3206\n",
      "Epoch: 5/20, Batch: 200/1586, Loss: 0.2355\n",
      "Epoch: 5/20, Batch: 250/1586, Loss: 0.2586\n",
      "Epoch: 5/20, Batch: 300/1586, Loss: 0.2151\n",
      "Epoch: 5/20, Batch: 350/1586, Loss: 0.2030\n",
      "Epoch: 5/20, Batch: 400/1586, Loss: 0.1948\n",
      "Epoch: 5/20, Batch: 450/1586, Loss: 0.2575\n",
      "Epoch: 5/20, Batch: 500/1586, Loss: 0.1873\n",
      "Epoch: 5/20, Batch: 550/1586, Loss: 0.2745\n",
      "Epoch: 5/20, Batch: 600/1586, Loss: 0.2703\n",
      "Epoch: 5/20, Batch: 650/1586, Loss: 0.1901\n",
      "Epoch: 5/20, Batch: 700/1586, Loss: 0.1700\n",
      "Epoch: 5/20, Batch: 750/1586, Loss: 0.1884\n",
      "Epoch: 5/20, Batch: 800/1586, Loss: 0.2768\n",
      "Epoch: 5/20, Batch: 850/1586, Loss: 0.2619\n",
      "Epoch: 5/20, Batch: 900/1586, Loss: 0.2872\n",
      "Epoch: 5/20, Batch: 950/1586, Loss: 0.2650\n",
      "Epoch: 5/20, Batch: 1000/1586, Loss: 0.3552\n",
      "Epoch: 5/20, Batch: 1050/1586, Loss: 0.3045\n",
      "Epoch: 5/20, Batch: 1100/1586, Loss: 0.2060\n",
      "Epoch: 5/20, Batch: 1150/1586, Loss: 0.3041\n",
      "Epoch: 5/20, Batch: 1200/1586, Loss: 0.3000\n",
      "Epoch: 5/20, Batch: 1250/1586, Loss: 0.2126\n",
      "Epoch: 5/20, Batch: 1300/1586, Loss: 0.1951\n",
      "Epoch: 5/20, Batch: 1350/1586, Loss: 0.1942\n",
      "Epoch: 5/20, Batch: 1400/1586, Loss: 0.2289\n",
      "Epoch: 5/20, Batch: 1450/1586, Loss: 0.2359\n",
      "Epoch: 5/20, Batch: 1500/1586, Loss: 0.3312\n",
      "Epoch: 5/20, Batch: 1550/1586, Loss: 0.2582\n",
      "Epoch: 5, Train Loss: 0.2432\n",
      "Epoch: 6/20, Batch: 0/1586, Loss: 0.2696\n",
      "Epoch: 6/20, Batch: 50/1586, Loss: 0.2293\n",
      "Epoch: 6/20, Batch: 100/1586, Loss: 0.1954\n",
      "Epoch: 6/20, Batch: 150/1586, Loss: 0.2593\n",
      "Epoch: 6/20, Batch: 200/1586, Loss: 0.2731\n",
      "Epoch: 6/20, Batch: 250/1586, Loss: 0.2793\n",
      "Epoch: 6/20, Batch: 300/1586, Loss: 0.2095\n",
      "Epoch: 6/20, Batch: 350/1586, Loss: 0.3906\n",
      "Epoch: 6/20, Batch: 400/1586, Loss: 0.2670\n",
      "Epoch: 6/20, Batch: 450/1586, Loss: 0.3526\n",
      "Epoch: 6/20, Batch: 500/1586, Loss: 0.2753\n",
      "Epoch: 6/20, Batch: 550/1586, Loss: 0.3372\n",
      "Epoch: 6/20, Batch: 600/1586, Loss: 0.4498\n",
      "Epoch: 6/20, Batch: 650/1586, Loss: 0.2502\n",
      "Epoch: 6/20, Batch: 700/1586, Loss: 0.2496\n",
      "Epoch: 6/20, Batch: 750/1586, Loss: 0.1917\n",
      "Epoch: 6/20, Batch: 800/1586, Loss: 0.3088\n",
      "Epoch: 6/20, Batch: 850/1586, Loss: 0.2074\n",
      "Epoch: 6/20, Batch: 900/1586, Loss: 0.2368\n",
      "Epoch: 6/20, Batch: 950/1586, Loss: 0.2194\n",
      "Epoch: 6/20, Batch: 1000/1586, Loss: 0.1715\n",
      "Epoch: 6/20, Batch: 1050/1586, Loss: 0.2120\n",
      "Epoch: 6/20, Batch: 1100/1586, Loss: 0.2148\n",
      "Epoch: 6/20, Batch: 1150/1586, Loss: 0.1924\n",
      "Epoch: 6/20, Batch: 1200/1586, Loss: 0.2641\n",
      "Epoch: 6/20, Batch: 1250/1586, Loss: 0.2643\n",
      "Epoch: 6/20, Batch: 1300/1586, Loss: 0.2581\n",
      "Epoch: 6/20, Batch: 1350/1586, Loss: 0.2316\n",
      "Epoch: 6/20, Batch: 1400/1586, Loss: 0.2224\n",
      "Epoch: 6/20, Batch: 1450/1586, Loss: 0.1946\n",
      "Epoch: 6/20, Batch: 1500/1586, Loss: 0.2842\n",
      "Epoch: 6/20, Batch: 1550/1586, Loss: 0.1876\n",
      "Epoch: 6, Train Loss: 0.2427\n",
      "Epoch: 7/20, Batch: 0/1586, Loss: 0.2605\n",
      "Epoch: 7/20, Batch: 50/1586, Loss: 0.3385\n",
      "Epoch: 7/20, Batch: 100/1586, Loss: 0.1853\n",
      "Epoch: 7/20, Batch: 150/1586, Loss: 0.2622\n",
      "Epoch: 7/20, Batch: 200/1586, Loss: 0.2361\n",
      "Epoch: 7/20, Batch: 250/1586, Loss: 0.3160\n",
      "Epoch: 7/20, Batch: 300/1586, Loss: 0.1789\n",
      "Epoch: 7/20, Batch: 350/1586, Loss: 0.3080\n",
      "Epoch: 7/20, Batch: 400/1586, Loss: 0.1733\n",
      "Epoch: 7/20, Batch: 450/1586, Loss: 0.2039\n",
      "Epoch: 7/20, Batch: 500/1586, Loss: 0.3011\n",
      "Epoch: 7/20, Batch: 550/1586, Loss: 0.2458\n",
      "Epoch: 7/20, Batch: 600/1586, Loss: 0.3027\n",
      "Epoch: 7/20, Batch: 650/1586, Loss: 0.2730\n",
      "Epoch: 7/20, Batch: 700/1586, Loss: 0.2258\n",
      "Epoch: 7/20, Batch: 750/1586, Loss: 0.2172\n",
      "Epoch: 7/20, Batch: 800/1586, Loss: 0.2166\n",
      "Epoch: 7/20, Batch: 850/1586, Loss: 0.3514\n",
      "Epoch: 7/20, Batch: 900/1586, Loss: 0.1793\n",
      "Epoch: 7/20, Batch: 950/1586, Loss: 0.2600\n",
      "Epoch: 7/20, Batch: 1000/1586, Loss: 0.3254\n",
      "Epoch: 7/20, Batch: 1050/1586, Loss: 0.2060\n",
      "Epoch: 7/20, Batch: 1100/1586, Loss: 0.2184\n",
      "Epoch: 7/20, Batch: 1150/1586, Loss: 0.2119\n",
      "Epoch: 7/20, Batch: 1200/1586, Loss: 0.2344\n",
      "Epoch: 7/20, Batch: 1250/1586, Loss: 0.3261\n",
      "Epoch: 7/20, Batch: 1300/1586, Loss: 0.1288\n",
      "Epoch: 7/20, Batch: 1350/1586, Loss: 0.3939\n",
      "Epoch: 7/20, Batch: 1400/1586, Loss: 0.2651\n",
      "Epoch: 7/20, Batch: 1450/1586, Loss: 0.2870\n",
      "Epoch: 7/20, Batch: 1500/1586, Loss: 0.2407\n",
      "Epoch: 7/20, Batch: 1550/1586, Loss: 0.2347\n",
      "Epoch: 7, Train Loss: 0.2421\n",
      "Epoch: 8/20, Batch: 0/1586, Loss: 0.2483\n",
      "Epoch: 8/20, Batch: 50/1586, Loss: 0.2069\n",
      "Epoch: 8/20, Batch: 100/1586, Loss: 0.1893\n",
      "Epoch: 8/20, Batch: 150/1586, Loss: 0.2338\n",
      "Epoch: 8/20, Batch: 200/1586, Loss: 0.2818\n",
      "Epoch: 8/20, Batch: 250/1586, Loss: 0.3574\n",
      "Epoch: 8/20, Batch: 300/1586, Loss: 0.2688\n",
      "Epoch: 8/20, Batch: 350/1586, Loss: 0.1695\n",
      "Epoch: 8/20, Batch: 400/1586, Loss: 0.2585\n",
      "Epoch: 8/20, Batch: 450/1586, Loss: 0.3584\n",
      "Epoch: 8/20, Batch: 500/1586, Loss: 0.1934\n",
      "Epoch: 8/20, Batch: 550/1586, Loss: 0.3502\n",
      "Epoch: 8/20, Batch: 600/1586, Loss: 0.3838\n",
      "Epoch: 8/20, Batch: 650/1586, Loss: 0.2224\n",
      "Epoch: 8/20, Batch: 700/1586, Loss: 0.2593\n",
      "Epoch: 8/20, Batch: 750/1586, Loss: 0.2457\n",
      "Epoch: 8/20, Batch: 800/1586, Loss: 0.2492\n",
      "Epoch: 8/20, Batch: 850/1586, Loss: 0.2597\n",
      "Epoch: 8/20, Batch: 900/1586, Loss: 0.1976\n",
      "Epoch: 8/20, Batch: 950/1586, Loss: 0.2422\n",
      "Epoch: 8/20, Batch: 1000/1586, Loss: 0.2010\n",
      "Epoch: 8/20, Batch: 1050/1586, Loss: 0.2668\n",
      "Epoch: 8/20, Batch: 1100/1586, Loss: 0.2348\n",
      "Epoch: 8/20, Batch: 1150/1586, Loss: 0.3093\n",
      "Epoch: 8/20, Batch: 1200/1586, Loss: 0.2621\n",
      "Epoch: 8/20, Batch: 1250/1586, Loss: 0.1806\n",
      "Epoch: 8/20, Batch: 1300/1586, Loss: 0.2409\n",
      "Epoch: 8/20, Batch: 1350/1586, Loss: 0.1924\n",
      "Epoch: 8/20, Batch: 1400/1586, Loss: 0.2906\n",
      "Epoch: 8/20, Batch: 1450/1586, Loss: 0.2714\n",
      "Epoch: 8/20, Batch: 1500/1586, Loss: 0.1621\n",
      "Epoch: 8/20, Batch: 1550/1586, Loss: 0.2534\n",
      "Epoch: 8, Train Loss: 0.2417\n",
      "Epoch: 9/20, Batch: 0/1586, Loss: 0.2473\n",
      "Epoch: 9/20, Batch: 50/1586, Loss: 0.3189\n",
      "Epoch: 9/20, Batch: 100/1586, Loss: 0.2649\n",
      "Epoch: 9/20, Batch: 150/1586, Loss: 0.2700\n",
      "Epoch: 9/20, Batch: 200/1586, Loss: 0.1906\n",
      "Epoch: 9/20, Batch: 250/1586, Loss: 0.2502\n",
      "Epoch: 9/20, Batch: 300/1586, Loss: 0.2573\n",
      "Epoch: 9/20, Batch: 350/1586, Loss: 0.3087\n",
      "Epoch: 9/20, Batch: 400/1586, Loss: 0.2306\n",
      "Epoch: 9/20, Batch: 450/1586, Loss: 0.1844\n",
      "Epoch: 9/20, Batch: 500/1586, Loss: 0.1502\n",
      "Epoch: 9/20, Batch: 550/1586, Loss: 0.2394\n",
      "Epoch: 9/20, Batch: 600/1586, Loss: 0.2067\n",
      "Epoch: 9/20, Batch: 650/1586, Loss: 0.1856\n",
      "Epoch: 9/20, Batch: 700/1586, Loss: 0.3451\n",
      "Epoch: 9/20, Batch: 750/1586, Loss: 0.2320\n",
      "Epoch: 9/20, Batch: 800/1586, Loss: 0.2644\n",
      "Epoch: 9/20, Batch: 850/1586, Loss: 0.2349\n",
      "Epoch: 9/20, Batch: 900/1586, Loss: 0.2952\n",
      "Epoch: 9/20, Batch: 950/1586, Loss: 0.1808\n",
      "Epoch: 9/20, Batch: 1000/1586, Loss: 0.2146\n",
      "Epoch: 9/20, Batch: 1050/1586, Loss: 0.3164\n",
      "Epoch: 9/20, Batch: 1100/1586, Loss: 0.1867\n",
      "Epoch: 9/20, Batch: 1150/1586, Loss: 0.2045\n",
      "Epoch: 9/20, Batch: 1200/1586, Loss: 0.2323\n",
      "Epoch: 9/20, Batch: 1250/1586, Loss: 0.2500\n",
      "Epoch: 9/20, Batch: 1300/1586, Loss: 0.2634\n",
      "Epoch: 9/20, Batch: 1350/1586, Loss: 0.2232\n",
      "Epoch: 9/20, Batch: 1400/1586, Loss: 0.2927\n",
      "Epoch: 9/20, Batch: 1450/1586, Loss: 0.2561\n",
      "Epoch: 9/20, Batch: 1500/1586, Loss: 0.1670\n",
      "Epoch: 9/20, Batch: 1550/1586, Loss: 0.2587\n",
      "Epoch: 9, Train Loss: 0.2415\n",
      "Epoch: 10/20, Batch: 0/1586, Loss: 0.2870\n",
      "Epoch: 10/20, Batch: 50/1586, Loss: 0.1586\n",
      "Epoch: 10/20, Batch: 100/1586, Loss: 0.1672\n",
      "Epoch: 10/20, Batch: 150/1586, Loss: 0.2303\n",
      "Epoch: 10/20, Batch: 200/1586, Loss: 0.2351\n",
      "Epoch: 10/20, Batch: 250/1586, Loss: 0.2475\n",
      "Epoch: 10/20, Batch: 300/1586, Loss: 0.2423\n",
      "Epoch: 10/20, Batch: 350/1586, Loss: 0.2192\n",
      "Epoch: 10/20, Batch: 400/1586, Loss: 0.3552\n",
      "Epoch: 10/20, Batch: 450/1586, Loss: 0.2315\n",
      "Epoch: 10/20, Batch: 500/1586, Loss: 0.1899\n",
      "Epoch: 10/20, Batch: 550/1586, Loss: 0.2532\n",
      "Epoch: 10/20, Batch: 600/1586, Loss: 0.2338\n",
      "Epoch: 10/20, Batch: 650/1586, Loss: 0.2866\n",
      "Epoch: 10/20, Batch: 700/1586, Loss: 0.2626\n",
      "Epoch: 10/20, Batch: 750/1586, Loss: 0.2065\n",
      "Epoch: 10/20, Batch: 800/1586, Loss: 0.1903\n",
      "Epoch: 10/20, Batch: 850/1586, Loss: 0.1608\n",
      "Epoch: 10/20, Batch: 900/1586, Loss: 0.1722\n",
      "Epoch: 10/20, Batch: 950/1586, Loss: 0.1846\n",
      "Epoch: 10/20, Batch: 1000/1586, Loss: 0.1711\n",
      "Epoch: 10/20, Batch: 1050/1586, Loss: 0.2832\n",
      "Epoch: 10/20, Batch: 1100/1586, Loss: 0.2048\n",
      "Epoch: 10/20, Batch: 1150/1586, Loss: 0.3159\n",
      "Epoch: 10/20, Batch: 1200/1586, Loss: 0.1623\n",
      "Epoch: 10/20, Batch: 1250/1586, Loss: 0.2777\n",
      "Epoch: 10/20, Batch: 1300/1586, Loss: 0.2414\n",
      "Epoch: 10/20, Batch: 1350/1586, Loss: 0.2730\n",
      "Epoch: 10/20, Batch: 1400/1586, Loss: 0.2754\n",
      "Epoch: 10/20, Batch: 1450/1586, Loss: 0.2752\n",
      "Epoch: 10/20, Batch: 1500/1586, Loss: 0.2503\n",
      "Epoch: 10/20, Batch: 1550/1586, Loss: 0.2511\n",
      "Epoch: 10, Train Loss: 0.2412\n",
      "Epoch: 11/20, Batch: 0/1586, Loss: 0.2894\n",
      "Epoch: 11/20, Batch: 50/1586, Loss: 0.1895\n",
      "Epoch: 11/20, Batch: 100/1586, Loss: 0.2818\n",
      "Epoch: 11/20, Batch: 150/1586, Loss: 0.2266\n",
      "Epoch: 11/20, Batch: 200/1586, Loss: 0.2706\n",
      "Epoch: 11/20, Batch: 250/1586, Loss: 0.2449\n",
      "Epoch: 11/20, Batch: 300/1586, Loss: 0.2252\n",
      "Epoch: 11/20, Batch: 350/1586, Loss: 0.2112\n",
      "Epoch: 11/20, Batch: 400/1586, Loss: 0.2166\n",
      "Epoch: 11/20, Batch: 450/1586, Loss: 0.2421\n",
      "Epoch: 11/20, Batch: 500/1586, Loss: 0.2446\n",
      "Epoch: 11/20, Batch: 550/1586, Loss: 0.2748\n",
      "Epoch: 11/20, Batch: 600/1586, Loss: 0.2356\n",
      "Epoch: 11/20, Batch: 650/1586, Loss: 0.2443\n",
      "Epoch: 11/20, Batch: 700/1586, Loss: 0.3297\n",
      "Epoch: 11/20, Batch: 750/1586, Loss: 0.2196\n",
      "Epoch: 11/20, Batch: 800/1586, Loss: 0.2378\n",
      "Epoch: 11/20, Batch: 850/1586, Loss: 0.2971\n",
      "Epoch: 11/20, Batch: 900/1586, Loss: 0.1803\n",
      "Epoch: 11/20, Batch: 950/1586, Loss: 0.2462\n",
      "Epoch: 11/20, Batch: 1000/1586, Loss: 0.3457\n",
      "Epoch: 11/20, Batch: 1050/1586, Loss: 0.1922\n",
      "Epoch: 11/20, Batch: 1100/1586, Loss: 0.2368\n",
      "Epoch: 11/20, Batch: 1150/1586, Loss: 0.2307\n",
      "Epoch: 11/20, Batch: 1200/1586, Loss: 0.2296\n",
      "Epoch: 11/20, Batch: 1250/1586, Loss: 0.2596\n",
      "Epoch: 11/20, Batch: 1300/1586, Loss: 0.3112\n",
      "Epoch: 11/20, Batch: 1350/1586, Loss: 0.2617\n",
      "Epoch: 11/20, Batch: 1400/1586, Loss: 0.2873\n",
      "Epoch: 11/20, Batch: 1450/1586, Loss: 0.1753\n",
      "Epoch: 11/20, Batch: 1500/1586, Loss: 0.1844\n",
      "Epoch: 11/20, Batch: 1550/1586, Loss: 0.2267\n",
      "Epoch: 11, Train Loss: 0.2412\n",
      "Epoch: 12/20, Batch: 0/1586, Loss: 0.3718\n",
      "Epoch: 12/20, Batch: 50/1586, Loss: 0.3021\n",
      "Epoch: 12/20, Batch: 100/1586, Loss: 0.3022\n",
      "Epoch: 12/20, Batch: 150/1586, Loss: 0.1768\n",
      "Epoch: 12/20, Batch: 200/1586, Loss: 0.1847\n",
      "Epoch: 12/20, Batch: 250/1586, Loss: 0.3052\n",
      "Epoch: 12/20, Batch: 300/1586, Loss: 0.2769\n",
      "Epoch: 12/20, Batch: 350/1586, Loss: 0.2208\n",
      "Epoch: 12/20, Batch: 400/1586, Loss: 0.1820\n",
      "Epoch: 12/20, Batch: 450/1586, Loss: 0.2365\n",
      "Epoch: 12/20, Batch: 500/1586, Loss: 0.2462\n",
      "Epoch: 12/20, Batch: 550/1586, Loss: 0.3021\n",
      "Epoch: 12/20, Batch: 600/1586, Loss: 0.2790\n",
      "Epoch: 12/20, Batch: 650/1586, Loss: 0.3537\n",
      "Epoch: 12/20, Batch: 700/1586, Loss: 0.2131\n",
      "Epoch: 12/20, Batch: 750/1586, Loss: 0.2926\n",
      "Epoch: 12/20, Batch: 800/1586, Loss: 0.1499\n",
      "Epoch: 12/20, Batch: 850/1586, Loss: 0.2751\n",
      "Epoch: 12/20, Batch: 900/1586, Loss: 0.2556\n",
      "Epoch: 12/20, Batch: 950/1586, Loss: 0.2016\n",
      "Epoch: 12/20, Batch: 1000/1586, Loss: 0.3119\n",
      "Epoch: 12/20, Batch: 1050/1586, Loss: 0.1786\n",
      "Epoch: 12/20, Batch: 1100/1586, Loss: 0.1987\n",
      "Epoch: 12/20, Batch: 1150/1586, Loss: 0.2155\n",
      "Epoch: 12/20, Batch: 1200/1586, Loss: 0.3152\n",
      "Epoch: 12/20, Batch: 1250/1586, Loss: 0.2214\n",
      "Epoch: 12/20, Batch: 1300/1586, Loss: 0.3492\n",
      "Epoch: 12/20, Batch: 1350/1586, Loss: 0.2444\n",
      "Epoch: 12/20, Batch: 1400/1586, Loss: 0.2212\n",
      "Epoch: 12/20, Batch: 1450/1586, Loss: 0.2011\n",
      "Epoch: 12/20, Batch: 1500/1586, Loss: 0.2148\n",
      "Epoch: 12/20, Batch: 1550/1586, Loss: 0.1472\n",
      "Epoch: 12, Train Loss: 0.2413\n",
      "Epoch: 13/20, Batch: 0/1586, Loss: 0.2424\n",
      "Epoch: 13/20, Batch: 50/1586, Loss: 0.2534\n",
      "Epoch: 13/20, Batch: 100/1586, Loss: 0.2353\n",
      "Epoch: 13/20, Batch: 150/1586, Loss: 0.2641\n",
      "Epoch: 13/20, Batch: 200/1586, Loss: 0.2414\n",
      "Epoch: 13/20, Batch: 250/1586, Loss: 0.2972\n",
      "Epoch: 13/20, Batch: 300/1586, Loss: 0.2481\n",
      "Epoch: 13/20, Batch: 350/1586, Loss: 0.2574\n",
      "Epoch: 13/20, Batch: 400/1586, Loss: 0.2799\n",
      "Epoch: 13/20, Batch: 450/1586, Loss: 0.2069\n",
      "Epoch: 13/20, Batch: 500/1586, Loss: 0.2768\n",
      "Epoch: 13/20, Batch: 550/1586, Loss: 0.2276\n",
      "Epoch: 13/20, Batch: 600/1586, Loss: 0.2449\n",
      "Epoch: 13/20, Batch: 650/1586, Loss: 0.1980\n",
      "Epoch: 13/20, Batch: 700/1586, Loss: 0.2502\n",
      "Epoch: 13/20, Batch: 750/1586, Loss: 0.3044\n",
      "Epoch: 13/20, Batch: 800/1586, Loss: 0.2968\n",
      "Epoch: 13/20, Batch: 850/1586, Loss: 0.2794\n",
      "Epoch: 13/20, Batch: 900/1586, Loss: 0.2891\n",
      "Epoch: 13/20, Batch: 950/1586, Loss: 0.1994\n",
      "Epoch: 13/20, Batch: 1000/1586, Loss: 0.1454\n",
      "Epoch: 13/20, Batch: 1050/1586, Loss: 0.1942\n",
      "Epoch: 13/20, Batch: 1100/1586, Loss: 0.2488\n",
      "Epoch: 13/20, Batch: 1150/1586, Loss: 0.2281\n",
      "Epoch: 13/20, Batch: 1200/1586, Loss: 0.2763\n",
      "Epoch: 13/20, Batch: 1250/1586, Loss: 0.2368\n",
      "Epoch: 13/20, Batch: 1300/1586, Loss: 0.3653\n",
      "Epoch: 13/20, Batch: 1350/1586, Loss: 0.1788\n",
      "Epoch: 13/20, Batch: 1400/1586, Loss: 0.3194\n",
      "Epoch: 13/20, Batch: 1450/1586, Loss: 0.2187\n",
      "Epoch: 13/20, Batch: 1500/1586, Loss: 0.2666\n",
      "Epoch: 13/20, Batch: 1550/1586, Loss: 0.3799\n",
      "Epoch: 13, Train Loss: 0.2410\n",
      "Epoch: 14/20, Batch: 0/1586, Loss: 0.2601\n",
      "Epoch: 14/20, Batch: 50/1586, Loss: 0.2122\n",
      "Epoch: 14/20, Batch: 100/1586, Loss: 0.1962\n",
      "Epoch: 14/20, Batch: 150/1586, Loss: 0.2265\n",
      "Epoch: 14/20, Batch: 200/1586, Loss: 0.3072\n",
      "Epoch: 14/20, Batch: 250/1586, Loss: 0.3004\n",
      "Epoch: 14/20, Batch: 300/1586, Loss: 0.2539\n",
      "Epoch: 14/20, Batch: 350/1586, Loss: 0.2223\n",
      "Epoch: 14/20, Batch: 400/1586, Loss: 0.2807\n",
      "Epoch: 14/20, Batch: 450/1586, Loss: 0.2424\n",
      "Epoch: 14/20, Batch: 500/1586, Loss: 0.2714\n",
      "Epoch: 14/20, Batch: 550/1586, Loss: 0.2870\n",
      "Epoch: 14/20, Batch: 600/1586, Loss: 0.3080\n",
      "Epoch: 14/20, Batch: 650/1586, Loss: 0.2329\n",
      "Epoch: 14/20, Batch: 700/1586, Loss: 0.2041\n",
      "Epoch: 14/20, Batch: 750/1586, Loss: 0.2207\n",
      "Epoch: 14/20, Batch: 800/1586, Loss: 0.1861\n",
      "Epoch: 14/20, Batch: 850/1586, Loss: 0.2004\n",
      "Epoch: 14/20, Batch: 900/1586, Loss: 0.2319\n",
      "Epoch: 14/20, Batch: 950/1586, Loss: 0.2855\n",
      "Epoch: 14/20, Batch: 1000/1586, Loss: 0.2644\n",
      "Epoch: 14/20, Batch: 1050/1586, Loss: 0.2631\n",
      "Epoch: 14/20, Batch: 1100/1586, Loss: 0.2584\n",
      "Epoch: 14/20, Batch: 1150/1586, Loss: 0.2933\n",
      "Epoch: 14/20, Batch: 1200/1586, Loss: 0.2203\n",
      "Epoch: 14/20, Batch: 1250/1586, Loss: 0.3261\n",
      "Epoch: 14/20, Batch: 1300/1586, Loss: 0.2545\n",
      "Epoch: 14/20, Batch: 1350/1586, Loss: 0.2192\n",
      "Epoch: 14/20, Batch: 1400/1586, Loss: 0.2447\n",
      "Epoch: 14/20, Batch: 1450/1586, Loss: 0.2230\n",
      "Epoch: 14/20, Batch: 1500/1586, Loss: 0.2153\n",
      "Epoch: 14/20, Batch: 1550/1586, Loss: 0.2458\n",
      "Epoch: 14, Train Loss: 0.2405\n",
      "Epoch: 15/20, Batch: 0/1586, Loss: 0.2529\n",
      "Epoch: 15/20, Batch: 50/1586, Loss: 0.2827\n",
      "Epoch: 15/20, Batch: 100/1586, Loss: 0.2338\n",
      "Epoch: 15/20, Batch: 150/1586, Loss: 0.3404\n",
      "Epoch: 15/20, Batch: 200/1586, Loss: 0.2296\n",
      "Epoch: 15/20, Batch: 250/1586, Loss: 0.2400\n",
      "Epoch: 15/20, Batch: 300/1586, Loss: 0.1809\n",
      "Epoch: 15/20, Batch: 350/1586, Loss: 0.1565\n",
      "Epoch: 15/20, Batch: 400/1586, Loss: 0.1305\n",
      "Epoch: 15/20, Batch: 450/1586, Loss: 0.2870\n",
      "Epoch: 15/20, Batch: 500/1586, Loss: 0.2631\n",
      "Epoch: 15/20, Batch: 550/1586, Loss: 0.2837\n",
      "Epoch: 15/20, Batch: 600/1586, Loss: 0.3346\n",
      "Epoch: 15/20, Batch: 650/1586, Loss: 0.3000\n",
      "Epoch: 15/20, Batch: 700/1586, Loss: 0.2268\n",
      "Epoch: 15/20, Batch: 750/1586, Loss: 0.3134\n",
      "Epoch: 15/20, Batch: 800/1586, Loss: 0.2290\n",
      "Epoch: 15/20, Batch: 850/1586, Loss: 0.3127\n",
      "Epoch: 15/20, Batch: 900/1586, Loss: 0.2663\n",
      "Epoch: 15/20, Batch: 950/1586, Loss: 0.1743\n",
      "Epoch: 15/20, Batch: 1000/1586, Loss: 0.2522\n",
      "Epoch: 15/20, Batch: 1050/1586, Loss: 0.1495\n",
      "Epoch: 15/20, Batch: 1100/1586, Loss: 0.2381\n",
      "Epoch: 15/20, Batch: 1150/1586, Loss: 0.1900\n",
      "Epoch: 15/20, Batch: 1200/1586, Loss: 0.3040\n",
      "Epoch: 15/20, Batch: 1250/1586, Loss: 0.2342\n",
      "Epoch: 15/20, Batch: 1300/1586, Loss: 0.2404\n",
      "Epoch: 15/20, Batch: 1350/1586, Loss: 0.2717\n",
      "Epoch: 15/20, Batch: 1400/1586, Loss: 0.3199\n",
      "Epoch: 15/20, Batch: 1450/1586, Loss: 0.2976\n",
      "Epoch: 15/20, Batch: 1500/1586, Loss: 0.2626\n",
      "Epoch: 15/20, Batch: 1550/1586, Loss: 0.2662\n",
      "Epoch: 15, Train Loss: 0.2405\n",
      "Epoch: 16/20, Batch: 0/1586, Loss: 0.3191\n",
      "Epoch: 16/20, Batch: 50/1586, Loss: 0.2687\n",
      "Epoch: 16/20, Batch: 100/1586, Loss: 0.2616\n",
      "Epoch: 16/20, Batch: 150/1586, Loss: 0.3796\n",
      "Epoch: 16/20, Batch: 200/1586, Loss: 0.2524\n",
      "Epoch: 16/20, Batch: 250/1586, Loss: 0.2226\n",
      "Epoch: 16/20, Batch: 300/1586, Loss: 0.2196\n",
      "Epoch: 16/20, Batch: 350/1586, Loss: 0.2452\n",
      "Epoch: 16/20, Batch: 400/1586, Loss: 0.3287\n",
      "Epoch: 16/20, Batch: 450/1586, Loss: 0.2817\n",
      "Epoch: 16/20, Batch: 500/1586, Loss: 0.2519\n",
      "Epoch: 16/20, Batch: 550/1586, Loss: 0.2476\n",
      "Epoch: 16/20, Batch: 600/1586, Loss: 0.2414\n",
      "Epoch: 16/20, Batch: 650/1586, Loss: 0.2236\n",
      "Epoch: 16/20, Batch: 700/1586, Loss: 0.2873\n",
      "Epoch: 16/20, Batch: 750/1586, Loss: 0.2236\n",
      "Epoch: 16/20, Batch: 800/1586, Loss: 0.2503\n",
      "Epoch: 16/20, Batch: 850/1586, Loss: 0.3012\n",
      "Epoch: 16/20, Batch: 900/1586, Loss: 0.1983\n",
      "Epoch: 16/20, Batch: 950/1586, Loss: 0.2391\n",
      "Epoch: 16/20, Batch: 1000/1586, Loss: 0.2431\n",
      "Epoch: 16/20, Batch: 1050/1586, Loss: 0.3201\n",
      "Epoch: 16/20, Batch: 1100/1586, Loss: 0.2898\n",
      "Epoch: 16/20, Batch: 1150/1586, Loss: 0.2398\n",
      "Epoch: 16/20, Batch: 1200/1586, Loss: 0.1925\n",
      "Epoch: 16/20, Batch: 1250/1586, Loss: 0.2567\n",
      "Epoch: 16/20, Batch: 1300/1586, Loss: 0.2429\n",
      "Epoch: 16/20, Batch: 1350/1586, Loss: 0.1738\n",
      "Epoch: 16/20, Batch: 1400/1586, Loss: 0.2076\n",
      "Epoch: 16/20, Batch: 1450/1586, Loss: 0.2395\n",
      "Epoch: 16/20, Batch: 1500/1586, Loss: 0.2928\n",
      "Epoch: 16/20, Batch: 1550/1586, Loss: 0.3192\n",
      "Epoch: 16, Train Loss: 0.2405\n",
      "Epoch: 17/20, Batch: 0/1586, Loss: 0.3178\n",
      "Epoch: 17/20, Batch: 50/1586, Loss: 0.1975\n",
      "Epoch: 17/20, Batch: 100/1586, Loss: 0.2509\n",
      "Epoch: 17/20, Batch: 150/1586, Loss: 0.3013\n",
      "Epoch: 17/20, Batch: 200/1586, Loss: 0.2071\n",
      "Epoch: 17/20, Batch: 250/1586, Loss: 0.2437\n",
      "Epoch: 17/20, Batch: 300/1586, Loss: 0.3783\n",
      "Epoch: 17/20, Batch: 350/1586, Loss: 0.2291\n",
      "Epoch: 17/20, Batch: 400/1586, Loss: 0.2110\n",
      "Epoch: 17/20, Batch: 450/1586, Loss: 0.2671\n",
      "Epoch: 17/20, Batch: 500/1586, Loss: 0.2022\n",
      "Epoch: 17/20, Batch: 550/1586, Loss: 0.2234\n",
      "Epoch: 17/20, Batch: 600/1586, Loss: 0.1828\n",
      "Epoch: 17/20, Batch: 650/1586, Loss: 0.2339\n",
      "Epoch: 17/20, Batch: 700/1586, Loss: 0.1727\n",
      "Epoch: 17/20, Batch: 750/1586, Loss: 0.2488\n",
      "Epoch: 17/20, Batch: 800/1586, Loss: 0.2700\n",
      "Epoch: 17/20, Batch: 850/1586, Loss: 0.1357\n",
      "Epoch: 17/20, Batch: 900/1586, Loss: 0.1799\n",
      "Epoch: 17/20, Batch: 950/1586, Loss: 0.2949\n",
      "Epoch: 17/20, Batch: 1000/1586, Loss: 0.2632\n",
      "Epoch: 17/20, Batch: 1050/1586, Loss: 0.2217\n",
      "Epoch: 17/20, Batch: 1100/1586, Loss: 0.3133\n",
      "Epoch: 17/20, Batch: 1150/1586, Loss: 0.2271\n",
      "Epoch: 17/20, Batch: 1200/1586, Loss: 0.2116\n",
      "Epoch: 17/20, Batch: 1250/1586, Loss: 0.2676\n",
      "Epoch: 17/20, Batch: 1300/1586, Loss: 0.1515\n",
      "Epoch: 17/20, Batch: 1350/1586, Loss: 0.2505\n",
      "Epoch: 17/20, Batch: 1400/1586, Loss: 0.2242\n",
      "Epoch: 17/20, Batch: 1450/1586, Loss: 0.2701\n",
      "Epoch: 17/20, Batch: 1500/1586, Loss: 0.3534\n",
      "Epoch: 17/20, Batch: 1550/1586, Loss: 0.2435\n",
      "Epoch: 17, Train Loss: 0.2402\n",
      "Epoch: 18/20, Batch: 0/1586, Loss: 0.2344\n",
      "Epoch: 18/20, Batch: 50/1586, Loss: 0.2478\n",
      "Epoch: 18/20, Batch: 100/1586, Loss: 0.2484\n",
      "Epoch: 18/20, Batch: 150/1586, Loss: 0.1932\n",
      "Epoch: 18/20, Batch: 200/1586, Loss: 0.1939\n",
      "Epoch: 18/20, Batch: 250/1586, Loss: 0.2932\n",
      "Epoch: 18/20, Batch: 300/1586, Loss: 0.1700\n",
      "Epoch: 18/20, Batch: 350/1586, Loss: 0.2709\n",
      "Epoch: 18/20, Batch: 400/1586, Loss: 0.1644\n",
      "Epoch: 18/20, Batch: 450/1586, Loss: 0.1818\n",
      "Epoch: 18/20, Batch: 500/1586, Loss: 0.2567\n",
      "Epoch: 18/20, Batch: 550/1586, Loss: 0.2852\n",
      "Epoch: 18/20, Batch: 600/1586, Loss: 0.2595\n",
      "Epoch: 18/20, Batch: 650/1586, Loss: 0.3082\n",
      "Epoch: 18/20, Batch: 700/1586, Loss: 0.2044\n",
      "Epoch: 18/20, Batch: 750/1586, Loss: 0.2853\n",
      "Epoch: 18/20, Batch: 800/1586, Loss: 0.2805\n",
      "Epoch: 18/20, Batch: 850/1586, Loss: 0.2378\n",
      "Epoch: 18/20, Batch: 900/1586, Loss: 0.1735\n",
      "Epoch: 18/20, Batch: 950/1586, Loss: 0.2264\n",
      "Epoch: 18/20, Batch: 1000/1586, Loss: 0.2979\n",
      "Epoch: 18/20, Batch: 1050/1586, Loss: 0.1947\n",
      "Epoch: 18/20, Batch: 1100/1586, Loss: 0.2387\n",
      "Epoch: 18/20, Batch: 1150/1586, Loss: 0.2861\n",
      "Epoch: 18/20, Batch: 1200/1586, Loss: 0.2980\n",
      "Epoch: 18/20, Batch: 1250/1586, Loss: 0.2907\n",
      "Epoch: 18/20, Batch: 1300/1586, Loss: 0.2545\n",
      "Epoch: 18/20, Batch: 1350/1586, Loss: 0.2732\n",
      "Epoch: 18/20, Batch: 1400/1586, Loss: 0.2067\n",
      "Epoch: 18/20, Batch: 1450/1586, Loss: 0.1985\n",
      "Epoch: 18/20, Batch: 1500/1586, Loss: 0.2351\n",
      "Epoch: 18/20, Batch: 1550/1586, Loss: 0.1906\n",
      "Epoch: 18, Train Loss: 0.2406\n",
      "Epoch: 19/20, Batch: 0/1586, Loss: 0.2310\n",
      "Epoch: 19/20, Batch: 50/1586, Loss: 0.2086\n",
      "Epoch: 19/20, Batch: 100/1586, Loss: 0.2588\n",
      "Epoch: 19/20, Batch: 150/1586, Loss: 0.2643\n",
      "Epoch: 19/20, Batch: 200/1586, Loss: 0.2531\n",
      "Epoch: 19/20, Batch: 250/1586, Loss: 0.2879\n",
      "Epoch: 19/20, Batch: 300/1586, Loss: 0.2347\n",
      "Epoch: 19/20, Batch: 350/1586, Loss: 0.1858\n",
      "Epoch: 19/20, Batch: 400/1586, Loss: 0.2168\n",
      "Epoch: 19/20, Batch: 450/1586, Loss: 0.2243\n",
      "Epoch: 19/20, Batch: 500/1586, Loss: 0.2529\n",
      "Epoch: 19/20, Batch: 550/1586, Loss: 0.3195\n",
      "Epoch: 19/20, Batch: 600/1586, Loss: 0.2544\n",
      "Epoch: 19/20, Batch: 650/1586, Loss: 0.1862\n",
      "Epoch: 19/20, Batch: 700/1586, Loss: 0.2697\n",
      "Epoch: 19/20, Batch: 750/1586, Loss: 0.2706\n",
      "Epoch: 19/20, Batch: 800/1586, Loss: 0.2847\n",
      "Epoch: 19/20, Batch: 850/1586, Loss: 0.3008\n",
      "Epoch: 19/20, Batch: 900/1586, Loss: 0.1913\n",
      "Epoch: 19/20, Batch: 950/1586, Loss: 0.1801\n",
      "Epoch: 19/20, Batch: 1000/1586, Loss: 0.2300\n",
      "Epoch: 19/20, Batch: 1050/1586, Loss: 0.2547\n",
      "Epoch: 19/20, Batch: 1100/1586, Loss: 0.2619\n",
      "Epoch: 19/20, Batch: 1150/1586, Loss: 0.2123\n",
      "Epoch: 19/20, Batch: 1200/1586, Loss: 0.3100\n",
      "Epoch: 19/20, Batch: 1250/1586, Loss: 0.1763\n",
      "Epoch: 19/20, Batch: 1300/1586, Loss: 0.2200\n",
      "Epoch: 19/20, Batch: 1350/1586, Loss: 0.2372\n",
      "Epoch: 19/20, Batch: 1400/1586, Loss: 0.3249\n",
      "Epoch: 19/20, Batch: 1450/1586, Loss: 0.2038\n",
      "Epoch: 19/20, Batch: 1500/1586, Loss: 0.1572\n",
      "Epoch: 19/20, Batch: 1550/1586, Loss: 0.2029\n",
      "Epoch: 19, Train Loss: 0.2400\n",
      "Epoch: 20/20, Batch: 0/1586, Loss: 0.2605\n",
      "Epoch: 20/20, Batch: 50/1586, Loss: 0.2207\n",
      "Epoch: 20/20, Batch: 100/1586, Loss: 0.2613\n",
      "Epoch: 20/20, Batch: 150/1586, Loss: 0.2638\n",
      "Epoch: 20/20, Batch: 200/1586, Loss: 0.2203\n",
      "Epoch: 20/20, Batch: 250/1586, Loss: 0.2660\n",
      "Epoch: 20/20, Batch: 300/1586, Loss: 0.2645\n",
      "Epoch: 20/20, Batch: 350/1586, Loss: 0.2502\n",
      "Epoch: 20/20, Batch: 400/1586, Loss: 0.1990\n",
      "Epoch: 20/20, Batch: 450/1586, Loss: 0.2202\n",
      "Epoch: 20/20, Batch: 500/1586, Loss: 0.1798\n",
      "Epoch: 20/20, Batch: 550/1586, Loss: 0.3300\n",
      "Epoch: 20/20, Batch: 600/1586, Loss: 0.2051\n",
      "Epoch: 20/20, Batch: 650/1586, Loss: 0.2516\n",
      "Epoch: 20/20, Batch: 700/1586, Loss: 0.2849\n",
      "Epoch: 20/20, Batch: 750/1586, Loss: 0.1936\n",
      "Epoch: 20/20, Batch: 800/1586, Loss: 0.2112\n",
      "Epoch: 20/20, Batch: 850/1586, Loss: 0.2895\n",
      "Epoch: 20/20, Batch: 900/1586, Loss: 0.2098\n",
      "Epoch: 20/20, Batch: 950/1586, Loss: 0.2270\n",
      "Epoch: 20/20, Batch: 1000/1586, Loss: 0.2751\n",
      "Epoch: 20/20, Batch: 1050/1586, Loss: 0.2503\n",
      "Epoch: 20/20, Batch: 1100/1586, Loss: 0.2929\n",
      "Epoch: 20/20, Batch: 1150/1586, Loss: 0.2008\n",
      "Epoch: 20/20, Batch: 1200/1586, Loss: 0.2555\n",
      "Epoch: 20/20, Batch: 1250/1586, Loss: 0.2682\n",
      "Epoch: 20/20, Batch: 1300/1586, Loss: 0.2465\n",
      "Epoch: 20/20, Batch: 1350/1586, Loss: 0.2529\n",
      "Epoch: 20/20, Batch: 1400/1586, Loss: 0.2620\n",
      "Epoch: 20/20, Batch: 1450/1586, Loss: 0.3539\n",
      "Epoch: 20/20, Batch: 1500/1586, Loss: 0.1674\n",
      "Epoch: 20/20, Batch: 1550/1586, Loss: 0.2190\n",
      "Epoch: 20, Train Loss: 0.2400\n",
      "Training finished!\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_aucs = []\n",
    "\n",
    "print(\"\\nStarting training...\")\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(data)\n",
    "    \n",
    "        target = target.float()\n",
    "        output = output.float()    \n",
    "        loss = criterion(output, target)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if batch_idx % 50 == 0:\n",
    "            print(f'Epoch: {epoch+1}/{num_epochs}, Batch: {batch_idx}/{len(train_loader)}, Loss: {loss.item():.4f}')\n",
    "    \n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1}, Train Loss: {avg_train_loss:.4f}')\n",
    "\n",
    "print('Training finished!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.01      0.01     45997\n",
      "         1.0       0.09      0.99      0.17      4739\n",
      "\n",
      "    accuracy                           0.10     50736\n",
      "   macro avg       0.50      0.50      0.09     50736\n",
      "weighted avg       0.83      0.10      0.03     50736\n",
      "\n",
      "\n",
      "Confusion Matrix:\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.eval()\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        output = model(data)\n",
    "        \n",
    "        y_true.extend(target.cpu().numpy())\n",
    "        y_pred.extend((output > 0.5).cpu().numpy())\n",
    "\n",
    "y_true = np.array(y_true)\n",
    "y_pred = np.array(y_pred)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred))\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Weight-Balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class frequencies: [0.9058149749966211, 0.09418502500337883]\n",
      "HeartDiseaseMLPClassifier(\n",
      "  (mlp): MLP(\n",
      "    (0): Linear(in_features=21, out_features=128, bias=True)\n",
      "    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.3, inplace=False)\n",
      "    (4): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (5): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU()\n",
      "    (7): Dropout(p=0.3, inplace=False)\n",
      "    (8): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (9): Dropout(p=0.3, inplace=False)\n",
      "  )\n",
      "  (classifier): Linear(in_features=32, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class_0_frequency = np.sum(y_train == 0) / len(y_train)\n",
    "class_1_frequency = np.sum(y_train == 1) / len(y_train)\n",
    "class_frequencies = [class_0_frequency, class_1_frequency]\n",
    "print(f\"Class frequencies: {class_frequencies}\")\n",
    "\n",
    "input_features = X.shape[1]  \n",
    "\n",
    "weighted_model = HeartDiseaseMLPClassifier(input_size=input_features, class_frequencies=class_frequencies).to(device)\n",
    "print(weighted_model)\n",
    "\n",
    "optimizer = optim.Adam(weighted_model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training...\n",
      "Epoch: 1/20, Batch: 0/1586, Loss: 1.1011\n",
      "Epoch: 1/20, Batch: 50/1586, Loss: 1.0249\n",
      "Epoch: 1/20, Batch: 100/1586, Loss: 1.2473\n",
      "Epoch: 1/20, Batch: 150/1586, Loss: 1.0338\n",
      "Epoch: 1/20, Batch: 200/1586, Loss: 1.0753\n",
      "Epoch: 1/20, Batch: 250/1586, Loss: 0.9725\n",
      "Epoch: 1/20, Batch: 300/1586, Loss: 1.4831\n",
      "Epoch: 1/20, Batch: 350/1586, Loss: 0.7219\n",
      "Epoch: 1/20, Batch: 400/1586, Loss: 0.7772\n",
      "Epoch: 1/20, Batch: 450/1586, Loss: 0.8207\n",
      "Epoch: 1/20, Batch: 500/1586, Loss: 1.5171\n",
      "Epoch: 1/20, Batch: 550/1586, Loss: 0.9059\n",
      "Epoch: 1/20, Batch: 600/1586, Loss: 1.1192\n",
      "Epoch: 1/20, Batch: 650/1586, Loss: 0.7311\n",
      "Epoch: 1/20, Batch: 700/1586, Loss: 0.7898\n",
      "Epoch: 1/20, Batch: 750/1586, Loss: 1.2886\n",
      "Epoch: 1/20, Batch: 800/1586, Loss: 1.0831\n",
      "Epoch: 1/20, Batch: 850/1586, Loss: 0.8187\n",
      "Epoch: 1/20, Batch: 900/1586, Loss: 0.8784\n",
      "Epoch: 1/20, Batch: 950/1586, Loss: 1.0067\n",
      "Epoch: 1/20, Batch: 1000/1586, Loss: 0.7972\n",
      "Epoch: 1/20, Batch: 1050/1586, Loss: 0.9218\n",
      "Epoch: 1/20, Batch: 1100/1586, Loss: 0.9519\n",
      "Epoch: 1/20, Batch: 1150/1586, Loss: 1.0466\n",
      "Epoch: 1/20, Batch: 1200/1586, Loss: 1.2886\n",
      "Epoch: 1/20, Batch: 1250/1586, Loss: 0.8217\n",
      "Epoch: 1/20, Batch: 1300/1586, Loss: 1.0282\n",
      "Epoch: 1/20, Batch: 1350/1586, Loss: 0.8206\n",
      "Epoch: 1/20, Batch: 1400/1586, Loss: 1.1405\n",
      "Epoch: 1/20, Batch: 1450/1586, Loss: 1.4311\n",
      "Epoch: 1/20, Batch: 1500/1586, Loss: 0.9081\n",
      "Epoch: 1/20, Batch: 1550/1586, Loss: 1.4122\n",
      "Epoch: 1, Train Loss: 1.0176\n",
      "Epoch: 2/20, Batch: 0/1586, Loss: 0.8075\n",
      "Epoch: 2/20, Batch: 50/1586, Loss: 1.2648\n",
      "Epoch: 2/20, Batch: 100/1586, Loss: 0.9022\n",
      "Epoch: 2/20, Batch: 150/1586, Loss: 1.4484\n",
      "Epoch: 2/20, Batch: 200/1586, Loss: 0.6524\n",
      "Epoch: 2/20, Batch: 250/1586, Loss: 1.0187\n",
      "Epoch: 2/20, Batch: 300/1586, Loss: 0.6875\n",
      "Epoch: 2/20, Batch: 350/1586, Loss: 0.8405\n",
      "Epoch: 2/20, Batch: 400/1586, Loss: 1.2417\n",
      "Epoch: 2/20, Batch: 450/1586, Loss: 0.8614\n",
      "Epoch: 2/20, Batch: 500/1586, Loss: 1.3819\n",
      "Epoch: 2/20, Batch: 550/1586, Loss: 0.9287\n",
      "Epoch: 2/20, Batch: 600/1586, Loss: 1.0372\n",
      "Epoch: 2/20, Batch: 650/1586, Loss: 0.7527\n",
      "Epoch: 2/20, Batch: 700/1586, Loss: 1.1214\n",
      "Epoch: 2/20, Batch: 750/1586, Loss: 0.7582\n",
      "Epoch: 2/20, Batch: 800/1586, Loss: 1.0925\n",
      "Epoch: 2/20, Batch: 850/1586, Loss: 0.9792\n",
      "Epoch: 2/20, Batch: 900/1586, Loss: 0.9297\n",
      "Epoch: 2/20, Batch: 950/1586, Loss: 0.8210\n",
      "Epoch: 2/20, Batch: 1000/1586, Loss: 0.8607\n",
      "Epoch: 2/20, Batch: 1050/1586, Loss: 1.0745\n",
      "Epoch: 2/20, Batch: 1100/1586, Loss: 1.1300\n",
      "Epoch: 2/20, Batch: 1150/1586, Loss: 1.0710\n",
      "Epoch: 2/20, Batch: 1200/1586, Loss: 0.9152\n",
      "Epoch: 2/20, Batch: 1250/1586, Loss: 1.3791\n",
      "Epoch: 2/20, Batch: 1300/1586, Loss: 0.9796\n",
      "Epoch: 2/20, Batch: 1350/1586, Loss: 1.1816\n",
      "Epoch: 2/20, Batch: 1400/1586, Loss: 0.9275\n",
      "Epoch: 2/20, Batch: 1450/1586, Loss: 1.0049\n",
      "Epoch: 2/20, Batch: 1500/1586, Loss: 0.9304\n",
      "Epoch: 2/20, Batch: 1550/1586, Loss: 0.8352\n",
      "Epoch: 2, Train Loss: 0.9864\n",
      "Epoch: 3/20, Batch: 0/1586, Loss: 0.9907\n",
      "Epoch: 3/20, Batch: 50/1586, Loss: 1.2776\n",
      "Epoch: 3/20, Batch: 100/1586, Loss: 0.9678\n",
      "Epoch: 3/20, Batch: 150/1586, Loss: 0.7189\n",
      "Epoch: 3/20, Batch: 200/1586, Loss: 0.7538\n",
      "Epoch: 3/20, Batch: 250/1586, Loss: 0.8410\n",
      "Epoch: 3/20, Batch: 300/1586, Loss: 0.9073\n",
      "Epoch: 3/20, Batch: 350/1586, Loss: 1.1532\n",
      "Epoch: 3/20, Batch: 400/1586, Loss: 0.8755\n",
      "Epoch: 3/20, Batch: 450/1586, Loss: 1.3080\n",
      "Epoch: 3/20, Batch: 500/1586, Loss: 0.8346\n",
      "Epoch: 3/20, Batch: 550/1586, Loss: 0.8087\n",
      "Epoch: 3/20, Batch: 600/1586, Loss: 0.7371\n",
      "Epoch: 3/20, Batch: 650/1586, Loss: 0.8877\n",
      "Epoch: 3/20, Batch: 700/1586, Loss: 0.8443\n",
      "Epoch: 3/20, Batch: 750/1586, Loss: 1.1990\n",
      "Epoch: 3/20, Batch: 800/1586, Loss: 1.0167\n",
      "Epoch: 3/20, Batch: 850/1586, Loss: 1.1858\n",
      "Epoch: 3/20, Batch: 900/1586, Loss: 1.1720\n",
      "Epoch: 3/20, Batch: 950/1586, Loss: 0.8571\n",
      "Epoch: 3/20, Batch: 1000/1586, Loss: 1.3120\n",
      "Epoch: 3/20, Batch: 1050/1586, Loss: 0.8158\n",
      "Epoch: 3/20, Batch: 1100/1586, Loss: 0.8477\n",
      "Epoch: 3/20, Batch: 1150/1586, Loss: 0.9809\n",
      "Epoch: 3/20, Batch: 1200/1586, Loss: 0.7210\n",
      "Epoch: 3/20, Batch: 1250/1586, Loss: 1.1482\n",
      "Epoch: 3/20, Batch: 1300/1586, Loss: 0.8897\n",
      "Epoch: 3/20, Batch: 1350/1586, Loss: 1.1878\n",
      "Epoch: 3/20, Batch: 1400/1586, Loss: 1.4762\n",
      "Epoch: 3/20, Batch: 1450/1586, Loss: 1.0769\n",
      "Epoch: 3/20, Batch: 1500/1586, Loss: 0.9602\n",
      "Epoch: 3/20, Batch: 1550/1586, Loss: 1.2208\n",
      "Epoch: 3, Train Loss: 0.9792\n",
      "Epoch: 4/20, Batch: 0/1586, Loss: 0.9696\n",
      "Epoch: 4/20, Batch: 50/1586, Loss: 0.7143\n",
      "Epoch: 4/20, Batch: 100/1586, Loss: 0.9380\n",
      "Epoch: 4/20, Batch: 150/1586, Loss: 0.9396\n",
      "Epoch: 4/20, Batch: 200/1586, Loss: 0.9987\n",
      "Epoch: 4/20, Batch: 250/1586, Loss: 0.8840\n",
      "Epoch: 4/20, Batch: 300/1586, Loss: 1.0271\n",
      "Epoch: 4/20, Batch: 350/1586, Loss: 0.9437\n",
      "Epoch: 4/20, Batch: 400/1586, Loss: 0.6368\n",
      "Epoch: 4/20, Batch: 450/1586, Loss: 1.2400\n",
      "Epoch: 4/20, Batch: 500/1586, Loss: 0.8693\n",
      "Epoch: 4/20, Batch: 550/1586, Loss: 1.0506\n",
      "Epoch: 4/20, Batch: 600/1586, Loss: 0.7481\n",
      "Epoch: 4/20, Batch: 650/1586, Loss: 0.8510\n",
      "Epoch: 4/20, Batch: 700/1586, Loss: 0.8172\n",
      "Epoch: 4/20, Batch: 750/1586, Loss: 1.0357\n",
      "Epoch: 4/20, Batch: 800/1586, Loss: 0.7512\n",
      "Epoch: 4/20, Batch: 850/1586, Loss: 0.9160\n",
      "Epoch: 4/20, Batch: 900/1586, Loss: 0.9980\n",
      "Epoch: 4/20, Batch: 950/1586, Loss: 0.7843\n",
      "Epoch: 4/20, Batch: 1000/1586, Loss: 0.9369\n",
      "Epoch: 4/20, Batch: 1050/1586, Loss: 0.8998\n",
      "Epoch: 4/20, Batch: 1100/1586, Loss: 0.9837\n",
      "Epoch: 4/20, Batch: 1150/1586, Loss: 1.0001\n",
      "Epoch: 4/20, Batch: 1200/1586, Loss: 1.2091\n",
      "Epoch: 4/20, Batch: 1250/1586, Loss: 1.0043\n",
      "Epoch: 4/20, Batch: 1300/1586, Loss: 1.3041\n",
      "Epoch: 4/20, Batch: 1350/1586, Loss: 0.8836\n",
      "Epoch: 4/20, Batch: 1400/1586, Loss: 0.6931\n",
      "Epoch: 4/20, Batch: 1450/1586, Loss: 0.6708\n",
      "Epoch: 4/20, Batch: 1500/1586, Loss: 0.7756\n",
      "Epoch: 4/20, Batch: 1550/1586, Loss: 1.0062\n",
      "Epoch: 4, Train Loss: 0.9763\n",
      "Epoch: 5/20, Batch: 0/1586, Loss: 1.3732\n",
      "Epoch: 5/20, Batch: 50/1586, Loss: 0.9574\n",
      "Epoch: 5/20, Batch: 100/1586, Loss: 0.8337\n",
      "Epoch: 5/20, Batch: 150/1586, Loss: 1.0529\n",
      "Epoch: 5/20, Batch: 200/1586, Loss: 0.8501\n",
      "Epoch: 5/20, Batch: 250/1586, Loss: 0.9742\n",
      "Epoch: 5/20, Batch: 300/1586, Loss: 0.8475\n",
      "Epoch: 5/20, Batch: 350/1586, Loss: 0.8247\n",
      "Epoch: 5/20, Batch: 400/1586, Loss: 1.0451\n",
      "Epoch: 5/20, Batch: 450/1586, Loss: 1.1120\n",
      "Epoch: 5/20, Batch: 500/1586, Loss: 1.8618\n",
      "Epoch: 5/20, Batch: 550/1586, Loss: 0.7620\n",
      "Epoch: 5/20, Batch: 600/1586, Loss: 1.2740\n",
      "Epoch: 5/20, Batch: 650/1586, Loss: 1.0535\n",
      "Epoch: 5/20, Batch: 700/1586, Loss: 0.9335\n",
      "Epoch: 5/20, Batch: 750/1586, Loss: 1.1050\n",
      "Epoch: 5/20, Batch: 800/1586, Loss: 0.8550\n",
      "Epoch: 5/20, Batch: 850/1586, Loss: 0.8480\n",
      "Epoch: 5/20, Batch: 900/1586, Loss: 1.1729\n",
      "Epoch: 5/20, Batch: 950/1586, Loss: 1.3963\n",
      "Epoch: 5/20, Batch: 1000/1586, Loss: 1.1816\n",
      "Epoch: 5/20, Batch: 1050/1586, Loss: 0.7548\n",
      "Epoch: 5/20, Batch: 1100/1586, Loss: 0.8194\n",
      "Epoch: 5/20, Batch: 1150/1586, Loss: 1.0046\n",
      "Epoch: 5/20, Batch: 1200/1586, Loss: 0.9340\n",
      "Epoch: 5/20, Batch: 1250/1586, Loss: 1.1951\n",
      "Epoch: 5/20, Batch: 1300/1586, Loss: 1.0215\n",
      "Epoch: 5/20, Batch: 1350/1586, Loss: 1.1230\n",
      "Epoch: 5/20, Batch: 1400/1586, Loss: 0.9930\n",
      "Epoch: 5/20, Batch: 1450/1586, Loss: 1.1996\n",
      "Epoch: 5/20, Batch: 1500/1586, Loss: 1.4195\n",
      "Epoch: 5/20, Batch: 1550/1586, Loss: 1.2673\n",
      "Epoch: 5, Train Loss: 0.9761\n",
      "Epoch: 6/20, Batch: 0/1586, Loss: 0.8094\n",
      "Epoch: 6/20, Batch: 50/1586, Loss: 0.9336\n",
      "Epoch: 6/20, Batch: 100/1586, Loss: 1.3070\n",
      "Epoch: 6/20, Batch: 150/1586, Loss: 1.1936\n",
      "Epoch: 6/20, Batch: 200/1586, Loss: 0.9905\n",
      "Epoch: 6/20, Batch: 250/1586, Loss: 1.0086\n",
      "Epoch: 6/20, Batch: 300/1586, Loss: 0.7965\n",
      "Epoch: 6/20, Batch: 350/1586, Loss: 0.8360\n",
      "Epoch: 6/20, Batch: 400/1586, Loss: 0.8868\n",
      "Epoch: 6/20, Batch: 450/1586, Loss: 1.3993\n",
      "Epoch: 6/20, Batch: 500/1586, Loss: 1.2335\n",
      "Epoch: 6/20, Batch: 550/1586, Loss: 1.2210\n",
      "Epoch: 6/20, Batch: 600/1586, Loss: 0.9510\n",
      "Epoch: 6/20, Batch: 650/1586, Loss: 0.8920\n",
      "Epoch: 6/20, Batch: 700/1586, Loss: 1.0524\n",
      "Epoch: 6/20, Batch: 750/1586, Loss: 1.1978\n",
      "Epoch: 6/20, Batch: 800/1586, Loss: 0.8124\n",
      "Epoch: 6/20, Batch: 850/1586, Loss: 0.8915\n",
      "Epoch: 6/20, Batch: 900/1586, Loss: 1.2237\n",
      "Epoch: 6/20, Batch: 950/1586, Loss: 0.8627\n",
      "Epoch: 6/20, Batch: 1000/1586, Loss: 1.1567\n",
      "Epoch: 6/20, Batch: 1050/1586, Loss: 1.3207\n",
      "Epoch: 6/20, Batch: 1100/1586, Loss: 0.7745\n",
      "Epoch: 6/20, Batch: 1150/1586, Loss: 0.8968\n",
      "Epoch: 6/20, Batch: 1200/1586, Loss: 0.8357\n",
      "Epoch: 6/20, Batch: 1250/1586, Loss: 0.6840\n",
      "Epoch: 6/20, Batch: 1300/1586, Loss: 0.8516\n",
      "Epoch: 6/20, Batch: 1350/1586, Loss: 1.1594\n",
      "Epoch: 6/20, Batch: 1400/1586, Loss: 1.0375\n",
      "Epoch: 6/20, Batch: 1450/1586, Loss: 0.8411\n",
      "Epoch: 6/20, Batch: 1500/1586, Loss: 0.7250\n",
      "Epoch: 6/20, Batch: 1550/1586, Loss: 1.2538\n",
      "Epoch: 6, Train Loss: 0.9727\n",
      "Epoch: 7/20, Batch: 0/1586, Loss: 0.8532\n",
      "Epoch: 7/20, Batch: 50/1586, Loss: 0.9090\n",
      "Epoch: 7/20, Batch: 100/1586, Loss: 1.0698\n",
      "Epoch: 7/20, Batch: 150/1586, Loss: 0.8677\n",
      "Epoch: 7/20, Batch: 200/1586, Loss: 0.7198\n",
      "Epoch: 7/20, Batch: 250/1586, Loss: 1.0585\n",
      "Epoch: 7/20, Batch: 300/1586, Loss: 0.8463\n",
      "Epoch: 7/20, Batch: 350/1586, Loss: 1.1512\n",
      "Epoch: 7/20, Batch: 400/1586, Loss: 0.9280\n",
      "Epoch: 7/20, Batch: 450/1586, Loss: 0.7916\n",
      "Epoch: 7/20, Batch: 500/1586, Loss: 0.9300\n",
      "Epoch: 7/20, Batch: 550/1586, Loss: 0.8096\n",
      "Epoch: 7/20, Batch: 600/1586, Loss: 1.1707\n",
      "Epoch: 7/20, Batch: 650/1586, Loss: 1.1025\n",
      "Epoch: 7/20, Batch: 700/1586, Loss: 0.8208\n",
      "Epoch: 7/20, Batch: 750/1586, Loss: 1.0325\n",
      "Epoch: 7/20, Batch: 800/1586, Loss: 1.2517\n",
      "Epoch: 7/20, Batch: 850/1586, Loss: 0.9975\n",
      "Epoch: 7/20, Batch: 900/1586, Loss: 1.0008\n",
      "Epoch: 7/20, Batch: 950/1586, Loss: 0.9877\n",
      "Epoch: 7/20, Batch: 1000/1586, Loss: 0.9019\n",
      "Epoch: 7/20, Batch: 1050/1586, Loss: 0.9343\n",
      "Epoch: 7/20, Batch: 1100/1586, Loss: 0.9087\n",
      "Epoch: 7/20, Batch: 1150/1586, Loss: 0.7582\n",
      "Epoch: 7/20, Batch: 1200/1586, Loss: 0.9142\n",
      "Epoch: 7/20, Batch: 1250/1586, Loss: 0.9084\n",
      "Epoch: 7/20, Batch: 1300/1586, Loss: 0.9213\n",
      "Epoch: 7/20, Batch: 1350/1586, Loss: 0.9460\n",
      "Epoch: 7/20, Batch: 1400/1586, Loss: 1.2881\n",
      "Epoch: 7/20, Batch: 1450/1586, Loss: 0.8308\n",
      "Epoch: 7/20, Batch: 1500/1586, Loss: 1.2588\n",
      "Epoch: 7/20, Batch: 1550/1586, Loss: 0.6813\n",
      "Epoch: 7, Train Loss: 0.9724\n",
      "Epoch: 8/20, Batch: 0/1586, Loss: 0.8178\n",
      "Epoch: 8/20, Batch: 50/1586, Loss: 1.3624\n",
      "Epoch: 8/20, Batch: 100/1586, Loss: 1.0160\n",
      "Epoch: 8/20, Batch: 150/1586, Loss: 0.8870\n",
      "Epoch: 8/20, Batch: 200/1586, Loss: 0.8729\n",
      "Epoch: 8/20, Batch: 250/1586, Loss: 1.0183\n",
      "Epoch: 8/20, Batch: 300/1586, Loss: 0.8528\n",
      "Epoch: 8/20, Batch: 350/1586, Loss: 0.6922\n",
      "Epoch: 8/20, Batch: 400/1586, Loss: 1.0926\n",
      "Epoch: 8/20, Batch: 450/1586, Loss: 0.6946\n",
      "Epoch: 8/20, Batch: 500/1586, Loss: 0.8288\n",
      "Epoch: 8/20, Batch: 550/1586, Loss: 1.0876\n",
      "Epoch: 8/20, Batch: 600/1586, Loss: 0.9188\n",
      "Epoch: 8/20, Batch: 650/1586, Loss: 0.8521\n",
      "Epoch: 8/20, Batch: 700/1586, Loss: 0.9187\n",
      "Epoch: 8/20, Batch: 750/1586, Loss: 0.6704\n",
      "Epoch: 8/20, Batch: 800/1586, Loss: 0.9597\n",
      "Epoch: 8/20, Batch: 850/1586, Loss: 0.9900\n",
      "Epoch: 8/20, Batch: 900/1586, Loss: 1.1773\n",
      "Epoch: 8/20, Batch: 950/1586, Loss: 1.2266\n",
      "Epoch: 8/20, Batch: 1000/1586, Loss: 0.7298\n",
      "Epoch: 8/20, Batch: 1050/1586, Loss: 1.1914\n",
      "Epoch: 8/20, Batch: 1100/1586, Loss: 0.9511\n",
      "Epoch: 8/20, Batch: 1150/1586, Loss: 0.7900\n",
      "Epoch: 8/20, Batch: 1200/1586, Loss: 0.6520\n",
      "Epoch: 8/20, Batch: 1250/1586, Loss: 1.2895\n",
      "Epoch: 8/20, Batch: 1300/1586, Loss: 1.2785\n",
      "Epoch: 8/20, Batch: 1350/1586, Loss: 0.9369\n",
      "Epoch: 8/20, Batch: 1400/1586, Loss: 0.9872\n",
      "Epoch: 8/20, Batch: 1450/1586, Loss: 0.9816\n",
      "Epoch: 8/20, Batch: 1500/1586, Loss: 1.0628\n",
      "Epoch: 8/20, Batch: 1550/1586, Loss: 0.9032\n",
      "Epoch: 8, Train Loss: 0.9714\n",
      "Epoch: 9/20, Batch: 0/1586, Loss: 0.9869\n",
      "Epoch: 9/20, Batch: 50/1586, Loss: 0.8597\n",
      "Epoch: 9/20, Batch: 100/1586, Loss: 0.8117\n",
      "Epoch: 9/20, Batch: 150/1586, Loss: 1.3040\n",
      "Epoch: 9/20, Batch: 200/1586, Loss: 1.0971\n",
      "Epoch: 9/20, Batch: 250/1586, Loss: 0.7629\n",
      "Epoch: 9/20, Batch: 300/1586, Loss: 0.8697\n",
      "Epoch: 9/20, Batch: 350/1586, Loss: 1.0159\n",
      "Epoch: 9/20, Batch: 400/1586, Loss: 1.1168\n",
      "Epoch: 9/20, Batch: 450/1586, Loss: 1.1838\n",
      "Epoch: 9/20, Batch: 500/1586, Loss: 1.1326\n",
      "Epoch: 9/20, Batch: 550/1586, Loss: 0.8899\n",
      "Epoch: 9/20, Batch: 600/1586, Loss: 1.2914\n",
      "Epoch: 9/20, Batch: 650/1586, Loss: 1.0465\n",
      "Epoch: 9/20, Batch: 700/1586, Loss: 1.1042\n",
      "Epoch: 9/20, Batch: 750/1586, Loss: 1.0205\n",
      "Epoch: 9/20, Batch: 800/1586, Loss: 1.1834\n",
      "Epoch: 9/20, Batch: 850/1586, Loss: 1.5284\n",
      "Epoch: 9/20, Batch: 900/1586, Loss: 1.0940\n",
      "Epoch: 9/20, Batch: 950/1586, Loss: 0.7322\n",
      "Epoch: 9/20, Batch: 1000/1586, Loss: 1.0740\n",
      "Epoch: 9/20, Batch: 1050/1586, Loss: 1.1114\n",
      "Epoch: 9/20, Batch: 1100/1586, Loss: 0.8644\n",
      "Epoch: 9/20, Batch: 1150/1586, Loss: 0.8576\n",
      "Epoch: 9/20, Batch: 1200/1586, Loss: 1.1442\n",
      "Epoch: 9/20, Batch: 1250/1586, Loss: 0.7126\n",
      "Epoch: 9/20, Batch: 1300/1586, Loss: 1.0516\n",
      "Epoch: 9/20, Batch: 1350/1586, Loss: 1.1628\n",
      "Epoch: 9/20, Batch: 1400/1586, Loss: 1.2717\n",
      "Epoch: 9/20, Batch: 1450/1586, Loss: 0.7509\n",
      "Epoch: 9/20, Batch: 1500/1586, Loss: 0.7423\n",
      "Epoch: 9/20, Batch: 1550/1586, Loss: 1.0397\n",
      "Epoch: 9, Train Loss: 0.9702\n",
      "Epoch: 10/20, Batch: 0/1586, Loss: 0.8276\n",
      "Epoch: 10/20, Batch: 50/1586, Loss: 0.9612\n",
      "Epoch: 10/20, Batch: 100/1586, Loss: 0.8035\n",
      "Epoch: 10/20, Batch: 150/1586, Loss: 0.9129\n",
      "Epoch: 10/20, Batch: 200/1586, Loss: 1.0767\n",
      "Epoch: 10/20, Batch: 250/1586, Loss: 0.8654\n",
      "Epoch: 10/20, Batch: 300/1586, Loss: 0.8516\n",
      "Epoch: 10/20, Batch: 350/1586, Loss: 0.6413\n",
      "Epoch: 10/20, Batch: 400/1586, Loss: 1.1066\n",
      "Epoch: 10/20, Batch: 450/1586, Loss: 1.3748\n",
      "Epoch: 10/20, Batch: 500/1586, Loss: 0.7243\n",
      "Epoch: 10/20, Batch: 550/1586, Loss: 1.1307\n",
      "Epoch: 10/20, Batch: 600/1586, Loss: 0.9380\n",
      "Epoch: 10/20, Batch: 650/1586, Loss: 1.1417\n",
      "Epoch: 10/20, Batch: 700/1586, Loss: 0.6970\n",
      "Epoch: 10/20, Batch: 750/1586, Loss: 0.8747\n",
      "Epoch: 10/20, Batch: 800/1586, Loss: 0.9426\n",
      "Epoch: 10/20, Batch: 850/1586, Loss: 0.9914\n",
      "Epoch: 10/20, Batch: 900/1586, Loss: 0.9510\n",
      "Epoch: 10/20, Batch: 950/1586, Loss: 1.4078\n",
      "Epoch: 10/20, Batch: 1000/1586, Loss: 1.0377\n",
      "Epoch: 10/20, Batch: 1050/1586, Loss: 0.8292\n",
      "Epoch: 10/20, Batch: 1100/1586, Loss: 1.2297\n",
      "Epoch: 10/20, Batch: 1150/1586, Loss: 1.0498\n",
      "Epoch: 10/20, Batch: 1200/1586, Loss: 1.2169\n",
      "Epoch: 10/20, Batch: 1250/1586, Loss: 0.6333\n",
      "Epoch: 10/20, Batch: 1300/1586, Loss: 0.8772\n",
      "Epoch: 10/20, Batch: 1350/1586, Loss: 0.8965\n",
      "Epoch: 10/20, Batch: 1400/1586, Loss: 1.1728\n",
      "Epoch: 10/20, Batch: 1450/1586, Loss: 1.4437\n",
      "Epoch: 10/20, Batch: 1500/1586, Loss: 0.9466\n",
      "Epoch: 10/20, Batch: 1550/1586, Loss: 0.8983\n",
      "Epoch: 10, Train Loss: 0.9697\n",
      "Epoch: 11/20, Batch: 0/1586, Loss: 0.8382\n",
      "Epoch: 11/20, Batch: 50/1586, Loss: 1.1782\n",
      "Epoch: 11/20, Batch: 100/1586, Loss: 0.8414\n",
      "Epoch: 11/20, Batch: 150/1586, Loss: 0.6387\n",
      "Epoch: 11/20, Batch: 200/1586, Loss: 1.0145\n",
      "Epoch: 11/20, Batch: 250/1586, Loss: 0.9129\n",
      "Epoch: 11/20, Batch: 300/1586, Loss: 0.9877\n",
      "Epoch: 11/20, Batch: 350/1586, Loss: 0.7738\n",
      "Epoch: 11/20, Batch: 400/1586, Loss: 0.6963\n",
      "Epoch: 11/20, Batch: 450/1586, Loss: 0.9753\n",
      "Epoch: 11/20, Batch: 500/1586, Loss: 0.8718\n",
      "Epoch: 11/20, Batch: 550/1586, Loss: 0.8270\n",
      "Epoch: 11/20, Batch: 600/1586, Loss: 0.7932\n",
      "Epoch: 11/20, Batch: 650/1586, Loss: 0.9954\n",
      "Epoch: 11/20, Batch: 700/1586, Loss: 1.2598\n",
      "Epoch: 11/20, Batch: 750/1586, Loss: 0.7389\n",
      "Epoch: 11/20, Batch: 800/1586, Loss: 0.7839\n",
      "Epoch: 11/20, Batch: 850/1586, Loss: 0.7800\n",
      "Epoch: 11/20, Batch: 900/1586, Loss: 1.0492\n",
      "Epoch: 11/20, Batch: 950/1586, Loss: 1.1317\n",
      "Epoch: 11/20, Batch: 1000/1586, Loss: 1.1597\n",
      "Epoch: 11/20, Batch: 1050/1586, Loss: 0.9535\n",
      "Epoch: 11/20, Batch: 1100/1586, Loss: 0.9180\n",
      "Epoch: 11/20, Batch: 1150/1586, Loss: 0.9700\n",
      "Epoch: 11/20, Batch: 1200/1586, Loss: 0.9091\n",
      "Epoch: 11/20, Batch: 1250/1586, Loss: 1.0582\n",
      "Epoch: 11/20, Batch: 1300/1586, Loss: 0.9903\n",
      "Epoch: 11/20, Batch: 1350/1586, Loss: 0.8742\n",
      "Epoch: 11/20, Batch: 1400/1586, Loss: 0.9185\n",
      "Epoch: 11/20, Batch: 1450/1586, Loss: 0.7268\n",
      "Epoch: 11/20, Batch: 1500/1586, Loss: 0.9725\n",
      "Epoch: 11/20, Batch: 1550/1586, Loss: 1.1476\n",
      "Epoch: 11, Train Loss: 0.9703\n",
      "Epoch: 12/20, Batch: 0/1586, Loss: 1.0477\n",
      "Epoch: 12/20, Batch: 50/1586, Loss: 0.9536\n",
      "Epoch: 12/20, Batch: 100/1586, Loss: 0.8101\n",
      "Epoch: 12/20, Batch: 150/1586, Loss: 0.9382\n",
      "Epoch: 12/20, Batch: 200/1586, Loss: 0.8012\n",
      "Epoch: 12/20, Batch: 250/1586, Loss: 0.9367\n",
      "Epoch: 12/20, Batch: 300/1586, Loss: 0.8147\n",
      "Epoch: 12/20, Batch: 350/1586, Loss: 0.7620\n",
      "Epoch: 12/20, Batch: 400/1586, Loss: 1.0528\n",
      "Epoch: 12/20, Batch: 450/1586, Loss: 0.7128\n",
      "Epoch: 12/20, Batch: 500/1586, Loss: 1.0736\n",
      "Epoch: 12/20, Batch: 550/1586, Loss: 1.2549\n",
      "Epoch: 12/20, Batch: 600/1586, Loss: 0.8270\n",
      "Epoch: 12/20, Batch: 650/1586, Loss: 1.1711\n",
      "Epoch: 12/20, Batch: 700/1586, Loss: 1.4746\n",
      "Epoch: 12/20, Batch: 750/1586, Loss: 1.2502\n",
      "Epoch: 12/20, Batch: 800/1586, Loss: 0.9305\n",
      "Epoch: 12/20, Batch: 850/1586, Loss: 1.3566\n",
      "Epoch: 12/20, Batch: 900/1586, Loss: 1.2115\n",
      "Epoch: 12/20, Batch: 950/1586, Loss: 0.9130\n",
      "Epoch: 12/20, Batch: 1000/1586, Loss: 0.7513\n",
      "Epoch: 12/20, Batch: 1050/1586, Loss: 0.8198\n",
      "Epoch: 12/20, Batch: 1100/1586, Loss: 0.7743\n",
      "Epoch: 12/20, Batch: 1150/1586, Loss: 1.3782\n",
      "Epoch: 12/20, Batch: 1200/1586, Loss: 0.9817\n",
      "Epoch: 12/20, Batch: 1250/1586, Loss: 1.0504\n",
      "Epoch: 12/20, Batch: 1300/1586, Loss: 0.9020\n",
      "Epoch: 12/20, Batch: 1350/1586, Loss: 0.8570\n",
      "Epoch: 12/20, Batch: 1400/1586, Loss: 0.8228\n",
      "Epoch: 12/20, Batch: 1450/1586, Loss: 0.7397\n",
      "Epoch: 12/20, Batch: 1500/1586, Loss: 0.9666\n",
      "Epoch: 12/20, Batch: 1550/1586, Loss: 1.0794\n",
      "Epoch: 12, Train Loss: 0.9668\n",
      "Epoch: 13/20, Batch: 0/1586, Loss: 0.6672\n",
      "Epoch: 13/20, Batch: 50/1586, Loss: 0.8869\n",
      "Epoch: 13/20, Batch: 100/1586, Loss: 0.8460\n",
      "Epoch: 13/20, Batch: 150/1586, Loss: 0.9105\n",
      "Epoch: 13/20, Batch: 200/1586, Loss: 1.0661\n",
      "Epoch: 13/20, Batch: 250/1586, Loss: 0.7240\n",
      "Epoch: 13/20, Batch: 300/1586, Loss: 0.8018\n",
      "Epoch: 13/20, Batch: 350/1586, Loss: 0.8908\n",
      "Epoch: 13/20, Batch: 400/1586, Loss: 1.0514\n",
      "Epoch: 13/20, Batch: 450/1586, Loss: 0.9837\n",
      "Epoch: 13/20, Batch: 500/1586, Loss: 0.9969\n",
      "Epoch: 13/20, Batch: 550/1586, Loss: 1.2069\n",
      "Epoch: 13/20, Batch: 600/1586, Loss: 1.0757\n",
      "Epoch: 13/20, Batch: 650/1586, Loss: 1.5066\n",
      "Epoch: 13/20, Batch: 700/1586, Loss: 0.7344\n",
      "Epoch: 13/20, Batch: 750/1586, Loss: 1.5761\n",
      "Epoch: 13/20, Batch: 800/1586, Loss: 1.1122\n",
      "Epoch: 13/20, Batch: 850/1586, Loss: 0.7256\n",
      "Epoch: 13/20, Batch: 900/1586, Loss: 0.8939\n",
      "Epoch: 13/20, Batch: 950/1586, Loss: 1.5000\n",
      "Epoch: 13/20, Batch: 1000/1586, Loss: 0.7868\n",
      "Epoch: 13/20, Batch: 1050/1586, Loss: 0.8604\n",
      "Epoch: 13/20, Batch: 1100/1586, Loss: 0.7367\n",
      "Epoch: 13/20, Batch: 1150/1586, Loss: 1.1562\n",
      "Epoch: 13/20, Batch: 1200/1586, Loss: 1.2458\n",
      "Epoch: 13/20, Batch: 1250/1586, Loss: 1.2881\n",
      "Epoch: 13/20, Batch: 1300/1586, Loss: 0.9049\n",
      "Epoch: 13/20, Batch: 1350/1586, Loss: 0.9434\n",
      "Epoch: 13/20, Batch: 1400/1586, Loss: 0.8225\n",
      "Epoch: 13/20, Batch: 1450/1586, Loss: 1.1651\n",
      "Epoch: 13/20, Batch: 1500/1586, Loss: 0.6915\n",
      "Epoch: 13/20, Batch: 1550/1586, Loss: 1.2076\n",
      "Epoch: 13, Train Loss: 0.9682\n",
      "Epoch: 14/20, Batch: 0/1586, Loss: 0.7827\n",
      "Epoch: 14/20, Batch: 50/1586, Loss: 0.9468\n",
      "Epoch: 14/20, Batch: 100/1586, Loss: 0.9004\n",
      "Epoch: 14/20, Batch: 150/1586, Loss: 1.0581\n",
      "Epoch: 14/20, Batch: 200/1586, Loss: 1.3250\n",
      "Epoch: 14/20, Batch: 250/1586, Loss: 0.9750\n",
      "Epoch: 14/20, Batch: 300/1586, Loss: 0.8913\n",
      "Epoch: 14/20, Batch: 350/1586, Loss: 0.8900\n",
      "Epoch: 14/20, Batch: 400/1586, Loss: 0.7643\n",
      "Epoch: 14/20, Batch: 450/1586, Loss: 0.7712\n",
      "Epoch: 14/20, Batch: 500/1586, Loss: 0.9783\n",
      "Epoch: 14/20, Batch: 550/1586, Loss: 0.8180\n",
      "Epoch: 14/20, Batch: 600/1586, Loss: 1.0082\n",
      "Epoch: 14/20, Batch: 650/1586, Loss: 0.9410\n",
      "Epoch: 14/20, Batch: 700/1586, Loss: 1.1923\n",
      "Epoch: 14/20, Batch: 750/1586, Loss: 0.9953\n",
      "Epoch: 14/20, Batch: 800/1586, Loss: 0.7334\n",
      "Epoch: 14/20, Batch: 850/1586, Loss: 0.7143\n",
      "Epoch: 14/20, Batch: 900/1586, Loss: 0.7408\n",
      "Epoch: 14/20, Batch: 950/1586, Loss: 0.9920\n",
      "Epoch: 14/20, Batch: 1000/1586, Loss: 1.0074\n",
      "Epoch: 14/20, Batch: 1050/1586, Loss: 0.7218\n",
      "Epoch: 14/20, Batch: 1100/1586, Loss: 0.8695\n",
      "Epoch: 14/20, Batch: 1150/1586, Loss: 0.8270\n",
      "Epoch: 14/20, Batch: 1200/1586, Loss: 1.0266\n",
      "Epoch: 14/20, Batch: 1250/1586, Loss: 1.2430\n",
      "Epoch: 14/20, Batch: 1300/1586, Loss: 1.1428\n",
      "Epoch: 14/20, Batch: 1350/1586, Loss: 0.8141\n",
      "Epoch: 14/20, Batch: 1400/1586, Loss: 1.3610\n",
      "Epoch: 14/20, Batch: 1450/1586, Loss: 1.2047\n",
      "Epoch: 14/20, Batch: 1500/1586, Loss: 1.2989\n",
      "Epoch: 14/20, Batch: 1550/1586, Loss: 1.0161\n",
      "Epoch: 14, Train Loss: 0.9688\n",
      "Epoch: 15/20, Batch: 0/1586, Loss: 1.3056\n",
      "Epoch: 15/20, Batch: 50/1586, Loss: 0.8999\n",
      "Epoch: 15/20, Batch: 100/1586, Loss: 1.2097\n",
      "Epoch: 15/20, Batch: 150/1586, Loss: 1.3958\n",
      "Epoch: 15/20, Batch: 200/1586, Loss: 1.5898\n",
      "Epoch: 15/20, Batch: 250/1586, Loss: 1.1852\n",
      "Epoch: 15/20, Batch: 300/1586, Loss: 1.1563\n",
      "Epoch: 15/20, Batch: 350/1586, Loss: 1.0692\n",
      "Epoch: 15/20, Batch: 400/1586, Loss: 0.7687\n",
      "Epoch: 15/20, Batch: 450/1586, Loss: 1.0578\n",
      "Epoch: 15/20, Batch: 500/1586, Loss: 0.6804\n",
      "Epoch: 15/20, Batch: 550/1586, Loss: 0.9089\n",
      "Epoch: 15/20, Batch: 600/1586, Loss: 1.1804\n",
      "Epoch: 15/20, Batch: 650/1586, Loss: 0.8146\n",
      "Epoch: 15/20, Batch: 700/1586, Loss: 1.2387\n",
      "Epoch: 15/20, Batch: 750/1586, Loss: 0.9403\n",
      "Epoch: 15/20, Batch: 800/1586, Loss: 1.3415\n",
      "Epoch: 15/20, Batch: 850/1586, Loss: 1.5930\n",
      "Epoch: 15/20, Batch: 900/1586, Loss: 0.6924\n",
      "Epoch: 15/20, Batch: 950/1586, Loss: 1.4654\n",
      "Epoch: 15/20, Batch: 1000/1586, Loss: 0.7727\n",
      "Epoch: 15/20, Batch: 1050/1586, Loss: 1.2252\n",
      "Epoch: 15/20, Batch: 1100/1586, Loss: 1.1122\n",
      "Epoch: 15/20, Batch: 1150/1586, Loss: 1.0668\n",
      "Epoch: 15/20, Batch: 1200/1586, Loss: 0.9611\n",
      "Epoch: 15/20, Batch: 1250/1586, Loss: 1.0607\n",
      "Epoch: 15/20, Batch: 1300/1586, Loss: 0.8102\n",
      "Epoch: 15/20, Batch: 1350/1586, Loss: 1.1488\n",
      "Epoch: 15/20, Batch: 1400/1586, Loss: 0.7357\n",
      "Epoch: 15/20, Batch: 1450/1586, Loss: 0.9702\n",
      "Epoch: 15/20, Batch: 1500/1586, Loss: 1.0363\n",
      "Epoch: 15/20, Batch: 1550/1586, Loss: 0.8258\n",
      "Epoch: 15, Train Loss: 0.9670\n",
      "Epoch: 16/20, Batch: 0/1586, Loss: 1.0902\n",
      "Epoch: 16/20, Batch: 50/1586, Loss: 0.7079\n",
      "Epoch: 16/20, Batch: 100/1586, Loss: 0.9779\n",
      "Epoch: 16/20, Batch: 150/1586, Loss: 1.2318\n",
      "Epoch: 16/20, Batch: 200/1586, Loss: 1.0270\n",
      "Epoch: 16/20, Batch: 250/1586, Loss: 1.2294\n",
      "Epoch: 16/20, Batch: 300/1586, Loss: 0.6251\n",
      "Epoch: 16/20, Batch: 350/1586, Loss: 0.9197\n",
      "Epoch: 16/20, Batch: 400/1586, Loss: 0.8684\n",
      "Epoch: 16/20, Batch: 450/1586, Loss: 0.7683\n",
      "Epoch: 16/20, Batch: 500/1586, Loss: 1.3730\n",
      "Epoch: 16/20, Batch: 550/1586, Loss: 1.1966\n",
      "Epoch: 16/20, Batch: 600/1586, Loss: 1.1013\n",
      "Epoch: 16/20, Batch: 650/1586, Loss: 1.2262\n",
      "Epoch: 16/20, Batch: 700/1586, Loss: 0.7107\n",
      "Epoch: 16/20, Batch: 750/1586, Loss: 0.7328\n",
      "Epoch: 16/20, Batch: 800/1586, Loss: 1.2163\n",
      "Epoch: 16/20, Batch: 850/1586, Loss: 1.1454\n",
      "Epoch: 16/20, Batch: 900/1586, Loss: 1.0073\n",
      "Epoch: 16/20, Batch: 950/1586, Loss: 0.8656\n",
      "Epoch: 16/20, Batch: 1000/1586, Loss: 1.2360\n",
      "Epoch: 16/20, Batch: 1050/1586, Loss: 0.6473\n",
      "Epoch: 16/20, Batch: 1100/1586, Loss: 1.0452\n",
      "Epoch: 16/20, Batch: 1150/1586, Loss: 0.9956\n",
      "Epoch: 16/20, Batch: 1200/1586, Loss: 0.8508\n",
      "Epoch: 16/20, Batch: 1250/1586, Loss: 0.8303\n",
      "Epoch: 16/20, Batch: 1300/1586, Loss: 1.1483\n",
      "Epoch: 16/20, Batch: 1350/1586, Loss: 1.1761\n",
      "Epoch: 16/20, Batch: 1400/1586, Loss: 0.8313\n",
      "Epoch: 16/20, Batch: 1450/1586, Loss: 1.0432\n",
      "Epoch: 16/20, Batch: 1500/1586, Loss: 0.8252\n",
      "Epoch: 16/20, Batch: 1550/1586, Loss: 0.7936\n",
      "Epoch: 16, Train Loss: 0.9653\n",
      "Epoch: 17/20, Batch: 0/1586, Loss: 0.9485\n",
      "Epoch: 17/20, Batch: 50/1586, Loss: 1.0845\n",
      "Epoch: 17/20, Batch: 100/1586, Loss: 1.0018\n",
      "Epoch: 17/20, Batch: 150/1586, Loss: 1.2186\n",
      "Epoch: 17/20, Batch: 200/1586, Loss: 0.9382\n",
      "Epoch: 17/20, Batch: 250/1586, Loss: 0.9784\n",
      "Epoch: 17/20, Batch: 300/1586, Loss: 0.6987\n",
      "Epoch: 17/20, Batch: 350/1586, Loss: 1.4804\n",
      "Epoch: 17/20, Batch: 400/1586, Loss: 0.7038\n",
      "Epoch: 17/20, Batch: 450/1586, Loss: 1.0383\n",
      "Epoch: 17/20, Batch: 500/1586, Loss: 0.7112\n",
      "Epoch: 17/20, Batch: 550/1586, Loss: 0.6950\n",
      "Epoch: 17/20, Batch: 600/1586, Loss: 0.7392\n",
      "Epoch: 17/20, Batch: 650/1586, Loss: 0.9263\n",
      "Epoch: 17/20, Batch: 700/1586, Loss: 0.9789\n",
      "Epoch: 17/20, Batch: 750/1586, Loss: 1.3453\n",
      "Epoch: 17/20, Batch: 800/1586, Loss: 0.8803\n",
      "Epoch: 17/20, Batch: 850/1586, Loss: 0.8332\n",
      "Epoch: 17/20, Batch: 900/1586, Loss: 0.8228\n",
      "Epoch: 17/20, Batch: 950/1586, Loss: 0.8434\n",
      "Epoch: 17/20, Batch: 1000/1586, Loss: 0.7430\n",
      "Epoch: 17/20, Batch: 1050/1586, Loss: 1.1972\n",
      "Epoch: 17/20, Batch: 1100/1586, Loss: 1.0753\n",
      "Epoch: 17/20, Batch: 1150/1586, Loss: 1.0101\n",
      "Epoch: 17/20, Batch: 1200/1586, Loss: 1.0312\n",
      "Epoch: 17/20, Batch: 1250/1586, Loss: 1.2551\n",
      "Epoch: 17/20, Batch: 1300/1586, Loss: 0.9067\n",
      "Epoch: 17/20, Batch: 1350/1586, Loss: 0.8674\n",
      "Epoch: 17/20, Batch: 1400/1586, Loss: 1.1664\n",
      "Epoch: 17/20, Batch: 1450/1586, Loss: 1.0535\n",
      "Epoch: 17/20, Batch: 1500/1586, Loss: 1.0761\n",
      "Epoch: 17/20, Batch: 1550/1586, Loss: 1.1736\n",
      "Epoch: 17, Train Loss: 0.9674\n",
      "Epoch: 18/20, Batch: 0/1586, Loss: 1.1302\n",
      "Epoch: 18/20, Batch: 50/1586, Loss: 0.7718\n",
      "Epoch: 18/20, Batch: 100/1586, Loss: 0.8145\n",
      "Epoch: 18/20, Batch: 150/1586, Loss: 1.1612\n",
      "Epoch: 18/20, Batch: 200/1586, Loss: 0.7744\n",
      "Epoch: 18/20, Batch: 250/1586, Loss: 1.0776\n",
      "Epoch: 18/20, Batch: 300/1586, Loss: 1.1594\n",
      "Epoch: 18/20, Batch: 350/1586, Loss: 1.0117\n",
      "Epoch: 18/20, Batch: 400/1586, Loss: 0.9766\n",
      "Epoch: 18/20, Batch: 450/1586, Loss: 1.2615\n",
      "Epoch: 18/20, Batch: 500/1586, Loss: 1.1577\n",
      "Epoch: 18/20, Batch: 550/1586, Loss: 1.0152\n",
      "Epoch: 18/20, Batch: 600/1586, Loss: 0.7661\n",
      "Epoch: 18/20, Batch: 650/1586, Loss: 1.0299\n",
      "Epoch: 18/20, Batch: 700/1586, Loss: 0.9280\n",
      "Epoch: 18/20, Batch: 750/1586, Loss: 1.1575\n",
      "Epoch: 18/20, Batch: 800/1586, Loss: 0.8683\n",
      "Epoch: 18/20, Batch: 850/1586, Loss: 0.6824\n",
      "Epoch: 18/20, Batch: 900/1586, Loss: 0.8040\n",
      "Epoch: 18/20, Batch: 950/1586, Loss: 1.2085\n",
      "Epoch: 18/20, Batch: 1000/1586, Loss: 1.0172\n",
      "Epoch: 18/20, Batch: 1050/1586, Loss: 1.0116\n",
      "Epoch: 18/20, Batch: 1100/1586, Loss: 0.8818\n",
      "Epoch: 18/20, Batch: 1150/1586, Loss: 0.9926\n",
      "Epoch: 18/20, Batch: 1200/1586, Loss: 0.9583\n",
      "Epoch: 18/20, Batch: 1250/1586, Loss: 1.1244\n",
      "Epoch: 18/20, Batch: 1300/1586, Loss: 1.1842\n",
      "Epoch: 18/20, Batch: 1350/1586, Loss: 1.0239\n",
      "Epoch: 18/20, Batch: 1400/1586, Loss: 0.6629\n",
      "Epoch: 18/20, Batch: 1450/1586, Loss: 1.0765\n",
      "Epoch: 18/20, Batch: 1500/1586, Loss: 1.1239\n",
      "Epoch: 18/20, Batch: 1550/1586, Loss: 0.9935\n",
      "Epoch: 18, Train Loss: 0.9673\n",
      "Epoch: 19/20, Batch: 0/1586, Loss: 0.9360\n",
      "Epoch: 19/20, Batch: 50/1586, Loss: 0.7348\n",
      "Epoch: 19/20, Batch: 100/1586, Loss: 0.8938\n",
      "Epoch: 19/20, Batch: 150/1586, Loss: 0.9030\n",
      "Epoch: 19/20, Batch: 200/1586, Loss: 1.0230\n",
      "Epoch: 19/20, Batch: 250/1586, Loss: 1.0864\n",
      "Epoch: 19/20, Batch: 300/1586, Loss: 0.9423\n",
      "Epoch: 19/20, Batch: 350/1586, Loss: 0.7639\n",
      "Epoch: 19/20, Batch: 400/1586, Loss: 0.9999\n",
      "Epoch: 19/20, Batch: 450/1586, Loss: 1.1285\n",
      "Epoch: 19/20, Batch: 500/1586, Loss: 0.6985\n",
      "Epoch: 19/20, Batch: 550/1586, Loss: 1.0338\n",
      "Epoch: 19/20, Batch: 600/1586, Loss: 1.1084\n",
      "Epoch: 19/20, Batch: 650/1586, Loss: 1.1297\n",
      "Epoch: 19/20, Batch: 700/1586, Loss: 0.9866\n",
      "Epoch: 19/20, Batch: 750/1586, Loss: 0.8236\n",
      "Epoch: 19/20, Batch: 800/1586, Loss: 1.1133\n",
      "Epoch: 19/20, Batch: 850/1586, Loss: 0.9609\n",
      "Epoch: 19/20, Batch: 900/1586, Loss: 1.1456\n",
      "Epoch: 19/20, Batch: 950/1586, Loss: 0.7549\n",
      "Epoch: 19/20, Batch: 1000/1586, Loss: 0.8373\n",
      "Epoch: 19/20, Batch: 1050/1586, Loss: 0.8242\n",
      "Epoch: 19/20, Batch: 1100/1586, Loss: 1.3801\n",
      "Epoch: 19/20, Batch: 1150/1586, Loss: 0.9749\n",
      "Epoch: 19/20, Batch: 1200/1586, Loss: 0.7354\n",
      "Epoch: 19/20, Batch: 1250/1586, Loss: 1.2658\n",
      "Epoch: 19/20, Batch: 1300/1586, Loss: 1.0682\n",
      "Epoch: 19/20, Batch: 1350/1586, Loss: 0.8681\n",
      "Epoch: 19/20, Batch: 1400/1586, Loss: 0.7606\n",
      "Epoch: 19/20, Batch: 1450/1586, Loss: 0.8337\n",
      "Epoch: 19/20, Batch: 1500/1586, Loss: 1.1663\n",
      "Epoch: 19/20, Batch: 1550/1586, Loss: 0.7717\n",
      "Epoch: 19, Train Loss: 0.9625\n",
      "Epoch: 20/20, Batch: 0/1586, Loss: 0.8693\n",
      "Epoch: 20/20, Batch: 50/1586, Loss: 1.1563\n",
      "Epoch: 20/20, Batch: 100/1586, Loss: 1.1509\n",
      "Epoch: 20/20, Batch: 150/1586, Loss: 0.8751\n",
      "Epoch: 20/20, Batch: 200/1586, Loss: 1.4697\n",
      "Epoch: 20/20, Batch: 250/1586, Loss: 0.9669\n",
      "Epoch: 20/20, Batch: 300/1586, Loss: 1.0808\n",
      "Epoch: 20/20, Batch: 350/1586, Loss: 1.0351\n",
      "Epoch: 20/20, Batch: 400/1586, Loss: 0.8052\n",
      "Epoch: 20/20, Batch: 450/1586, Loss: 0.9825\n",
      "Epoch: 20/20, Batch: 500/1586, Loss: 0.7280\n",
      "Epoch: 20/20, Batch: 550/1586, Loss: 0.7600\n",
      "Epoch: 20/20, Batch: 600/1586, Loss: 0.8803\n",
      "Epoch: 20/20, Batch: 650/1586, Loss: 1.1889\n",
      "Epoch: 20/20, Batch: 700/1586, Loss: 0.9931\n",
      "Epoch: 20/20, Batch: 750/1586, Loss: 0.8603\n",
      "Epoch: 20/20, Batch: 800/1586, Loss: 0.8292\n",
      "Epoch: 20/20, Batch: 850/1586, Loss: 1.1863\n",
      "Epoch: 20/20, Batch: 900/1586, Loss: 0.9883\n",
      "Epoch: 20/20, Batch: 950/1586, Loss: 1.1001\n",
      "Epoch: 20/20, Batch: 1000/1586, Loss: 0.9577\n",
      "Epoch: 20/20, Batch: 1050/1586, Loss: 1.1690\n",
      "Epoch: 20/20, Batch: 1100/1586, Loss: 0.8324\n",
      "Epoch: 20/20, Batch: 1150/1586, Loss: 0.7431\n",
      "Epoch: 20/20, Batch: 1200/1586, Loss: 0.8926\n",
      "Epoch: 20/20, Batch: 1250/1586, Loss: 0.9623\n",
      "Epoch: 20/20, Batch: 1300/1586, Loss: 0.8834\n",
      "Epoch: 20/20, Batch: 1350/1586, Loss: 0.7055\n",
      "Epoch: 20/20, Batch: 1400/1586, Loss: 0.9494\n",
      "Epoch: 20/20, Batch: 1450/1586, Loss: 1.6874\n",
      "Epoch: 20/20, Batch: 1500/1586, Loss: 0.7964\n",
      "Epoch: 20/20, Batch: 1550/1586, Loss: 0.8829\n",
      "Epoch: 20, Train Loss: 0.9654\n",
      "Training finished!\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "weighted_train_losses = []\n",
    "\n",
    "print(\"\\nStarting training...\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    weighted_model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = weighted_model(data)\n",
    "    \n",
    "        target = target.float()\n",
    "        output = output.float()    \n",
    "        loss = weighted_model.get_weighted_loss(output, target)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if batch_idx % 50 == 0:\n",
    "            print(f'Epoch: {epoch+1}/{num_epochs}, Batch: {batch_idx}/{len(train_loader)}, Loss: {loss.item():.4f}')\n",
    "    \n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "    weighted_train_losses.append(avg_train_loss)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1}, Train Loss: {avg_train_loss:.4f}')\n",
    "\n",
    "print('Training finished!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report (Weighted):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.71      0.82     45997\n",
      "         1.0       0.23      0.84      0.36      4739\n",
      "\n",
      "    accuracy                           0.72     50736\n",
      "   macro avg       0.60      0.77      0.59     50736\n",
      "weighted avg       0.91      0.72      0.78     50736\n",
      "\n",
      "\n",
      "Confusion Matrix (Weighted):\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAi8AAAHHCAYAAAB3K7g2AAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAANfxJREFUeJzt3Wd4FOXCh/H/pm1CeqihhSYQiiD1hNCrikixAYoJTbFgoajo4dCUiAUEpIkIiOgLiiCCBaRIy1G6UqQ36RASCKSRzPuBkz0uSSCBFB/O/bsuP+wzszPPrAu52ZnZ2CzLsgQAAGAIl4KeAAAAQE4QLwAAwCjECwAAMArxAgAAjEK8AAAAoxAvAADAKMQLAAAwCvECAACMQrwAAACjEC9AHtu3b5/atm0rf39/2Ww2LVq0KFe3f/jwYdlsNs2aNStXt2uy5s2bq3nz5rm6zWPHjsnT01Pr16/P1e1mR7ly5RQZGXnLz33ggQdyd0I5FBkZqXLlyjkenz9/Xt7e3vruu+8KblIwGvGC/wkHDhzQ008/rQoVKsjT01N+fn4KDw/X+PHjlZCQkKf7joiI0O+//6633npLc+bMUb169fJ0f/kpMjJSNptNfn5+mb6O+/btk81mk81m03vvvZfj7Z84cULDhw/Xtm3bcmG2t2fkyJFq2LChwsPDJUnPPvusXFxcFBMT47ReTEyMXFxcZLfblZiY6LTs4MGDstlsev311/Nt3tm1a9cuDR8+XIcPH87zfRUuXFh9+vTR0KFD83xfuDMRL7jjLV26VDVr1tT8+fPVoUMHTZw4UVFRUSpbtqwGDx6sF198Mc/2nZCQoOjoaPXu3VvPP/+8nnjiCZUuXTpX9xESEqKEhAT16NEjV7ebXW5ubrpy5Yq+/fbbDMvmzp0rT0/PW972iRMnNGLEiBzHy7Jly7Rs2bJb3u/1zp49q9mzZ6tfv36OscaNG8uyrAyfxGzYsEEuLi5KSUnRpk2bnJalr9u4ceMc7X/Pnj2aPn36Lc4+e3bt2qURI0bkS7xIUr9+/bRlyxatXLkyX/aHOwvxgjvaoUOH1LVrV4WEhGjXrl0aP368+vbtq+eee05ffPGFdu3aperVq+fZ/s+ePStJCggIyLN92Gw2eXp6ytXVNc/2cSN2u12tWrXSF198kWHZ559/rvbt2+fbXK5cuSJJ8vDwkIeHR65t97PPPpObm5s6dOjgGEsPkHXr1jmtu379et19992qUqVKhmXr1q2Ti4uLGjVqlKP92+12ubu73+Ls/55CQ0NVo0YNTnfilhAvuKO98847io+P14wZMxQcHJxheaVKlZw+ebl69apGjRqlihUrym63q1y5cnr99deVlJTk9Lz06wjWrVunBg0ayNPTUxUqVNCnn37qWGf48OEKCQmRJA0ePFg2m81x3v/6awD++hybzeY0tnz5cjVu3FgBAQHy8fFRlSpVnE47ZHXNy8qVK9WkSRN5e3srICBAHTt21O7duzPd3/79+xUZGamAgAD5+/urZ8+ejhDIju7du+v7779XbGysY2zjxo3at2+funfvnmH9mJgYDRo0SDVr1pSPj4/8/Px03333afv27Y51Vq9erfr160uSevbs6Tj9lH6czZs3V40aNbR582Y1bdpUhQoVcrwu11/zEhERIU9PzwzH365dOwUGBurEiRM3PL5FixapYcOG8vHxcYyVLVtWZcqUyfDJy/r16xUeHq5GjRpluqx69eqOmE1KStKwYcNUqVIl2e12lSlTRq+88kqm77frr3n57bff1KxZM3l5eal06dJ68803NXPmTNlstkw/PbnRe3XWrFl65JFHJEktWrRwvNarV692rPP999873k++vr5q3769du7cmelrVaNGDXl6eqpGjRpauHBhlq9rmzZt9O2338qyrCzXATJDvOCO9u2336pChQrZ/pdunz599K9//Ut16tTRuHHj1KxZM0VFRalr164Z1t2/f78efvhhtWnTRu+//74CAwMVGRnp+Au9S5cuGjdunCSpW7dumjNnjj744IMczX/nzp164IEHlJSUpJEjR+r999/Xgw8+eNOLRn/66Se1a9dOZ86c0fDhwzVgwABt2LBB4eHhmf5ge/TRR3Xp0iVFRUXp0Ucf1axZszRixIhsz7NLly6y2Wz6+uuvHWOff/65qlatqjp16mRY/+DBg1q0aJEeeOABjR07VoMHD9bvv/+uZs2aOUIiNDRUI0eOlCQ99dRTmjNnjubMmaOmTZs6tnP+/Hndd999ql27tj744AO1aNEi0/mNHz9eRYsWVUREhFJTUyVJ06ZN07JlyzRx4kSVLFkyy2NLSUnRxo0bMz2Oxo0ba9OmTY7YSE5O1saNG9WoUSM1atRIGzZscPxgvnDhgnbt2uX4xCYtLU0PPvig3nvvPcfpzE6dOmncuHF67LHHsn6xJR0/flwtWrTQzp07NWTIEL388suaO3euxo8fn+n6N3uvNm3aVC+88IIk6fXXX3e81qGhoZKkOXPmqH379vLx8dGYMWM0dOhQx7H89f20bNkyPfTQQ7LZbIqKilKnTp3Us2fPDKfP0tWtW1exsbGZRhBwQxZwh4qLi7MkWR07dszW+tu2bbMkWX369HEaHzRokCXJWrlypWMsJCTEkmStWbPGMXbmzBnLbrdbAwcOdIwdOnTIkmS9++67TtuMiIiwQkJCMsxh2LBh1l//WI4bN86SZJ09ezbLeafvY+bMmY6x2rVrW8WKFbPOnz/vGNu+fbvl4uJiPfnkkxn216tXL6dtdu7c2SpcuHCW+/zrcXh7e1uWZVkPP/yw1apVK8uyLCs1NdUqUaKENWLEiExfg8TERCs1NTXDcdjtdmvkyJGOsY0bN2Y4tnTNmjWzJFlTp07NdFmzZs2cxn788UdLkvXmm29aBw8etHx8fKxOnTrd9Bj3799vSbImTpyYYdmkSZMsSdbatWsty7Ks6OhoS5J15MgRa9euXZYka+fOnZZlWdaSJUssSdbcuXMty7KsOXPmWC4uLo7npps6daolyVq/fr1jLCQkxIqIiHA87t+/v2Wz2aytW7c6xs6fP28FBQVZkqxDhw45PTc779Uvv/zSkmStWrXKaT6XLl2yAgICrL59+zqNnzp1yvL393car127thUcHGzFxsY6xpYtW2ZJyvT9vmHDBkuSNW/evAzLgBvhkxfcsS5evChJ8vX1zdb66bdtDhgwwGl84MCBkq5d+PtX1apVU5MmTRyPixYtqipVqujgwYO3POfrpZ9e+Oabb5SWlpat55w8eVLbtm1TZGSkgoKCHON333232rRpk+ntqX+9EFWSmjRpovPnzztew+zo3r27Vq9erVOnTmnlypU6depUpqeMpGvXcLi4XPvrJzU1VefPn3ecEtuyZUu292m329WzZ89srdu2bVs9/fTTGjlypLp06SJPT09Nmzbtps87f/68JCkwMDDDsuuve1m/fr1KlSqlsmXLqmrVqgoKCnJ8Snb9xbpffvmlQkNDVbVqVZ07d87xX8uWLSVJq1atynJOP/zwg8LCwlS7dm3HWFBQkB5//PFM17+d9+ry5csVGxurbt26Oc3T1dVVDRs2dMwz/X0XEREhf39/x/PbtGmjatWqZbrt9Nf03LlzN50H8FfEC+5Yfn5+kqRLly5la/0jR47IxcVFlSpVchovUaKEAgICdOTIEafxsmXLZthGYGCgLly4cIszzuixxx5TeHi4+vTpo+LFi6tr166aP3/+DUMmfZ5VqlTJsCw0NFTnzp3T5cuXncavP5b0Hyo5OZb7779fvr6+mjdvnubOnav69etneC3TpaWlady4cbrrrrtkt9tVpEgRFS1aVL/99pvi4uKyvc9SpUrl6MLc9957T0FBQdq2bZsmTJigYsWKZfu5VibXZdSoUUMBAQFOgZJ+K7XNZlNYWJjTsjJlyjhe63379mnnzp0qWrSo03+VK1eWJJ05cybLuRw5ciTT1zar1/t23qv79u2TJLVs2TLDXJctW+aYZ/r77q677sqwjczei9J/X9Prr/MCbsatoCcA5BU/Pz+VLFlSO3bsyNHzsvsXaVZ392T2Qy67+0i/HiOdl5eX1qxZo1WrVmnp0qX64YcfNG/ePLVs2VLLli3LtTuMbudY0tntdnXp0kWzZ8/WwYMHNXz48CzXHT16tIYOHapevXpp1KhRCgoKkouLi1566aVsf8IkXXt9cmLr1q2OH7a///67unXrdtPnFC5cWFLmIefi4qKwsDDHtS3r1693upi6UaNG+uSTTxzXwnTq1MmxLC0tTTVr1tTYsWMz3W+ZMmVycmg3dDv/f9P/f8yZM0clSpTIsNzN7dZ/jKS/pkWKFLnlbeB/E/GCO9oDDzygjz76SNHR0QoLC7vhuiEhIUpLS9O+ffscFypK0unTpxUbG+u4cyg3BAYGOt2Zk+76T3ekaz8gW7VqpVatWmns2LEaPXq03njjDa1atUqtW7fO9Dika98Ncr0//vhDRYoUkbe39+0fRCa6d++uTz75RC4uLple5Jzuq6++UosWLTRjxgyn8djYWKcfZLn5L/LLly+rZ8+eqlatmho1aqR33nlHnTt3dtzRlJWyZcvKy8tLhw4dynR548aN9f3332vx4sU6c+aM45MX6Vq8vPHGG/ruu++UkJDg9P0uFStW1Pbt29WqVascH2dISIj279+fYTyzsezKag4VK1aUJBUrVizT99tf5yT995Oav8rsvSjJ8Zr+9c8bkB2cNsId7ZVXXpG3t7f69Omj06dPZ1h+4MABxx0a999/vyRluCMo/V/Gufl9JRUrVlRcXJx+++03x9jJkycz3FZ6/be3SnJc53D97bTpgoODVbt2bc2ePdspkHbs2KFly5Y5jjMvtGjRQqNGjdKHH36Y6b/S07m6umb4V/+XX36p48ePO42lR1ZmoZdTr776qo4eParZs2dr7NixKleunCIiIrJ8HdO5u7urXr16Wd4xkx4kY8aMUaFChZyuQ2nQoIHc3Nz0zjvvOK0rXbvD6/jx45l++VxCQkKGU3t/1a5dO0VHRzt9eV9MTIzmzp17w2O5kaxe63bt2snPz0+jR49WSkpKhuelf5fRX993fz31t3z5cu3atSvTfW7evFn+/v55+l1LuDPxyQvuaBUrVtTnn3+uxx57TKGhoXryySdVo0YNJScna8OGDfryyy8d359Rq1YtRURE6KOPPlJsbKyaNWumX3/9VbNnz1anTp2yvA33VnTt2lWvvvqqOnfurBdeeEFXrlzRlClTVLlyZacLVkeOHKk1a9aoffv2CgkJ0ZkzZzR58mSVLl36ht/S+u677+q+++5TWFiYevfurYSEBE2cOFH+/v43PJ1zu1xcXPTPf/7zpus98MADGjlypHr27KlGjRrp999/19y5c1WhQgWn9SpWrKiAgABNnTpVvr6+8vb2VsOGDVW+fPkczWvlypWaPHmyhg0b5rjleebMmWrevLmGDh3qiIusdOzYUW+88YYuXrzouJYqXYMGDeTh4aHo6Gg1b97c6TRKoUKFVKtWLUVHRysgIEA1atRwLOvRo4fmz5+vfv36adWqVQoPD1dqaqr++OMPzZ8/Xz/++GOWv0rilVde0WeffaY2bdqof//+8vb21scff6yyZcsqJibmlj6xql27tlxdXTVmzBjFxcXJbrerZcuWKlasmKZMmaIePXqoTp066tq1q4oWLaqjR49q6dKlCg8P14cffihJioqKUvv27dW4cWP16tVLMTExmjhxoqpXr674+PgM+1y+fLk6dOjANS/IuQK80wnIN3v37rX69u1rlStXzvLw8LB8fX2t8PBwa+LEiVZiYqJjvZSUFGvEiBFW+fLlLXd3d6tMmTLWkCFDnNaxrGu3n7Zv3z7Dfq6/RTerW6Ut69otpDVq1LA8PDysKlWqWJ999lmGW6VXrFhhdezY0SpZsqTl4eFhlSxZ0urWrZu1d+/eDPu4/nbin376yQoPD7e8vLwsPz8/q0OHDtauXbuc1knf3/W3Ys+cOTPDLbeZ+eut0lnJ6lbpgQMHWsHBwZaXl5cVHh5uRUdHZ3qL8zfffGNVq1bNcnNzczrOZs2aWdWrV890n3/dzsWLF62QkBCrTp06VkpKitN6L7/8suXi4mJFR0ff8BhOnz5tubm5WXPmzMl0eVhYmCXJev311zMse+GFFyxJ1n333ZdhWXJysjVmzBirevXqlt1utwIDA626detaI0aMsOLi4hzrXX+rtGVZ1tatW60mTZpYdrvdKl26tBUVFWVNmDDBkmSdOnXK6bnZea9almVNnz7dqlChguXq6prhtulVq1ZZ7dq1s/z9/S1PT0+rYsWKVmRkpLVp0yanbSxYsMAKDQ217Ha7Va1aNevrr7/O9KsBdu/ebUmyfvrppwxzA27GZll8tSEA3Ezv3r21d+9erV27tqCnkqWXXnpJ06ZNU3x8fIH9uojseumll7RmzRpt3ryZT16QY8QLAGTD0aNHVblyZa1YscLpotyCkpCQ4HS31fnz51W5cmXVqVNHy5cvL8CZ3dz58+cVEhKi+fPn5+k1WLhzES8AYKDatWurefPmCg0N1enTpzVjxgydOHFCK1ascPoVCsCdiAt2AcBA999/v7766it99NFHstlsqlOnjmbMmEG44H8Cn7wAAACj8D0vAADAKMQLAAAwCvECAACMckdesOt1z/MFPQUAeWT8lMEFPQUAeeSpf2Tvd8jxyQsAADAK8QIAAIxCvAAAAKMQLwAAwCjECwAAMArxAgAAjEK8AAAAoxAvAADAKMQLAAAwCvECAACMQrwAAACjEC8AAMAoxAsAADAK8QIAAIxCvAAAAKMQLwAAwCjECwAAMArxAgAAjEK8AAAAoxAvAADAKMQLAAAwCvECAACMQrwAAACjEC8AAMAoxAsAADAK8QIAAIxCvAAAAKMQLwAAwCjECwAAMArxAgAAjEK8AAAAoxAvAADAKMQLAAAwCvECAACMQrwAAACjEC8AAMAoxAsAADAK8QIAAIxCvAAAAKMQLwAAwCjECwAAMArxAgAAjEK8AAAAoxAvAADAKMQLAAAwCvECAACMQrwAAACjEC8AAMAoxAsAADAK8QIAAIxCvAAAAKMQLwAAwCjECwAAMArxAgAAjEK8AAAAoxAvAADAKMQLAAAwCvECAACMQrwAAACjEC8AAMAoxAsAADAK8QIAAIxCvAAAAKMQLwAAwCjECwAAMArxAgAAjEK8AAAAoxAvAADAKMQLAAAwCvECAACMQrwAAACjEC8AAMAoxAsAADAK8QIAAIxCvAAAAKMQLwAAwCjECwAAMArxAgAAjEK8AAAAoxAvAADAKMQLAAAwCvECAACMQrwAAACjEC8AAMAoxAsAADAK8QIAAIziVtATwP+Wvo80Vt+HmyikZJAkaffBUxr90fdatn6XAv0Kaegz7dXqH1VVpkSgzl2I17erf9OIyUt0MT5RkvREh4aaPrJHptsu2/I1nb0QL0l6+tGm6vdYU4WUDNKxUxc0ZsaP+nzJr451O7aspcG926limSJyd3PV/qNnNX7OCn2xdGMevwLAne3PP37Txu+/1OnD+3Q5NkYPvjBMd9UNdyzfsPBT/fHLal06f1aubu4qXu4uNX44UsEVQyVJcWdP6d+L5+rorm26EndB3gGFFdqolf7xYDe5urk7tnP26EGtmPOhTh3aIy9ff93TupMatH800zn98e9VWjolShXrhKnTiyPy9gVAviBekK+On47V0InfaP/Rs7LJpic6NNSX457SP7q+LZvNpuCi/hoybqF2HzylssFBmvhGVwUX9Vf3wTMkSV8t26LlG3Y5bfOjET3kaXd3hEvfRxprZP8Oem7UF9q084jq1yinSUO7KfbiFX23ZockKSbuit75+AftOXxaySmpur9JDX00/AmdjYnXT9G78/dFAe4gKUmJKlqmgmo0aafFE0dmWB5YorRa9Xhe/kWDdTU5SZt//FpfvTtEvd+ZpUJ+AYo5eUxWmqU2kS8qoHgpnfvzsJbPHKeUpEQ17/aUJCkp4bK+em+IQqrdo9YRL+jcn4f044yx8izkrbtbtHfaX9zZU/r5/6arVOUa+XL8yB/EC/JVejykGz7pW/V9pLEa3F1esxdFq9ugjx3LDv15TsM//FafvPWkXF1dlJqapsSkFCUmpTjWKRLoo+YNKqvfiLmOse7tG2jGgvX6atkWSdLh4+dVt3pZDYxs49j/2s37nOYx6YvVerxDQzW6pwLxAtyG8rUaqHytBlkuDw1r6fS4efentWPNDzp77JBCqt+j8nfXV/m76zuWBxQL1oVTx7R95RJHvOzesFJpV6+qXZ+BcnVzV5HS5XTm6EFt+nGBU7ykpaXqu6lvq1HnHjq+d4cSr8Tn8tGioBRovJw7d06ffPKJoqOjderUKUlSiRIl1KhRI0VGRqpo0aIFOT3kMRcXmx5qU0feXh765bdDma7j5+upi5cTlZqalunyxx9ooCuJyVr40zbHmIe7mxKTU5zWS0hMUb0aIXJzc9HVqxm31bxBZVUuV0z/HH/g1g8IQI6kXk3Rb6u+k72Qt4qWrZDleklXLsvT29fx+OT+3SpVpYbTaaRyNetq49J5Srx8ybFu9KK5KuQXoJrN7tPxvTsybBfmKrB42bhxo9q1a6dChQqpdevWqly5siTp9OnTmjBhgt5++239+OOPqlevXkFNEXmkeqWSWj17oDw93BSfkKTHBk7XHwdPZVivcIC3hvS9T58s2JDltiI6hWne95ucPo35KXq3Ijs10rerftPW3cdUp1pZRXZuJA93NxUJ8NGpcxclSX4+njrw41uyu7spNS1NL0bN08pf/sj9Awbg5MC2f2vp5NFKSU6Sj3+QHh78tgr5+me67oXTx7X1p2/UrOtTjrHLcTHyL1rCaT1vv8D/LLsgT29f/bl3h3as+UE9Rk3JuwNBgSmweOnfv78eeeQRTZ06VTabzWmZZVnq16+f+vfvr+jo6BtuJykpSUlJSc7PT0uVzcU11+eM3LH38Gk17Bolfx8vdW59j6aP7KG2fcY7BYyvt6cWTnhGuw+e1JvTlma6nYZ3l1dohWD1/uenTuNR039Q8cJ++nn2INls0pmYS5r77S8a2LON0tIsx3qXLiepYdco+XjZ1aJhFY0Z2EWH/jyf4ZQSgNxVNrSWeoyaooRLF/X7z9/p20lv6vFhE1ToPwGS7lLMOX393huqXL+p7m5+f7a3n5xwRd9PG6O2PV/KMopgtgKLl+3bt2vWrFkZwkWSbDabXn75Zd1zzz033U5UVJRGjHC+ety1eH25B2d9zhUFK+Vqqg4eOydJ2rr7mOpWL6vnujVX/7f+T5LkU8iuxZOe1aUriXpswPRMT/NIUmTnMG3745i27j7mNJ6YlKJ+I+bq+be+UPEgP508F6feD4XrYnyC46Je6Vokp8/jt73HVaV8CQ3u1ZZ4AfKYu91LgcVLKbB4KZWsFKoZr0Tq959/UMMO3RzrxF84ry/fHqySlaqpbc+XnJ7v7R+kyxcvOI2lP/b2D1TsmZO6eO60Fn7wL8dyy7r2D5exPe9Vr7c/UUDxknl0dMgPBRYvJUqU0K+//qqqVatmuvzXX39V8eLFb7qdIUOGaMCAAU5jxZq8mitzRP5wsdlk97j2VvT19tS3k59TUvJVPfzSNCUlX830Od5eHnqoTR39a+LiLLd79Wqajp+JlSQ90q6uvl+70/EX2M3mASD/WGmWUq/+99TvpZhz+vLtwSpW7i616ztQNhfnryQLrhSq9QtmKfXqVbm6Xfsze2THFgUGl5ant6/c3O2KeGua03PWLZillMQEtXj8GfkW5npK0xXY39SDBg3SU089pc2bN6tVq1aOUDl9+rRWrFih6dOn67333rvpdux2u+x2u9MYp4z+vkb2f1A/rt+pYycvyNfbU4/dV09N692lDs9Olq+3p5ZMfk5enh7q+cZs+Xl7ys/bU5J09kK80ymfh9vVlZurS6bfy1KpbDHVqxGijTsOK9C3kF7o0VLVKpZUn6FzHOsM6tVWW3Ye1cE/z8ru4aZ7G1dX9/YN9ELU/+X9iwDcwZITExR7+oTj8cWzp3TmyAF5+vjKy8dX/178hSreEyafgCAlXIrT1hXfKj72nCrXbyrpWrjMf3uQ/AoXV7OuTynhYpxjW94B174fKjSspaK/+UzLZoxV/faP6tzxw9qybKFadO8nSXLz8FCR0uWd5uVZyEeSMozDTAUWL88995yKFCmicePGafLkyUpNTZUkubq6qm7dupo1a5YefTTzLxyCuYoG+WjGqCdVooif4uITtWPfcXV4drJW/vKHmtS9Sw3uvvYXy65vhzs9r8r9/9LRkzGOx5GdwvTNyu2Ki0/IsA9XV5te7NFSlUOKK+VqqtZs2qsWke87Pd/b00PjX39UpYoFKCEpRXsPn1avf8523F4N4NacPrRX898e7Hi8+otrn4BUb9xGrSNeVMzJY9q1brkS4i/K08dXJcpXUdfXx6pI6XKSpCM7tyj29AnFnj6hj17u7rTtgbOXSZLshbz18KAorZjzoT4b/py8fPwV1umJDN/xgjuXzbrR5+j5JCUlRefOXbv2oEiRInJ3d7/JM27M657nc2NaAP6Gxk8ZfPOVABjpqX+EZGu9v8UJfnd3dwUHBxf0NAAAgAH4xYwAAMAoxAsAADAK8QIAAIxCvAAAAKMQLwAAwCjECwAAMArxAgAAjEK8AAAAoxAvAADAKMQLAAAwCvECAACMQrwAAACjEC8AAMAoxAsAADAK8QIAAIxCvAAAAKMQLwAAwCjECwAAMArxAgAAjEK8AAAAoxAvAADAKMQLAAAwCvECAACMQrwAAACjEC8AAMAoxAsAADAK8QIAAIxCvAAAAKMQLwAAwCjECwAAMArxAgAAjEK8AAAAoxAvAADAKMQLAAAwCvECAACMQrwAAACjEC8AAMAoxAsAADAK8QIAAIxCvAAAAKMQLwAAwCjECwAAMArxAgAAjEK8AAAAoxAvAADAKMQLAAAwCvECAACMQrwAAACjEC8AAMAoxAsAADAK8QIAAIxCvAAAAKMQLwAAwCjECwAAMArxAgAAjEK8AAAAoxAvAADAKMQLAAAwCvECAACMQrwAAACjEC8AAMAoxAsAADAK8QIAAIxCvAAAAKMQLwAAwCjECwAAMArxAgAAjEK8AAAAoxAvAADAKMQLAAAwCvECAACMQrwAAACjEC8AAMAotxQva9eu1RNPPKGwsDAdP35ckjRnzhytW7cuVycHAABwvRzHy4IFC9SuXTt5eXlp69atSkpKkiTFxcVp9OjRuT5BAACAv8pxvLz55puaOnWqpk+fLnd3d8d4eHi4tmzZkquTAwAAuF6O42XPnj1q2rRphnF/f3/FxsbmxpwAAACylON4KVGihPbv359hfN26dapQoUKuTAoAACArOY6Xvn376sUXX9Qvv/wim82mEydOaO7cuRo0aJCeeeaZvJgjAACAg1tOn/Daa68pLS1NrVq10pUrV9S0aVPZ7XYNGjRI/fv3z4s5AgAAONgsy7Ju5YnJycnav3+/4uPjVa1aNfn4+OT23G6Z1z3PF/QUAOSR8VMGF/QUAOSRp/4Rkq31cvzJSzoPDw9Vq1btVp8OAABwS3IcLy1atJDNZsty+cqVK29rQgAAADeS43ipXbu20+OUlBRt27ZNO3bsUERERG7NCwAAIFM5jpdx48ZlOj58+HDFx8ff9oQAAABuJNd+MeMTTzyhTz75JLc2BwAAkKlbvmD3etHR0fL09Mytzd2WCxs/LOgpAMgj5y4lF/QUABSwHMdLly5dnB5blqWTJ09q06ZNGjp0aK5NDAAAIDM5jhd/f3+nxy4uLqpSpYpGjhyptm3b5trEAAAAMpOjL6lLTU3V+vXrVbNmTQUGBublvG5L4tWCngGAvMJpI+DOVTrQI1vr5eiCXVdXV7Vt25bfHg0AAApMju82qlGjhg4ePJgXcwEAALipHMfLm2++qUGDBmnJkiU6efKkLl686PQfAABAXsr2NS8jR47UwIED5evr+98n/+XXBFiWJZvNptTU1NyfZQ5xzQtw5+KaF+DOld1rXrIdL66urjp58qR27959w/WaNWuWrR3nJeIFuHMRL8CdK7vxku1bpdMb5+8QJwAA4H9Xjq55udFvkwYAAMgPOfqSusqVK980YGJiYm5rQgAAADeSo3gZMWJEhm/YBQAAyE/ZvmDXxcVFp06dUrFixfJ6TreNC3aBOxcX7AJ3rlz/hl2udwEAAH8H2Y6XHPwKJAAAgDyT7Wte0tLS8nIeAAAA2ZLjXw8AAABQkIgXAABgFOIFAAAYhXgBAABGIV4AAIBRiBcAAGAU4gUAABiFeAEAAEYhXgAAgFGIFwAAYBTiBQAAGIV4AQAARiFeAACAUYgXAABgFOIFAAAYhXgBAABGIV4AAIBRiBcAAGAU4gUAABiFeAEAAEYhXgAAgFGIFwAAYBTiBQAAGIV4AQAARiFeAACAUYgXAABgFOIFAAAYhXgBAABGIV4AAIBRiBcAAGAU4gUAABiFeAEAAEYhXgAAgFGIFwAAYBTiBQAAGIV4AQAARiFeAACAUYgXAABgFOIFAAAYhXgBAABGIV4AAIBRiBcAAGAU4gUAABiFeAEAAEYhXgAAgFGIFwAAYBTiBQAAGIV4AQAARiFeAACAUYgXAABgFOIFAAAYhXgBAABGIV4AAIBRiBcAAGAU4gUAABiFeAEAAEYhXgAAgFGIFwAAYBTiBQAAGIV4AQAARiFeAACAUYgXAABgFOIFAAAYhXgBAABGIV4AAIBRiBcAAGAU4gUAABiFeAEAAEYhXgAAgFGIFwAAYBS3gp4AcL372rTUiRPHM4w/1rW7Inr11v1tW2X6vHfHfqC27e5zPP5m4dea8+lMHTl8WN4+Pmrb9l69PnRYns0bQEaLF8zT4q/n6fTJE5KkkAoV1aNXPzVs1ESSdOLPY5o68T3t2L5VKcnJqh8WrucHDFFQ4SJO2/n3+jWaM2OqDh7YKw8PD919Tz2NemeCJCkuLlZRw17Twf17dTEuVgGBQWrUtIV6P/OivL198veAkS9slmVZBT2J3JZ4taBngNsRExOjtNRUx+P9+/fp6T499fHMT1Wnbj1diIlxWv+rL+dp9swZWrF6nQp5e0uSPp01U5/O/kQDBr6imnfXUkLCFZ04flzNW2YePjDHuUvJBT0F5MCGtavl6uqiUqVDZMnSsqWLNX/uTE379EsVDy6pvk88pIqVqiii77OSpJkffajz587qw4/nysXl2smBNSuXa+zbw9W734uqXa+BUlNTdfjAPjVvfa8k6dLFOK1a/oOqVKuhgIBAHf/zqCa895buqhKqN0a+U1CHjltQOtAjW+sRL/jbeyfqLa35ebW+/X6ZbDZbhuWPPtRJodWqacSo0ZKki3FxatOyqSZMmqqG/wjL7+kijxEv5uvUNlxPPT9QxYqX0JCXn9Gi5esdn5DEx19SpzbhGjN+muo2CFPq1avq3rmdIvo+p/sf7JLtfXw9b67mz52p/1v8U14dBvJAduOFa17wt5aSnKylSxarU5eHMg2XXTt3aM8fu9W5y8OOsejo9UpLS9OZ06fVqcN9atOyqQYPeFGnTp7Mz6kDuE5qaqpWLv9eiQkJqlazlpKTkyWbTe7u//2B5eFhl83FRTu2b5Uk7duzW+fOnpGLi01PP/mIHmnfQq+91E+HDuzLcj/nzp7RutU/6e576uX5MaFg/K3j5dixY+rVq9cN10lKStLFixed/ktKSsqnGSKvrVz5ky5duqQHO3XOdPnCBV+pQoWKqn1PHcfYn8f+VFqapY+nT9XgV1/X++MmKC4uTk/37amUZP7VDuS3g/v3qn2LBrq3aV19MGaURoz5QOXKV1S1GnfLy9NL0yeNU2JighISrmjahPeUlpqq8+fPSpJOnPhTkjT74yl6PPIpvfX+h/L189OAZ3vpYlyc037eHPqK7m9WX491aKVC3j4a9PqIfD9W5I+/dbzExMRo9uzZN1wnKipK/v7+Tv+9OyYqn2aIvLZwwQKFN26qYsWKZ1iWmJio779bok4PPew0bllpuno1Ra8O+afCGzfR3bVq6+13x+rokSP69ddf8mvqAP6jTEh5ffTpV5o0Y64e7PKoxoz8pw4fOqCAwCD9a/T7il63Wg+0aKgHWzdSfPwl3VUlVC62az+erLQ0SdLjkX3VtGUbVa5aXYP/+aZsNpt+Xvmj036efekVTZ09T6PemaATx49pyvh38/1YkT8K9G6jxYsX33D5wYMHb7qNIUOGaMCAAU5jlqv9tuaFv4cTJ47rl39v0NjxEzNdvnzZD0pISFSHBzs5jRcpWlSSVLFiJcdYUFCQAgIDOXUEFAB3d3eVKlNWklS5anXt2bVDX8/7TANeG6Z6DRvpswXfKy72glxdXeXj66eH72+u4FKlJUlBRa79eQ4pV9GxPQ8PDwWXLK0zp0457SeocBEFFS6isuUqyNfPXy/1i9ATvZ5W4f9sA3eOAo2XTp06yWaz6UbXDGd2ncNf2e122e3OscIFu3eGbxZ+raCgwmrStHmmyxd9vUDNW7RUUFCQ03j6KaTDhw+peIkSkqS42FjFXrig4JIl83TOAG4uzbIynML1DwiUJG3d9ItiL8SoUZPmkqTKVavJ3cNDx44eVs3a1/5sX72aolMnj6t4cHCW+7Csa5/YcKr4zlSg8RIcHKzJkyerY8eOmS7ftm2b6tatm8+zwt9BWlqavln4tTp07CQ3t4xv06NHjmjzpo2aNOWjDMvKlSuvFi1baUzUW/rX8JHy9vHRhHFjVa58BdVv0DA/pg/gPz6e/IEahDVWseLBunLlslYu+07bt2zU2x9MlST9sGShyparoICAIO38fZsmjRujh7r2UJmQ8pIkb28fdej8qGZPn6RixUuoeIlgzftsliSpWcu2kqRfNqzRhZjzqhJaQ15ehXT40AFNm/i+atx9j0qULFUgx428VaDxUrduXW3evDnLeLnZpzK4c/07eoNOnjyhTl0eynT5ooULVLx4CYWFN850+ZtR7+jdMaP1/LNPy8Xmorr162vKtI/l7u6el9MGcJ0LF2L09og3FHP+rLx9fFWh4l16+4OpqtewkSTp2JHD+njyeF26GKfiwaX0eGRfPdztSadtPN1/gFxdXRU1fIiSk5JUtXpNvT9phnz9/CVJHnZPLf1mgSZ/8K5SUpJVtFgJNWneSt2e7J3vx4v8UaDf87J27VpdvnxZ9957b6bLL1++rE2bNqlZs2Y52i6njYA7F9/zAty5+JI6AHck4gW4c/EldQAA4I5EvAAAAKMQLwAAwCjECwAAMArxAgAAjEK8AAAAoxAvAADAKMQLAAAwCvECAACMQrwAAACjEC8AAMAoxAsAADAK8QIAAIxCvAAAAKMQLwAAwCjECwAAMArxAgAAjEK8AAAAoxAvAADAKMQLAAAwCvECAACMQrwAAACjEC8AAMAoxAsAADAK8QIAAIxCvAAAAKMQLwAAwCjECwAAMArxAgAAjEK8AAAAoxAvAADAKMQLAAAwCvECAACMQrwAAACjEC8AAMAoxAsAADAK8QIAAIxCvAAAAKMQLwAAwCjECwAAMArxAgAAjEK8AAAAoxAvAADAKMQLAAAwCvECAACMQrwAAACjEC8AAMAoxAsAADAK8QIAAIxCvAAAAKMQLwAAwCjECwAAMArxAgAAjEK8AAAAoxAvAADAKMQLAAAwCvECAACMQrwAAACjEC8AAMAoxAsAADAK8QIAAIxCvAAAAKMQLwAAwCjECwAAMArxAgAAjEK8AAAAoxAvAADAKMQLAAAwCvECAACMQrwAAACjEC8AAMAoxAsAADAK8QIAAIxCvAAAAKMQLwAAwCjECwAAMArxAgAAjEK8AAAAoxAvAADAKMQLAAAwCvECAACMQrwAAACjEC8AAMAoxAsAADAK8QIAAIxCvAAAAKPYLMuyCnoSwK1KSkpSVFSUhgwZIrvdXtDTAZCL+PONrBAvMNrFixfl7++vuLg4+fn5FfR0AOQi/nwjK5w2AgAARiFeAACAUYgXAABgFOIFRrPb7Ro2bBgX8wF3IP58IytcsAsAAIzCJy8AAMAoxAsAADAK8QIAAIxCvAAAAKMQLzDapEmTVK5cOXl6eqphw4b69ddfC3pKAG7TmjVr1KFDB5UsWVI2m02LFi0q6Cnhb4Z4gbHmzZunAQMGaNiwYdqyZYtq1aqldu3a6cyZMwU9NQC34fLly6pVq5YmTZpU0FPB3xS3SsNYDRs2VP369fXhhx9KktLS0lSmTBn1799fr732WgHPDkBusNlsWrhwoTp16lTQU8HfCJ+8wEjJycnavHmzWrdu7RhzcXFR69atFR0dXYAzAwDkNeIFRjp37pxSU1NVvHhxp/HixYvr1KlTBTQrAEB+IF4AAIBRiBcYqUiRInJ1ddXp06edxk+fPq0SJUoU0KwAAPmBeIGRPDw8VLduXa1YscIxlpaWphUrVigsLKwAZwYAyGtuBT0B4FYNGDBAERERqlevnho0aKAPPvhAly9fVs+ePQt6agBuQ3x8vPbv3+94fOjQIW3btk1BQUEqW7ZsAc4MfxfcKg2jffjhh3r33Xd16tQp1a5dWxMmTFDDhg0LeloAbsPq1avVokWLDOMRERGaNWtW/k8IfzvECwAAMArXvAAAAKMQLwAAwCjECwAAMArxAgAAjEK8AAAAoxAvAADAKMQLAAAwCvEC4G8rMjJSnTp1cjxu3ry5XnrppXyfx+rVq2Wz2RQbG5vv+waQEfECIMciIyNls9lks9nk4eGhSpUqaeTIkbp69Wqe7vfrr7/WqFGjsrUuwQHcufjdRgBuyb333quZM2cqKSlJ3333nZ577jm5u7tryJAhTuslJyfLw8MjV/YZFBSUK9sBYDY+eQFwS+x2u0qUKKGQkBA988wzat26tRYvXuw41fPWW2+pZMmSqlKliiTp2LFjevTRRxUQEKCgoCB17NhRhw8fdmwvNTVVAwYMUEBAgAoXLqxXXnlF1//2kutPGyUlJenVV19VmTJlZLfbValSJc2YMUOHDx92/G6cwMBA2Ww2RUZGSrr228ejoqJUvnx5eXl5qVatWvrqq6+c9vPdd9+pcuXK8vLyUosWLZzmCaDgES8AcoWXl5eSk5MlSStWrNCePXu0fPlyLVmyRCkpKWrXrp18fX21du1arV+/Xj4+Prr33nsdz3n//fc1a9YsffLJJ1q3bp1iYmK0cOHCG+7zySef1BdffKEJEyZo9+7dmjZtmnx8fFSmTBktWLBAkrRnzx6dPHlS48ePlyRFRUXp008/1dSpU7Vz5069/PLLeuKJJ/Tzzz9LuhZZXbp0UYcOHbRt2zb16dNHr732Wl69bABuhQUAORQREWF17NjRsizLSktLs5YvX27Z7XZr0KBBVkREhFW8eHErKSnJsf6cOXOsKlWqWGlpaY6xpKQky8vLy/rxxx8ty7Ks4OBg65133nEsT0lJsUqXLu3Yj2VZVrNmzawXX3zRsizL2rNnjyXJWr58eaZzXLVqlSXJunDhgmMsMTHRKlSokLVhwwandXv37m1169bNsizLGjJkiFWtWjWn5a+++mqGbQEoOFzzAuCWLFmyRD4+PkpJSVFaWpq6d++u4cOH67nnnlPNmjWdrnPZvn279u/fL19fX6dtJCYm6sCBA4qLi9PJkyfVsGFDxzI3NzfVq1cvw6mjdNu2bZOrq6uaNWuW7Tnv379fV65cUZs2bZzGk5OTdc8990iSdu/e7TQPSQoLC8v2PgDkPeIFwC1p0aKFpkyZIg8PD5UsWVJubv/968Tb29tp3fj4eNWtW1dz587NsJ2iRYve0v69vLxy/Jz4+HhJ0tKlS1WqVCmnZXa7/ZbmASD/ES8Abom3t7cqVaqUrXXr1KmjefPmqVixYvLz88t0neDgYP3yyy9q2rSpJOnq1avavHmz6tSpk+n6NWvWVFpamn7++We1bt06w/L0T35SU1MdY9WqVZPdbtfRo0ez/MQmNDRUixcvdhr797//ffODBJBvuGAXQJ57/PHHVaRIEXXs2FFr167VoUOHtHr1ar3wwgv6888/JUkvvvii3n77bS1atEh//PGHnn322Rt+R0u5cuUUERGhXr16adGiRY5tzp8/X5IUEhIim82mJUuW6OzZs4qPj5evr68GDRqkl19+WbNnz9aBAwe0ZcsWTZw4UbNnz5Yk9evXT/v27dPgwYO1Z88eff7555o1a1Zev0QAcoB4AZDnChUqpDVr1qhs2bLq0qWLQkND1bt3byUmJjo+iRk4cKB69OihiIgIhYWFydfXV507d77hdqdMmaKHH35Yzz77rKpWraq+ffvq8uXLkqRSpUppxIgReu2111S8eHE9//zzkqRRo0Zp6NChioqKUmhoqO69914tXbpU5cuXlySVLVtWCxYs0KJFi1SrVi1NnTpVo0ePzsNXB0BO2aysroYDAAD4G+KTFwAAYBTiBQAAGIV4AQAARiFeAACAUYgXAABgFOIFAAAYhXgBAABGIV4AAIBRiBcAAGAU4gUAABiFeAEAAEYhXgAAgFH+HwLwb49cif/NAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "weighted_model.eval()\n",
    "y_true_weighted = []\n",
    "y_pred_weighted = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        output = weighted_model(data)\n",
    "        \n",
    "        y_true_weighted.extend(target.cpu().numpy())\n",
    "        y_pred_weighted.extend((output > 0.5).cpu().numpy())\n",
    "\n",
    "y_true_weighted = np.array(y_true_weighted)\n",
    "y_pred_weighted = np.array(y_pred_weighted)\n",
    "\n",
    "print(\"\\nClassification Report (Weighted):\")\n",
    "print(classification_report(y_true_weighted, y_pred_weighted))\n",
    "print(\"\\nConfusion Matrix (Weighted):\")\n",
    "cm_weighted = confusion_matrix(y_true_weighted, y_pred_weighted)\n",
    "sns.heatmap(cm_weighted, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix (Weighted)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train:test = 7:3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_size = len(X)\n",
    "train_size = int(0.7 * total_size)\n",
    "test_size = total_size - train_size\n",
    "\n",
    "dataset = HeartDiseaseDataset(X, y)\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1024, shuffle=False)\n",
    "\n",
    "input_features = X.shape[1]  \n",
    "\n",
    "weighted_model_2 = HeartDiseaseMLPClassifier(input_size=input_features, class_frequencies=class_frequencies).to(device)\n",
    "\n",
    "optimizer = optim.Adam(weighted_model_2.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training...\n",
      "Epoch: 1/20, Batch: 0/1388, Loss: 1.6758\n",
      "Epoch: 1/20, Batch: 50/1388, Loss: 1.2104\n",
      "Epoch: 1/20, Batch: 100/1388, Loss: 1.0015\n",
      "Epoch: 1/20, Batch: 150/1388, Loss: 1.0412\n",
      "Epoch: 1/20, Batch: 200/1388, Loss: 1.5275\n",
      "Epoch: 1/20, Batch: 250/1388, Loss: 1.5474\n",
      "Epoch: 1/20, Batch: 300/1388, Loss: 1.6520\n",
      "Epoch: 1/20, Batch: 350/1388, Loss: 1.2456\n",
      "Epoch: 1/20, Batch: 400/1388, Loss: 0.9137\n",
      "Epoch: 1/20, Batch: 450/1388, Loss: 1.0506\n",
      "Epoch: 1/20, Batch: 500/1388, Loss: 0.7642\n",
      "Epoch: 1/20, Batch: 550/1388, Loss: 0.9763\n",
      "Epoch: 1/20, Batch: 600/1388, Loss: 0.8537\n",
      "Epoch: 1/20, Batch: 650/1388, Loss: 0.9318\n",
      "Epoch: 1/20, Batch: 700/1388, Loss: 0.7917\n",
      "Epoch: 1/20, Batch: 750/1388, Loss: 0.8473\n",
      "Epoch: 1/20, Batch: 800/1388, Loss: 0.7795\n",
      "Epoch: 1/20, Batch: 850/1388, Loss: 0.9775\n",
      "Epoch: 1/20, Batch: 900/1388, Loss: 0.9419\n",
      "Epoch: 1/20, Batch: 950/1388, Loss: 0.7233\n",
      "Epoch: 1/20, Batch: 1000/1388, Loss: 1.1763\n",
      "Epoch: 1/20, Batch: 1050/1388, Loss: 1.2521\n",
      "Epoch: 1/20, Batch: 1100/1388, Loss: 0.8029\n",
      "Epoch: 1/20, Batch: 1150/1388, Loss: 0.8513\n",
      "Epoch: 1/20, Batch: 1200/1388, Loss: 0.8702\n",
      "Epoch: 1/20, Batch: 1250/1388, Loss: 0.9686\n",
      "Epoch: 1/20, Batch: 1300/1388, Loss: 0.9483\n",
      "Epoch: 1/20, Batch: 1350/1388, Loss: 0.9284\n",
      "Epoch: 1, Train Loss: 1.0178\n",
      "Epoch: 2/20, Batch: 0/1388, Loss: 1.0578\n",
      "Epoch: 2/20, Batch: 50/1388, Loss: 0.7761\n",
      "Epoch: 2/20, Batch: 100/1388, Loss: 1.0752\n",
      "Epoch: 2/20, Batch: 150/1388, Loss: 1.1627\n",
      "Epoch: 2/20, Batch: 200/1388, Loss: 0.9531\n",
      "Epoch: 2/20, Batch: 250/1388, Loss: 1.2482\n",
      "Epoch: 2/20, Batch: 300/1388, Loss: 0.9092\n",
      "Epoch: 2/20, Batch: 350/1388, Loss: 1.2186\n",
      "Epoch: 2/20, Batch: 400/1388, Loss: 0.8382\n",
      "Epoch: 2/20, Batch: 450/1388, Loss: 0.9920\n",
      "Epoch: 2/20, Batch: 500/1388, Loss: 0.8647\n",
      "Epoch: 2/20, Batch: 550/1388, Loss: 1.0171\n",
      "Epoch: 2/20, Batch: 600/1388, Loss: 1.0134\n",
      "Epoch: 2/20, Batch: 650/1388, Loss: 0.9831\n",
      "Epoch: 2/20, Batch: 700/1388, Loss: 1.0305\n",
      "Epoch: 2/20, Batch: 750/1388, Loss: 1.1836\n",
      "Epoch: 2/20, Batch: 800/1388, Loss: 0.9287\n",
      "Epoch: 2/20, Batch: 850/1388, Loss: 1.1432\n",
      "Epoch: 2/20, Batch: 900/1388, Loss: 1.3835\n",
      "Epoch: 2/20, Batch: 950/1388, Loss: 1.1056\n",
      "Epoch: 2/20, Batch: 1000/1388, Loss: 0.7697\n",
      "Epoch: 2/20, Batch: 1050/1388, Loss: 0.8827\n",
      "Epoch: 2/20, Batch: 1100/1388, Loss: 0.8957\n",
      "Epoch: 2/20, Batch: 1150/1388, Loss: 0.7713\n",
      "Epoch: 2/20, Batch: 1200/1388, Loss: 0.7745\n",
      "Epoch: 2/20, Batch: 1250/1388, Loss: 0.7712\n",
      "Epoch: 2/20, Batch: 1300/1388, Loss: 1.0000\n",
      "Epoch: 2/20, Batch: 1350/1388, Loss: 1.5296\n",
      "Epoch: 2, Train Loss: 0.9821\n",
      "Epoch: 3/20, Batch: 0/1388, Loss: 0.8175\n",
      "Epoch: 3/20, Batch: 50/1388, Loss: 0.9444\n",
      "Epoch: 3/20, Batch: 100/1388, Loss: 1.3100\n",
      "Epoch: 3/20, Batch: 150/1388, Loss: 0.8313\n",
      "Epoch: 3/20, Batch: 200/1388, Loss: 0.9363\n",
      "Epoch: 3/20, Batch: 250/1388, Loss: 1.0118\n",
      "Epoch: 3/20, Batch: 300/1388, Loss: 1.0666\n",
      "Epoch: 3/20, Batch: 350/1388, Loss: 1.3265\n",
      "Epoch: 3/20, Batch: 400/1388, Loss: 0.7865\n",
      "Epoch: 3/20, Batch: 450/1388, Loss: 0.8191\n",
      "Epoch: 3/20, Batch: 500/1388, Loss: 0.6839\n",
      "Epoch: 3/20, Batch: 550/1388, Loss: 0.8030\n",
      "Epoch: 3/20, Batch: 600/1388, Loss: 0.7947\n",
      "Epoch: 3/20, Batch: 650/1388, Loss: 0.8119\n",
      "Epoch: 3/20, Batch: 700/1388, Loss: 0.8578\n",
      "Epoch: 3/20, Batch: 750/1388, Loss: 1.0540\n",
      "Epoch: 3/20, Batch: 800/1388, Loss: 1.0931\n",
      "Epoch: 3/20, Batch: 850/1388, Loss: 0.9611\n",
      "Epoch: 3/20, Batch: 900/1388, Loss: 0.9355\n",
      "Epoch: 3/20, Batch: 950/1388, Loss: 1.0291\n",
      "Epoch: 3/20, Batch: 1000/1388, Loss: 0.6972\n",
      "Epoch: 3/20, Batch: 1050/1388, Loss: 0.8576\n",
      "Epoch: 3/20, Batch: 1100/1388, Loss: 0.7133\n",
      "Epoch: 3/20, Batch: 1150/1388, Loss: 1.1297\n",
      "Epoch: 3/20, Batch: 1200/1388, Loss: 1.1987\n",
      "Epoch: 3/20, Batch: 1250/1388, Loss: 0.9719\n",
      "Epoch: 3/20, Batch: 1300/1388, Loss: 1.0963\n",
      "Epoch: 3/20, Batch: 1350/1388, Loss: 1.0807\n",
      "Epoch: 3, Train Loss: 0.9757\n",
      "Epoch: 4/20, Batch: 0/1388, Loss: 0.9487\n",
      "Epoch: 4/20, Batch: 50/1388, Loss: 0.8619\n",
      "Epoch: 4/20, Batch: 100/1388, Loss: 1.0190\n",
      "Epoch: 4/20, Batch: 150/1388, Loss: 0.9681\n",
      "Epoch: 4/20, Batch: 200/1388, Loss: 1.0367\n",
      "Epoch: 4/20, Batch: 250/1388, Loss: 0.7799\n",
      "Epoch: 4/20, Batch: 300/1388, Loss: 0.7974\n",
      "Epoch: 4/20, Batch: 350/1388, Loss: 1.2096\n",
      "Epoch: 4/20, Batch: 400/1388, Loss: 1.3348\n",
      "Epoch: 4/20, Batch: 450/1388, Loss: 0.8336\n",
      "Epoch: 4/20, Batch: 500/1388, Loss: 1.1104\n",
      "Epoch: 4/20, Batch: 550/1388, Loss: 1.1429\n",
      "Epoch: 4/20, Batch: 600/1388, Loss: 0.9809\n",
      "Epoch: 4/20, Batch: 650/1388, Loss: 0.7031\n",
      "Epoch: 4/20, Batch: 700/1388, Loss: 1.0320\n",
      "Epoch: 4/20, Batch: 750/1388, Loss: 1.1321\n",
      "Epoch: 4/20, Batch: 800/1388, Loss: 0.8005\n",
      "Epoch: 4/20, Batch: 850/1388, Loss: 0.8899\n",
      "Epoch: 4/20, Batch: 900/1388, Loss: 1.0804\n",
      "Epoch: 4/20, Batch: 950/1388, Loss: 1.0251\n",
      "Epoch: 4/20, Batch: 1000/1388, Loss: 1.2102\n",
      "Epoch: 4/20, Batch: 1050/1388, Loss: 0.6524\n",
      "Epoch: 4/20, Batch: 1100/1388, Loss: 0.9393\n",
      "Epoch: 4/20, Batch: 1150/1388, Loss: 1.0542\n",
      "Epoch: 4/20, Batch: 1200/1388, Loss: 0.7059\n",
      "Epoch: 4/20, Batch: 1250/1388, Loss: 1.2395\n",
      "Epoch: 4/20, Batch: 1300/1388, Loss: 0.9613\n",
      "Epoch: 4/20, Batch: 1350/1388, Loss: 0.8616\n",
      "Epoch: 4, Train Loss: 0.9745\n",
      "Epoch: 5/20, Batch: 0/1388, Loss: 0.9152\n",
      "Epoch: 5/20, Batch: 50/1388, Loss: 1.1340\n",
      "Epoch: 5/20, Batch: 100/1388, Loss: 0.7481\n",
      "Epoch: 5/20, Batch: 150/1388, Loss: 1.0668\n",
      "Epoch: 5/20, Batch: 200/1388, Loss: 1.0146\n",
      "Epoch: 5/20, Batch: 250/1388, Loss: 0.7966\n",
      "Epoch: 5/20, Batch: 300/1388, Loss: 0.8054\n",
      "Epoch: 5/20, Batch: 350/1388, Loss: 0.9257\n",
      "Epoch: 5/20, Batch: 400/1388, Loss: 0.8881\n",
      "Epoch: 5/20, Batch: 450/1388, Loss: 1.4112\n",
      "Epoch: 5/20, Batch: 500/1388, Loss: 1.4443\n",
      "Epoch: 5/20, Batch: 550/1388, Loss: 1.0497\n",
      "Epoch: 5/20, Batch: 600/1388, Loss: 0.8771\n",
      "Epoch: 5/20, Batch: 650/1388, Loss: 1.1795\n",
      "Epoch: 5/20, Batch: 700/1388, Loss: 0.8622\n",
      "Epoch: 5/20, Batch: 750/1388, Loss: 1.0255\n",
      "Epoch: 5/20, Batch: 800/1388, Loss: 0.9316\n",
      "Epoch: 5/20, Batch: 850/1388, Loss: 0.8113\n",
      "Epoch: 5/20, Batch: 900/1388, Loss: 0.9806\n",
      "Epoch: 5/20, Batch: 950/1388, Loss: 1.2808\n",
      "Epoch: 5/20, Batch: 1000/1388, Loss: 0.8553\n",
      "Epoch: 5/20, Batch: 1050/1388, Loss: 1.0998\n",
      "Epoch: 5/20, Batch: 1100/1388, Loss: 0.9731\n",
      "Epoch: 5/20, Batch: 1150/1388, Loss: 1.0274\n",
      "Epoch: 5/20, Batch: 1200/1388, Loss: 0.9516\n",
      "Epoch: 5/20, Batch: 1250/1388, Loss: 0.7964\n",
      "Epoch: 5/20, Batch: 1300/1388, Loss: 0.8959\n",
      "Epoch: 5/20, Batch: 1350/1388, Loss: 1.1241\n",
      "Epoch: 5, Train Loss: 0.9717\n",
      "Epoch: 6/20, Batch: 0/1388, Loss: 1.7205\n",
      "Epoch: 6/20, Batch: 50/1388, Loss: 0.9808\n",
      "Epoch: 6/20, Batch: 100/1388, Loss: 0.8328\n",
      "Epoch: 6/20, Batch: 150/1388, Loss: 1.0053\n",
      "Epoch: 6/20, Batch: 200/1388, Loss: 0.9681\n",
      "Epoch: 6/20, Batch: 250/1388, Loss: 0.9721\n",
      "Epoch: 6/20, Batch: 300/1388, Loss: 1.2179\n",
      "Epoch: 6/20, Batch: 350/1388, Loss: 0.8863\n",
      "Epoch: 6/20, Batch: 400/1388, Loss: 0.8939\n",
      "Epoch: 6/20, Batch: 450/1388, Loss: 0.7506\n",
      "Epoch: 6/20, Batch: 500/1388, Loss: 0.8456\n",
      "Epoch: 6/20, Batch: 550/1388, Loss: 0.8988\n",
      "Epoch: 6/20, Batch: 600/1388, Loss: 0.6138\n",
      "Epoch: 6/20, Batch: 650/1388, Loss: 1.0217\n",
      "Epoch: 6/20, Batch: 700/1388, Loss: 0.8477\n",
      "Epoch: 6/20, Batch: 750/1388, Loss: 0.8465\n",
      "Epoch: 6/20, Batch: 800/1388, Loss: 1.0755\n",
      "Epoch: 6/20, Batch: 850/1388, Loss: 0.8990\n",
      "Epoch: 6/20, Batch: 900/1388, Loss: 1.1757\n",
      "Epoch: 6/20, Batch: 950/1388, Loss: 0.8095\n",
      "Epoch: 6/20, Batch: 1000/1388, Loss: 1.0465\n",
      "Epoch: 6/20, Batch: 1050/1388, Loss: 1.4081\n",
      "Epoch: 6/20, Batch: 1100/1388, Loss: 0.7473\n",
      "Epoch: 6/20, Batch: 1150/1388, Loss: 0.8506\n",
      "Epoch: 6/20, Batch: 1200/1388, Loss: 0.9715\n",
      "Epoch: 6/20, Batch: 1250/1388, Loss: 0.8432\n",
      "Epoch: 6/20, Batch: 1300/1388, Loss: 0.7257\n",
      "Epoch: 6/20, Batch: 1350/1388, Loss: 1.1431\n",
      "Epoch: 6, Train Loss: 0.9692\n",
      "Epoch: 7/20, Batch: 0/1388, Loss: 1.2835\n",
      "Epoch: 7/20, Batch: 50/1388, Loss: 0.8957\n",
      "Epoch: 7/20, Batch: 100/1388, Loss: 1.2732\n",
      "Epoch: 7/20, Batch: 150/1388, Loss: 1.2083\n",
      "Epoch: 7/20, Batch: 200/1388, Loss: 0.8047\n",
      "Epoch: 7/20, Batch: 250/1388, Loss: 0.9781\n",
      "Epoch: 7/20, Batch: 300/1388, Loss: 1.3306\n",
      "Epoch: 7/20, Batch: 350/1388, Loss: 0.8863\n",
      "Epoch: 7/20, Batch: 400/1388, Loss: 1.0237\n",
      "Epoch: 7/20, Batch: 450/1388, Loss: 0.7886\n",
      "Epoch: 7/20, Batch: 500/1388, Loss: 0.7681\n",
      "Epoch: 7/20, Batch: 550/1388, Loss: 1.0201\n",
      "Epoch: 7/20, Batch: 600/1388, Loss: 1.1509\n",
      "Epoch: 7/20, Batch: 650/1388, Loss: 0.7706\n",
      "Epoch: 7/20, Batch: 700/1388, Loss: 1.0456\n",
      "Epoch: 7/20, Batch: 750/1388, Loss: 0.7618\n",
      "Epoch: 7/20, Batch: 800/1388, Loss: 0.8752\n",
      "Epoch: 7/20, Batch: 850/1388, Loss: 1.1726\n",
      "Epoch: 7/20, Batch: 900/1388, Loss: 1.4227\n",
      "Epoch: 7/20, Batch: 950/1388, Loss: 0.7977\n",
      "Epoch: 7/20, Batch: 1000/1388, Loss: 1.0567\n",
      "Epoch: 7/20, Batch: 1050/1388, Loss: 0.9498\n",
      "Epoch: 7/20, Batch: 1100/1388, Loss: 1.0035\n",
      "Epoch: 7/20, Batch: 1150/1388, Loss: 1.0522\n",
      "Epoch: 7/20, Batch: 1200/1388, Loss: 1.1150\n",
      "Epoch: 7/20, Batch: 1250/1388, Loss: 0.9435\n",
      "Epoch: 7/20, Batch: 1300/1388, Loss: 0.9130\n",
      "Epoch: 7/20, Batch: 1350/1388, Loss: 1.2627\n",
      "Epoch: 7, Train Loss: 0.9694\n",
      "Epoch: 8/20, Batch: 0/1388, Loss: 0.7519\n",
      "Epoch: 8/20, Batch: 50/1388, Loss: 0.8835\n",
      "Epoch: 8/20, Batch: 100/1388, Loss: 0.7467\n",
      "Epoch: 8/20, Batch: 150/1388, Loss: 0.7602\n",
      "Epoch: 8/20, Batch: 200/1388, Loss: 1.0152\n",
      "Epoch: 8/20, Batch: 250/1388, Loss: 0.7829\n",
      "Epoch: 8/20, Batch: 300/1388, Loss: 1.2592\n",
      "Epoch: 8/20, Batch: 350/1388, Loss: 1.0469\n",
      "Epoch: 8/20, Batch: 400/1388, Loss: 0.8492\n",
      "Epoch: 8/20, Batch: 450/1388, Loss: 0.8522\n",
      "Epoch: 8/20, Batch: 500/1388, Loss: 0.8773\n",
      "Epoch: 8/20, Batch: 550/1388, Loss: 0.9622\n",
      "Epoch: 8/20, Batch: 600/1388, Loss: 0.9643\n",
      "Epoch: 8/20, Batch: 650/1388, Loss: 1.1708\n",
      "Epoch: 8/20, Batch: 700/1388, Loss: 1.0983\n",
      "Epoch: 8/20, Batch: 750/1388, Loss: 1.0268\n",
      "Epoch: 8/20, Batch: 800/1388, Loss: 0.8641\n",
      "Epoch: 8/20, Batch: 850/1388, Loss: 0.6513\n",
      "Epoch: 8/20, Batch: 900/1388, Loss: 0.7998\n",
      "Epoch: 8/20, Batch: 950/1388, Loss: 0.9404\n",
      "Epoch: 8/20, Batch: 1000/1388, Loss: 0.8754\n",
      "Epoch: 8/20, Batch: 1050/1388, Loss: 1.1561\n",
      "Epoch: 8/20, Batch: 1100/1388, Loss: 0.8296\n",
      "Epoch: 8/20, Batch: 1150/1388, Loss: 1.0542\n",
      "Epoch: 8/20, Batch: 1200/1388, Loss: 0.8743\n",
      "Epoch: 8/20, Batch: 1250/1388, Loss: 1.1410\n",
      "Epoch: 8/20, Batch: 1300/1388, Loss: 0.8590\n",
      "Epoch: 8/20, Batch: 1350/1388, Loss: 0.5622\n",
      "Epoch: 8, Train Loss: 0.9669\n",
      "Epoch: 9/20, Batch: 0/1388, Loss: 0.9593\n",
      "Epoch: 9/20, Batch: 50/1388, Loss: 1.2127\n",
      "Epoch: 9/20, Batch: 100/1388, Loss: 1.0257\n",
      "Epoch: 9/20, Batch: 150/1388, Loss: 1.0213\n",
      "Epoch: 9/20, Batch: 200/1388, Loss: 1.1985\n",
      "Epoch: 9/20, Batch: 250/1388, Loss: 1.0152\n",
      "Epoch: 9/20, Batch: 300/1388, Loss: 0.7429\n",
      "Epoch: 9/20, Batch: 350/1388, Loss: 0.9371\n",
      "Epoch: 9/20, Batch: 400/1388, Loss: 1.0087\n",
      "Epoch: 9/20, Batch: 450/1388, Loss: 1.2057\n",
      "Epoch: 9/20, Batch: 500/1388, Loss: 0.8214\n",
      "Epoch: 9/20, Batch: 550/1388, Loss: 0.7510\n",
      "Epoch: 9/20, Batch: 600/1388, Loss: 0.7843\n",
      "Epoch: 9/20, Batch: 650/1388, Loss: 0.7791\n",
      "Epoch: 9/20, Batch: 700/1388, Loss: 0.6834\n",
      "Epoch: 9/20, Batch: 750/1388, Loss: 0.9212\n",
      "Epoch: 9/20, Batch: 800/1388, Loss: 1.0024\n",
      "Epoch: 9/20, Batch: 850/1388, Loss: 1.1951\n",
      "Epoch: 9/20, Batch: 900/1388, Loss: 0.8280\n",
      "Epoch: 9/20, Batch: 950/1388, Loss: 1.2210\n",
      "Epoch: 9/20, Batch: 1000/1388, Loss: 1.1915\n",
      "Epoch: 9/20, Batch: 1050/1388, Loss: 0.7870\n",
      "Epoch: 9/20, Batch: 1100/1388, Loss: 1.0137\n",
      "Epoch: 9/20, Batch: 1150/1388, Loss: 0.8785\n",
      "Epoch: 9/20, Batch: 1200/1388, Loss: 0.9108\n",
      "Epoch: 9/20, Batch: 1250/1388, Loss: 0.8665\n",
      "Epoch: 9/20, Batch: 1300/1388, Loss: 0.9924\n",
      "Epoch: 9/20, Batch: 1350/1388, Loss: 0.9050\n",
      "Epoch: 9, Train Loss: 0.9671\n",
      "Epoch: 10/20, Batch: 0/1388, Loss: 0.7362\n",
      "Epoch: 10/20, Batch: 50/1388, Loss: 0.7605\n",
      "Epoch: 10/20, Batch: 100/1388, Loss: 0.8573\n",
      "Epoch: 10/20, Batch: 150/1388, Loss: 0.8801\n",
      "Epoch: 10/20, Batch: 200/1388, Loss: 0.8120\n",
      "Epoch: 10/20, Batch: 250/1388, Loss: 1.2326\n",
      "Epoch: 10/20, Batch: 300/1388, Loss: 1.0729\n",
      "Epoch: 10/20, Batch: 350/1388, Loss: 1.0838\n",
      "Epoch: 10/20, Batch: 400/1388, Loss: 0.9775\n",
      "Epoch: 10/20, Batch: 450/1388, Loss: 1.3551\n",
      "Epoch: 10/20, Batch: 500/1388, Loss: 0.8365\n",
      "Epoch: 10/20, Batch: 550/1388, Loss: 0.8728\n",
      "Epoch: 10/20, Batch: 600/1388, Loss: 0.9936\n",
      "Epoch: 10/20, Batch: 650/1388, Loss: 0.8225\n",
      "Epoch: 10/20, Batch: 700/1388, Loss: 0.7142\n",
      "Epoch: 10/20, Batch: 750/1388, Loss: 0.8077\n",
      "Epoch: 10/20, Batch: 800/1388, Loss: 1.1261\n",
      "Epoch: 10/20, Batch: 850/1388, Loss: 0.8607\n",
      "Epoch: 10/20, Batch: 900/1388, Loss: 1.3058\n",
      "Epoch: 10/20, Batch: 950/1388, Loss: 1.1555\n",
      "Epoch: 10/20, Batch: 1000/1388, Loss: 0.6417\n",
      "Epoch: 10/20, Batch: 1050/1388, Loss: 1.1734\n",
      "Epoch: 10/20, Batch: 1100/1388, Loss: 0.8059\n",
      "Epoch: 10/20, Batch: 1150/1388, Loss: 0.8799\n",
      "Epoch: 10/20, Batch: 1200/1388, Loss: 0.9163\n",
      "Epoch: 10/20, Batch: 1250/1388, Loss: 0.7585\n",
      "Epoch: 10/20, Batch: 1300/1388, Loss: 0.8561\n",
      "Epoch: 10/20, Batch: 1350/1388, Loss: 1.1264\n",
      "Epoch: 10, Train Loss: 0.9667\n",
      "Epoch: 11/20, Batch: 0/1388, Loss: 1.2618\n",
      "Epoch: 11/20, Batch: 50/1388, Loss: 0.9417\n",
      "Epoch: 11/20, Batch: 100/1388, Loss: 0.8794\n",
      "Epoch: 11/20, Batch: 150/1388, Loss: 0.8446\n",
      "Epoch: 11/20, Batch: 200/1388, Loss: 0.7410\n",
      "Epoch: 11/20, Batch: 250/1388, Loss: 1.0049\n",
      "Epoch: 11/20, Batch: 300/1388, Loss: 0.8974\n",
      "Epoch: 11/20, Batch: 350/1388, Loss: 1.0191\n",
      "Epoch: 11/20, Batch: 400/1388, Loss: 1.2640\n",
      "Epoch: 11/20, Batch: 450/1388, Loss: 0.8702\n",
      "Epoch: 11/20, Batch: 500/1388, Loss: 1.4504\n",
      "Epoch: 11/20, Batch: 550/1388, Loss: 0.8914\n",
      "Epoch: 11/20, Batch: 600/1388, Loss: 1.0449\n",
      "Epoch: 11/20, Batch: 650/1388, Loss: 1.0061\n",
      "Epoch: 11/20, Batch: 700/1388, Loss: 0.9185\n",
      "Epoch: 11/20, Batch: 750/1388, Loss: 1.2587\n",
      "Epoch: 11/20, Batch: 800/1388, Loss: 1.0790\n",
      "Epoch: 11/20, Batch: 850/1388, Loss: 0.6873\n",
      "Epoch: 11/20, Batch: 900/1388, Loss: 0.9696\n",
      "Epoch: 11/20, Batch: 950/1388, Loss: 0.7831\n",
      "Epoch: 11/20, Batch: 1000/1388, Loss: 0.9784\n",
      "Epoch: 11/20, Batch: 1050/1388, Loss: 0.9567\n",
      "Epoch: 11/20, Batch: 1100/1388, Loss: 1.1455\n",
      "Epoch: 11/20, Batch: 1150/1388, Loss: 0.9524\n",
      "Epoch: 11/20, Batch: 1200/1388, Loss: 0.8807\n",
      "Epoch: 11/20, Batch: 1250/1388, Loss: 0.7005\n",
      "Epoch: 11/20, Batch: 1300/1388, Loss: 0.7272\n",
      "Epoch: 11/20, Batch: 1350/1388, Loss: 0.9356\n",
      "Epoch: 11, Train Loss: 0.9674\n",
      "Epoch: 12/20, Batch: 0/1388, Loss: 0.8258\n",
      "Epoch: 12/20, Batch: 50/1388, Loss: 0.8879\n",
      "Epoch: 12/20, Batch: 100/1388, Loss: 0.8101\n",
      "Epoch: 12/20, Batch: 150/1388, Loss: 0.9481\n",
      "Epoch: 12/20, Batch: 200/1388, Loss: 0.9395\n",
      "Epoch: 12/20, Batch: 250/1388, Loss: 0.9768\n",
      "Epoch: 12/20, Batch: 300/1388, Loss: 0.8811\n",
      "Epoch: 12/20, Batch: 350/1388, Loss: 1.0153\n",
      "Epoch: 12/20, Batch: 400/1388, Loss: 0.9801\n",
      "Epoch: 12/20, Batch: 450/1388, Loss: 0.7463\n",
      "Epoch: 12/20, Batch: 500/1388, Loss: 0.7920\n",
      "Epoch: 12/20, Batch: 550/1388, Loss: 0.8506\n",
      "Epoch: 12/20, Batch: 600/1388, Loss: 0.8492\n",
      "Epoch: 12/20, Batch: 650/1388, Loss: 0.8705\n",
      "Epoch: 12/20, Batch: 700/1388, Loss: 1.0020\n",
      "Epoch: 12/20, Batch: 750/1388, Loss: 1.2482\n",
      "Epoch: 12/20, Batch: 800/1388, Loss: 0.9289\n",
      "Epoch: 12/20, Batch: 850/1388, Loss: 0.7113\n",
      "Epoch: 12/20, Batch: 900/1388, Loss: 0.9850\n",
      "Epoch: 12/20, Batch: 950/1388, Loss: 0.9020\n",
      "Epoch: 12/20, Batch: 1000/1388, Loss: 0.9341\n",
      "Epoch: 12/20, Batch: 1050/1388, Loss: 0.8843\n",
      "Epoch: 12/20, Batch: 1100/1388, Loss: 0.9859\n",
      "Epoch: 12/20, Batch: 1150/1388, Loss: 0.8322\n",
      "Epoch: 12/20, Batch: 1200/1388, Loss: 0.8810\n",
      "Epoch: 12/20, Batch: 1250/1388, Loss: 0.8063\n",
      "Epoch: 12/20, Batch: 1300/1388, Loss: 0.8620\n",
      "Epoch: 12/20, Batch: 1350/1388, Loss: 1.0876\n",
      "Epoch: 12, Train Loss: 0.9654\n",
      "Epoch: 13/20, Batch: 0/1388, Loss: 0.9007\n",
      "Epoch: 13/20, Batch: 50/1388, Loss: 0.8767\n",
      "Epoch: 13/20, Batch: 100/1388, Loss: 0.9647\n",
      "Epoch: 13/20, Batch: 150/1388, Loss: 0.9774\n",
      "Epoch: 13/20, Batch: 200/1388, Loss: 0.7637\n",
      "Epoch: 13/20, Batch: 250/1388, Loss: 1.2189\n",
      "Epoch: 13/20, Batch: 300/1388, Loss: 0.9154\n",
      "Epoch: 13/20, Batch: 350/1388, Loss: 0.8501\n",
      "Epoch: 13/20, Batch: 400/1388, Loss: 0.8505\n",
      "Epoch: 13/20, Batch: 450/1388, Loss: 0.9038\n",
      "Epoch: 13/20, Batch: 500/1388, Loss: 0.7951\n",
      "Epoch: 13/20, Batch: 550/1388, Loss: 0.8104\n",
      "Epoch: 13/20, Batch: 600/1388, Loss: 0.6989\n",
      "Epoch: 13/20, Batch: 650/1388, Loss: 1.0655\n",
      "Epoch: 13/20, Batch: 700/1388, Loss: 0.8761\n",
      "Epoch: 13/20, Batch: 750/1388, Loss: 0.8178\n",
      "Epoch: 13/20, Batch: 800/1388, Loss: 1.3653\n",
      "Epoch: 13/20, Batch: 850/1388, Loss: 1.1107\n",
      "Epoch: 13/20, Batch: 900/1388, Loss: 1.2365\n",
      "Epoch: 13/20, Batch: 950/1388, Loss: 0.7209\n",
      "Epoch: 13/20, Batch: 1000/1388, Loss: 1.2657\n",
      "Epoch: 13/20, Batch: 1050/1388, Loss: 1.2199\n",
      "Epoch: 13/20, Batch: 1100/1388, Loss: 0.8503\n",
      "Epoch: 13/20, Batch: 1150/1388, Loss: 1.1819\n",
      "Epoch: 13/20, Batch: 1200/1388, Loss: 1.0824\n",
      "Epoch: 13/20, Batch: 1250/1388, Loss: 0.7432\n",
      "Epoch: 13/20, Batch: 1300/1388, Loss: 1.0211\n",
      "Epoch: 13/20, Batch: 1350/1388, Loss: 1.1687\n",
      "Epoch: 13, Train Loss: 0.9655\n",
      "Epoch: 14/20, Batch: 0/1388, Loss: 1.0227\n",
      "Epoch: 14/20, Batch: 50/1388, Loss: 1.0590\n",
      "Epoch: 14/20, Batch: 100/1388, Loss: 1.0452\n",
      "Epoch: 14/20, Batch: 150/1388, Loss: 1.1061\n",
      "Epoch: 14/20, Batch: 200/1388, Loss: 0.9276\n",
      "Epoch: 14/20, Batch: 250/1388, Loss: 0.9848\n",
      "Epoch: 14/20, Batch: 300/1388, Loss: 0.8509\n",
      "Epoch: 14/20, Batch: 350/1388, Loss: 1.3516\n",
      "Epoch: 14/20, Batch: 400/1388, Loss: 1.0947\n",
      "Epoch: 14/20, Batch: 450/1388, Loss: 1.0488\n",
      "Epoch: 14/20, Batch: 500/1388, Loss: 0.9185\n",
      "Epoch: 14/20, Batch: 550/1388, Loss: 1.1519\n",
      "Epoch: 14/20, Batch: 600/1388, Loss: 1.1113\n",
      "Epoch: 14/20, Batch: 650/1388, Loss: 0.8221\n",
      "Epoch: 14/20, Batch: 700/1388, Loss: 0.9630\n",
      "Epoch: 14/20, Batch: 750/1388, Loss: 1.2482\n",
      "Epoch: 14/20, Batch: 800/1388, Loss: 0.9457\n",
      "Epoch: 14/20, Batch: 850/1388, Loss: 0.8460\n",
      "Epoch: 14/20, Batch: 900/1388, Loss: 0.9015\n",
      "Epoch: 14/20, Batch: 950/1388, Loss: 1.0767\n",
      "Epoch: 14/20, Batch: 1000/1388, Loss: 0.9854\n",
      "Epoch: 14/20, Batch: 1050/1388, Loss: 0.9199\n",
      "Epoch: 14/20, Batch: 1100/1388, Loss: 0.9122\n",
      "Epoch: 14/20, Batch: 1150/1388, Loss: 0.9385\n",
      "Epoch: 14/20, Batch: 1200/1388, Loss: 1.0350\n",
      "Epoch: 14/20, Batch: 1250/1388, Loss: 1.0758\n",
      "Epoch: 14/20, Batch: 1300/1388, Loss: 1.0034\n",
      "Epoch: 14/20, Batch: 1350/1388, Loss: 0.8131\n",
      "Epoch: 14, Train Loss: 0.9662\n",
      "Epoch: 15/20, Batch: 0/1388, Loss: 1.0126\n",
      "Epoch: 15/20, Batch: 50/1388, Loss: 1.1876\n",
      "Epoch: 15/20, Batch: 100/1388, Loss: 0.8296\n",
      "Epoch: 15/20, Batch: 150/1388, Loss: 0.8592\n",
      "Epoch: 15/20, Batch: 200/1388, Loss: 0.6691\n",
      "Epoch: 15/20, Batch: 250/1388, Loss: 1.0614\n",
      "Epoch: 15/20, Batch: 300/1388, Loss: 0.7752\n",
      "Epoch: 15/20, Batch: 350/1388, Loss: 0.8924\n",
      "Epoch: 15/20, Batch: 400/1388, Loss: 1.0865\n",
      "Epoch: 15/20, Batch: 450/1388, Loss: 0.8371\n",
      "Epoch: 15/20, Batch: 500/1388, Loss: 0.8859\n",
      "Epoch: 15/20, Batch: 550/1388, Loss: 0.8325\n",
      "Epoch: 15/20, Batch: 600/1388, Loss: 0.9452\n",
      "Epoch: 15/20, Batch: 650/1388, Loss: 1.0912\n",
      "Epoch: 15/20, Batch: 700/1388, Loss: 0.8631\n",
      "Epoch: 15/20, Batch: 750/1388, Loss: 0.7777\n",
      "Epoch: 15/20, Batch: 800/1388, Loss: 1.3336\n",
      "Epoch: 15/20, Batch: 850/1388, Loss: 0.7976\n",
      "Epoch: 15/20, Batch: 900/1388, Loss: 1.1082\n",
      "Epoch: 15/20, Batch: 950/1388, Loss: 0.8537\n",
      "Epoch: 15/20, Batch: 1000/1388, Loss: 1.0459\n",
      "Epoch: 15/20, Batch: 1050/1388, Loss: 1.0074\n",
      "Epoch: 15/20, Batch: 1100/1388, Loss: 1.0212\n",
      "Epoch: 15/20, Batch: 1150/1388, Loss: 0.8166\n",
      "Epoch: 15/20, Batch: 1200/1388, Loss: 0.7804\n",
      "Epoch: 15/20, Batch: 1250/1388, Loss: 0.8975\n",
      "Epoch: 15/20, Batch: 1300/1388, Loss: 0.8480\n",
      "Epoch: 15/20, Batch: 1350/1388, Loss: 1.1053\n",
      "Epoch: 15, Train Loss: 0.9643\n",
      "Epoch: 16/20, Batch: 0/1388, Loss: 1.1167\n",
      "Epoch: 16/20, Batch: 50/1388, Loss: 1.1252\n",
      "Epoch: 16/20, Batch: 100/1388, Loss: 1.3181\n",
      "Epoch: 16/20, Batch: 150/1388, Loss: 0.9553\n",
      "Epoch: 16/20, Batch: 200/1388, Loss: 1.2574\n",
      "Epoch: 16/20, Batch: 250/1388, Loss: 0.9750\n",
      "Epoch: 16/20, Batch: 300/1388, Loss: 0.6781\n",
      "Epoch: 16/20, Batch: 350/1388, Loss: 0.8028\n",
      "Epoch: 16/20, Batch: 400/1388, Loss: 0.9363\n",
      "Epoch: 16/20, Batch: 450/1388, Loss: 1.5285\n",
      "Epoch: 16/20, Batch: 500/1388, Loss: 1.0449\n",
      "Epoch: 16/20, Batch: 550/1388, Loss: 0.8544\n",
      "Epoch: 16/20, Batch: 600/1388, Loss: 0.9552\n",
      "Epoch: 16/20, Batch: 650/1388, Loss: 0.7252\n",
      "Epoch: 16/20, Batch: 700/1388, Loss: 1.0860\n",
      "Epoch: 16/20, Batch: 750/1388, Loss: 1.1029\n",
      "Epoch: 16/20, Batch: 800/1388, Loss: 0.8377\n",
      "Epoch: 16/20, Batch: 850/1388, Loss: 0.7969\n",
      "Epoch: 16/20, Batch: 900/1388, Loss: 1.3838\n",
      "Epoch: 16/20, Batch: 950/1388, Loss: 0.8872\n",
      "Epoch: 16/20, Batch: 1000/1388, Loss: 1.4612\n",
      "Epoch: 16/20, Batch: 1050/1388, Loss: 1.1106\n",
      "Epoch: 16/20, Batch: 1100/1388, Loss: 0.6417\n",
      "Epoch: 16/20, Batch: 1150/1388, Loss: 0.7782\n",
      "Epoch: 16/20, Batch: 1200/1388, Loss: 0.7840\n",
      "Epoch: 16/20, Batch: 1250/1388, Loss: 0.7914\n",
      "Epoch: 16/20, Batch: 1300/1388, Loss: 1.1266\n",
      "Epoch: 16/20, Batch: 1350/1388, Loss: 0.9721\n",
      "Epoch: 16, Train Loss: 0.9657\n",
      "Epoch: 17/20, Batch: 0/1388, Loss: 0.9072\n",
      "Epoch: 17/20, Batch: 50/1388, Loss: 1.0344\n",
      "Epoch: 17/20, Batch: 100/1388, Loss: 1.1954\n",
      "Epoch: 17/20, Batch: 150/1388, Loss: 1.0307\n",
      "Epoch: 17/20, Batch: 200/1388, Loss: 1.1771\n",
      "Epoch: 17/20, Batch: 250/1388, Loss: 1.0669\n",
      "Epoch: 17/20, Batch: 300/1388, Loss: 1.3560\n",
      "Epoch: 17/20, Batch: 350/1388, Loss: 1.0837\n",
      "Epoch: 17/20, Batch: 400/1388, Loss: 0.7894\n",
      "Epoch: 17/20, Batch: 450/1388, Loss: 0.7115\n",
      "Epoch: 17/20, Batch: 500/1388, Loss: 0.7502\n",
      "Epoch: 17/20, Batch: 550/1388, Loss: 1.0369\n",
      "Epoch: 17/20, Batch: 600/1388, Loss: 0.9012\n",
      "Epoch: 17/20, Batch: 650/1388, Loss: 0.7497\n",
      "Epoch: 17/20, Batch: 700/1388, Loss: 1.2981\n",
      "Epoch: 17/20, Batch: 750/1388, Loss: 0.7524\n",
      "Epoch: 17/20, Batch: 800/1388, Loss: 0.9690\n",
      "Epoch: 17/20, Batch: 850/1388, Loss: 0.9438\n",
      "Epoch: 17/20, Batch: 900/1388, Loss: 0.8641\n",
      "Epoch: 17/20, Batch: 950/1388, Loss: 1.0361\n",
      "Epoch: 17/20, Batch: 1000/1388, Loss: 0.8153\n",
      "Epoch: 17/20, Batch: 1050/1388, Loss: 0.9754\n",
      "Epoch: 17/20, Batch: 1100/1388, Loss: 0.9615\n",
      "Epoch: 17/20, Batch: 1150/1388, Loss: 0.8883\n",
      "Epoch: 17/20, Batch: 1200/1388, Loss: 0.8571\n",
      "Epoch: 17/20, Batch: 1250/1388, Loss: 1.0149\n",
      "Epoch: 17/20, Batch: 1300/1388, Loss: 0.8941\n",
      "Epoch: 17/20, Batch: 1350/1388, Loss: 0.9551\n",
      "Epoch: 17, Train Loss: 0.9628\n",
      "Epoch: 18/20, Batch: 0/1388, Loss: 1.1109\n",
      "Epoch: 18/20, Batch: 50/1388, Loss: 0.8877\n",
      "Epoch: 18/20, Batch: 100/1388, Loss: 0.7590\n",
      "Epoch: 18/20, Batch: 150/1388, Loss: 0.9420\n",
      "Epoch: 18/20, Batch: 200/1388, Loss: 1.0726\n",
      "Epoch: 18/20, Batch: 250/1388, Loss: 1.2038\n",
      "Epoch: 18/20, Batch: 300/1388, Loss: 0.7503\n",
      "Epoch: 18/20, Batch: 350/1388, Loss: 1.0689\n",
      "Epoch: 18/20, Batch: 400/1388, Loss: 1.0441\n",
      "Epoch: 18/20, Batch: 450/1388, Loss: 0.9194\n",
      "Epoch: 18/20, Batch: 500/1388, Loss: 1.1304\n",
      "Epoch: 18/20, Batch: 550/1388, Loss: 0.9351\n",
      "Epoch: 18/20, Batch: 600/1388, Loss: 1.2719\n",
      "Epoch: 18/20, Batch: 650/1388, Loss: 0.8855\n",
      "Epoch: 18/20, Batch: 700/1388, Loss: 1.1359\n",
      "Epoch: 18/20, Batch: 750/1388, Loss: 0.7562\n",
      "Epoch: 18/20, Batch: 800/1388, Loss: 0.9284\n",
      "Epoch: 18/20, Batch: 850/1388, Loss: 1.2685\n",
      "Epoch: 18/20, Batch: 900/1388, Loss: 0.6766\n",
      "Epoch: 18/20, Batch: 950/1388, Loss: 0.9667\n",
      "Epoch: 18/20, Batch: 1000/1388, Loss: 0.9147\n",
      "Epoch: 18/20, Batch: 1050/1388, Loss: 0.7767\n",
      "Epoch: 18/20, Batch: 1100/1388, Loss: 0.8350\n",
      "Epoch: 18/20, Batch: 1150/1388, Loss: 1.1435\n",
      "Epoch: 18/20, Batch: 1200/1388, Loss: 0.7131\n",
      "Epoch: 18/20, Batch: 1250/1388, Loss: 1.0443\n",
      "Epoch: 18/20, Batch: 1300/1388, Loss: 0.7572\n",
      "Epoch: 18/20, Batch: 1350/1388, Loss: 0.9112\n",
      "Epoch: 18, Train Loss: 0.9629\n",
      "Epoch: 19/20, Batch: 0/1388, Loss: 0.8378\n",
      "Epoch: 19/20, Batch: 50/1388, Loss: 0.9322\n",
      "Epoch: 19/20, Batch: 100/1388, Loss: 0.9603\n",
      "Epoch: 19/20, Batch: 150/1388, Loss: 0.7783\n",
      "Epoch: 19/20, Batch: 200/1388, Loss: 0.8819\n",
      "Epoch: 19/20, Batch: 250/1388, Loss: 0.8880\n",
      "Epoch: 19/20, Batch: 300/1388, Loss: 0.9980\n",
      "Epoch: 19/20, Batch: 350/1388, Loss: 1.0432\n",
      "Epoch: 19/20, Batch: 400/1388, Loss: 0.7255\n",
      "Epoch: 19/20, Batch: 450/1388, Loss: 1.1440\n",
      "Epoch: 19/20, Batch: 500/1388, Loss: 2.0311\n",
      "Epoch: 19/20, Batch: 550/1388, Loss: 0.9674\n",
      "Epoch: 19/20, Batch: 600/1388, Loss: 1.3334\n",
      "Epoch: 19/20, Batch: 650/1388, Loss: 0.8485\n",
      "Epoch: 19/20, Batch: 700/1388, Loss: 1.1943\n",
      "Epoch: 19/20, Batch: 750/1388, Loss: 0.7128\n",
      "Epoch: 19/20, Batch: 800/1388, Loss: 1.2524\n",
      "Epoch: 19/20, Batch: 850/1388, Loss: 0.7416\n",
      "Epoch: 19/20, Batch: 900/1388, Loss: 0.9997\n",
      "Epoch: 19/20, Batch: 950/1388, Loss: 0.9426\n",
      "Epoch: 19/20, Batch: 1000/1388, Loss: 1.0047\n",
      "Epoch: 19/20, Batch: 1050/1388, Loss: 0.9028\n",
      "Epoch: 19/20, Batch: 1100/1388, Loss: 0.7843\n",
      "Epoch: 19/20, Batch: 1150/1388, Loss: 0.8589\n",
      "Epoch: 19/20, Batch: 1200/1388, Loss: 0.7941\n",
      "Epoch: 19/20, Batch: 1250/1388, Loss: 0.8604\n",
      "Epoch: 19/20, Batch: 1300/1388, Loss: 1.0139\n",
      "Epoch: 19/20, Batch: 1350/1388, Loss: 0.8236\n",
      "Epoch: 19, Train Loss: 0.9605\n",
      "Epoch: 20/20, Batch: 0/1388, Loss: 1.0291\n",
      "Epoch: 20/20, Batch: 50/1388, Loss: 1.0556\n",
      "Epoch: 20/20, Batch: 100/1388, Loss: 0.7376\n",
      "Epoch: 20/20, Batch: 150/1388, Loss: 1.1464\n",
      "Epoch: 20/20, Batch: 200/1388, Loss: 0.9132\n",
      "Epoch: 20/20, Batch: 250/1388, Loss: 1.5342\n",
      "Epoch: 20/20, Batch: 300/1388, Loss: 1.0193\n",
      "Epoch: 20/20, Batch: 350/1388, Loss: 1.0123\n",
      "Epoch: 20/20, Batch: 400/1388, Loss: 1.0662\n",
      "Epoch: 20/20, Batch: 450/1388, Loss: 0.9678\n",
      "Epoch: 20/20, Batch: 500/1388, Loss: 0.9683\n",
      "Epoch: 20/20, Batch: 550/1388, Loss: 0.8032\n",
      "Epoch: 20/20, Batch: 600/1388, Loss: 0.9144\n",
      "Epoch: 20/20, Batch: 650/1388, Loss: 1.0549\n",
      "Epoch: 20/20, Batch: 700/1388, Loss: 0.9054\n",
      "Epoch: 20/20, Batch: 750/1388, Loss: 0.9637\n",
      "Epoch: 20/20, Batch: 800/1388, Loss: 0.9830\n",
      "Epoch: 20/20, Batch: 850/1388, Loss: 0.8842\n",
      "Epoch: 20/20, Batch: 900/1388, Loss: 1.0130\n",
      "Epoch: 20/20, Batch: 950/1388, Loss: 0.8018\n",
      "Epoch: 20/20, Batch: 1000/1388, Loss: 0.7726\n",
      "Epoch: 20/20, Batch: 1050/1388, Loss: 0.7654\n",
      "Epoch: 20/20, Batch: 1100/1388, Loss: 0.9088\n",
      "Epoch: 20/20, Batch: 1150/1388, Loss: 0.9051\n",
      "Epoch: 20/20, Batch: 1200/1388, Loss: 1.3110\n",
      "Epoch: 20/20, Batch: 1250/1388, Loss: 1.2349\n",
      "Epoch: 20/20, Batch: 1300/1388, Loss: 0.8263\n",
      "Epoch: 20/20, Batch: 1350/1388, Loss: 0.8197\n",
      "Epoch: 20, Train Loss: 0.9601\n",
      "Training finished!\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "weighted_train_losses = []\n",
    "\n",
    "print(\"\\nStarting training...\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    weighted_model_2.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = weighted_model_2(data)\n",
    "    \n",
    "        target = target.float()\n",
    "        output = output.float()    \n",
    "        loss = weighted_model_2.get_weighted_loss(output, target)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if batch_idx % 50 == 0:\n",
    "            print(f'Epoch: {epoch+1}/{num_epochs}, Batch: {batch_idx}/{len(train_loader)}, Loss: {loss.item():.4f}')\n",
    "    \n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "    weighted_train_losses.append(avg_train_loss)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1}, Train Loss: {avg_train_loss:.4f}')\n",
    "\n",
    "print('Training finished!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report (Weighted 2):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.71      0.82     68886\n",
      "         1.0       0.23      0.83      0.36      7218\n",
      "\n",
      "    accuracy                           0.72     76104\n",
      "   macro avg       0.60      0.77      0.59     76104\n",
      "weighted avg       0.91      0.72      0.78     76104\n",
      "\n",
      "\n",
      "Confusion Matrix (Weighted 2):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 23.52222222222222, 'Predicted')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "weighted_model_2.eval()\n",
    "\n",
    "y_true_weighted_2 = []\n",
    "y_pred_weighted_2 = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        output = weighted_model_2(data)\n",
    "        \n",
    "        y_true_weighted_2.extend(target.cpu().numpy())\n",
    "        y_pred_weighted_2.extend((output > 0.5).cpu().numpy())\n",
    "        \n",
    "y_true_weighted_2 = np.array(y_true_weighted_2)\n",
    "y_pred_weighted_2 = np.array(y_pred_weighted_2)\n",
    "\n",
    "print(\"\\nClassification Report (Weighted 2):\")\n",
    "print(classification_report(y_true_weighted_2, y_pred_weighted_2))\n",
    "print(\"\\nConfusion Matrix (Weighted 2):\")\n",
    "cm_weighted_2 = confusion_matrix(y_true_weighted_2, y_pred_weighted_2)\n",
    "sns.heatmap(cm_weighted_2, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "plt.xlabel('Predicted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train:test = 6:4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training...\n",
      "Epoch: 1/20, Batch: 0/793, Loss: 1.6825\n",
      "Epoch: 1/20, Batch: 50/793, Loss: 0.9463\n",
      "Epoch: 1/20, Batch: 100/793, Loss: 1.1734\n",
      "Epoch: 1/20, Batch: 150/793, Loss: 0.8941\n",
      "Epoch: 1/20, Batch: 200/793, Loss: 1.1621\n",
      "Epoch: 1/20, Batch: 250/793, Loss: 1.1116\n",
      "Epoch: 1/20, Batch: 300/793, Loss: 1.0699\n",
      "Epoch: 1/20, Batch: 350/793, Loss: 1.0761\n",
      "Epoch: 1/20, Batch: 400/793, Loss: 1.0363\n",
      "Epoch: 1/20, Batch: 450/793, Loss: 0.8465\n",
      "Epoch: 1/20, Batch: 500/793, Loss: 0.8409\n",
      "Epoch: 1/20, Batch: 550/793, Loss: 0.7981\n",
      "Epoch: 1/20, Batch: 600/793, Loss: 1.1028\n",
      "Epoch: 1/20, Batch: 650/793, Loss: 1.2010\n",
      "Epoch: 1/20, Batch: 700/793, Loss: 0.9140\n",
      "Epoch: 1/20, Batch: 750/793, Loss: 1.1190\n",
      "Epoch: 1, Train Loss: 1.0429\n",
      "Epoch: 2/20, Batch: 0/793, Loss: 1.0307\n",
      "Epoch: 2/20, Batch: 50/793, Loss: 1.1839\n",
      "Epoch: 2/20, Batch: 100/793, Loss: 0.9581\n",
      "Epoch: 2/20, Batch: 150/793, Loss: 0.7813\n",
      "Epoch: 2/20, Batch: 200/793, Loss: 0.8764\n",
      "Epoch: 2/20, Batch: 250/793, Loss: 0.7880\n",
      "Epoch: 2/20, Batch: 300/793, Loss: 0.9908\n",
      "Epoch: 2/20, Batch: 350/793, Loss: 1.3911\n",
      "Epoch: 2/20, Batch: 400/793, Loss: 0.7891\n",
      "Epoch: 2/20, Batch: 450/793, Loss: 0.8294\n",
      "Epoch: 2/20, Batch: 500/793, Loss: 0.9507\n",
      "Epoch: 2/20, Batch: 550/793, Loss: 0.8747\n",
      "Epoch: 2/20, Batch: 600/793, Loss: 0.9692\n",
      "Epoch: 2/20, Batch: 650/793, Loss: 1.0207\n",
      "Epoch: 2/20, Batch: 700/793, Loss: 0.7876\n",
      "Epoch: 2/20, Batch: 750/793, Loss: 0.9556\n",
      "Epoch: 2, Train Loss: 0.9879\n",
      "Epoch: 3/20, Batch: 0/793, Loss: 1.2021\n",
      "Epoch: 3/20, Batch: 50/793, Loss: 0.9739\n",
      "Epoch: 3/20, Batch: 100/793, Loss: 0.9305\n",
      "Epoch: 3/20, Batch: 150/793, Loss: 1.0231\n",
      "Epoch: 3/20, Batch: 200/793, Loss: 1.4025\n",
      "Epoch: 3/20, Batch: 250/793, Loss: 0.9577\n",
      "Epoch: 3/20, Batch: 300/793, Loss: 1.3134\n",
      "Epoch: 3/20, Batch: 350/793, Loss: 0.9453\n",
      "Epoch: 3/20, Batch: 400/793, Loss: 1.5232\n",
      "Epoch: 3/20, Batch: 450/793, Loss: 1.0393\n",
      "Epoch: 3/20, Batch: 500/793, Loss: 0.9957\n",
      "Epoch: 3/20, Batch: 550/793, Loss: 1.0851\n",
      "Epoch: 3/20, Batch: 600/793, Loss: 0.8017\n",
      "Epoch: 3/20, Batch: 650/793, Loss: 0.8139\n",
      "Epoch: 3/20, Batch: 700/793, Loss: 1.3699\n",
      "Epoch: 3/20, Batch: 750/793, Loss: 1.1444\n",
      "Epoch: 3, Train Loss: 0.9766\n",
      "Epoch: 4/20, Batch: 0/793, Loss: 1.0798\n",
      "Epoch: 4/20, Batch: 50/793, Loss: 0.8384\n",
      "Epoch: 4/20, Batch: 100/793, Loss: 0.8082\n",
      "Epoch: 4/20, Batch: 150/793, Loss: 0.9015\n",
      "Epoch: 4/20, Batch: 200/793, Loss: 0.9059\n",
      "Epoch: 4/20, Batch: 250/793, Loss: 1.0033\n",
      "Epoch: 4/20, Batch: 300/793, Loss: 1.2095\n",
      "Epoch: 4/20, Batch: 350/793, Loss: 0.8577\n",
      "Epoch: 4/20, Batch: 400/793, Loss: 0.7614\n",
      "Epoch: 4/20, Batch: 450/793, Loss: 0.7675\n",
      "Epoch: 4/20, Batch: 500/793, Loss: 0.9965\n",
      "Epoch: 4/20, Batch: 550/793, Loss: 0.9933\n",
      "Epoch: 4/20, Batch: 600/793, Loss: 0.9672\n",
      "Epoch: 4/20, Batch: 650/793, Loss: 0.7883\n",
      "Epoch: 4/20, Batch: 700/793, Loss: 0.8416\n",
      "Epoch: 4/20, Batch: 750/793, Loss: 0.8493\n",
      "Epoch: 4, Train Loss: 0.9732\n",
      "Epoch: 5/20, Batch: 0/793, Loss: 1.0600\n",
      "Epoch: 5/20, Batch: 50/793, Loss: 0.9419\n",
      "Epoch: 5/20, Batch: 100/793, Loss: 1.0023\n",
      "Epoch: 5/20, Batch: 150/793, Loss: 0.7990\n",
      "Epoch: 5/20, Batch: 200/793, Loss: 0.9781\n",
      "Epoch: 5/20, Batch: 250/793, Loss: 0.6432\n",
      "Epoch: 5/20, Batch: 300/793, Loss: 1.1819\n",
      "Epoch: 5/20, Batch: 350/793, Loss: 0.6325\n",
      "Epoch: 5/20, Batch: 400/793, Loss: 0.8275\n",
      "Epoch: 5/20, Batch: 450/793, Loss: 0.9841\n",
      "Epoch: 5/20, Batch: 500/793, Loss: 0.7814\n",
      "Epoch: 5/20, Batch: 550/793, Loss: 0.6074\n",
      "Epoch: 5/20, Batch: 600/793, Loss: 1.0264\n",
      "Epoch: 5/20, Batch: 650/793, Loss: 0.8578\n",
      "Epoch: 5/20, Batch: 700/793, Loss: 0.8815\n",
      "Epoch: 5/20, Batch: 750/793, Loss: 0.7282\n",
      "Epoch: 5, Train Loss: 0.9699\n",
      "Epoch: 6/20, Batch: 0/793, Loss: 1.3772\n",
      "Epoch: 6/20, Batch: 50/793, Loss: 1.2306\n",
      "Epoch: 6/20, Batch: 100/793, Loss: 0.7267\n",
      "Epoch: 6/20, Batch: 150/793, Loss: 0.8592\n",
      "Epoch: 6/20, Batch: 200/793, Loss: 0.9754\n",
      "Epoch: 6/20, Batch: 250/793, Loss: 0.9808\n",
      "Epoch: 6/20, Batch: 300/793, Loss: 0.9043\n",
      "Epoch: 6/20, Batch: 350/793, Loss: 0.8147\n",
      "Epoch: 6/20, Batch: 400/793, Loss: 1.1018\n",
      "Epoch: 6/20, Batch: 450/793, Loss: 0.8376\n",
      "Epoch: 6/20, Batch: 500/793, Loss: 1.2671\n",
      "Epoch: 6/20, Batch: 550/793, Loss: 0.8462\n",
      "Epoch: 6/20, Batch: 600/793, Loss: 0.6929\n",
      "Epoch: 6/20, Batch: 650/793, Loss: 0.8252\n",
      "Epoch: 6/20, Batch: 700/793, Loss: 1.0176\n",
      "Epoch: 6/20, Batch: 750/793, Loss: 0.9292\n",
      "Epoch: 6, Train Loss: 0.9696\n",
      "Epoch: 7/20, Batch: 0/793, Loss: 0.9632\n",
      "Epoch: 7/20, Batch: 50/793, Loss: 0.9166\n",
      "Epoch: 7/20, Batch: 100/793, Loss: 0.8022\n",
      "Epoch: 7/20, Batch: 150/793, Loss: 0.8475\n",
      "Epoch: 7/20, Batch: 200/793, Loss: 1.0751\n",
      "Epoch: 7/20, Batch: 250/793, Loss: 1.4121\n",
      "Epoch: 7/20, Batch: 300/793, Loss: 0.9654\n",
      "Epoch: 7/20, Batch: 350/793, Loss: 0.7122\n",
      "Epoch: 7/20, Batch: 400/793, Loss: 0.8729\n",
      "Epoch: 7/20, Batch: 450/793, Loss: 0.7530\n",
      "Epoch: 7/20, Batch: 500/793, Loss: 0.8553\n",
      "Epoch: 7/20, Batch: 550/793, Loss: 0.8791\n",
      "Epoch: 7/20, Batch: 600/793, Loss: 0.9849\n",
      "Epoch: 7/20, Batch: 650/793, Loss: 0.9551\n",
      "Epoch: 7/20, Batch: 700/793, Loss: 0.8262\n",
      "Epoch: 7/20, Batch: 750/793, Loss: 0.9864\n",
      "Epoch: 7, Train Loss: 0.9684\n",
      "Epoch: 8/20, Batch: 0/793, Loss: 1.1352\n",
      "Epoch: 8/20, Batch: 50/793, Loss: 0.9768\n",
      "Epoch: 8/20, Batch: 100/793, Loss: 0.8933\n",
      "Epoch: 8/20, Batch: 150/793, Loss: 0.7411\n",
      "Epoch: 8/20, Batch: 200/793, Loss: 1.2755\n",
      "Epoch: 8/20, Batch: 250/793, Loss: 1.0408\n",
      "Epoch: 8/20, Batch: 300/793, Loss: 1.1685\n",
      "Epoch: 8/20, Batch: 350/793, Loss: 1.0386\n",
      "Epoch: 8/20, Batch: 400/793, Loss: 0.7961\n",
      "Epoch: 8/20, Batch: 450/793, Loss: 0.8334\n",
      "Epoch: 8/20, Batch: 500/793, Loss: 0.8328\n",
      "Epoch: 8/20, Batch: 550/793, Loss: 0.9206\n",
      "Epoch: 8/20, Batch: 600/793, Loss: 0.9837\n",
      "Epoch: 8/20, Batch: 650/793, Loss: 0.8494\n",
      "Epoch: 8/20, Batch: 700/793, Loss: 0.9985\n",
      "Epoch: 8/20, Batch: 750/793, Loss: 0.7612\n",
      "Epoch: 8, Train Loss: 0.9678\n",
      "Epoch: 9/20, Batch: 0/793, Loss: 1.1758\n",
      "Epoch: 9/20, Batch: 50/793, Loss: 1.1801\n",
      "Epoch: 9/20, Batch: 100/793, Loss: 1.2419\n",
      "Epoch: 9/20, Batch: 150/793, Loss: 0.9810\n",
      "Epoch: 9/20, Batch: 200/793, Loss: 0.9850\n",
      "Epoch: 9/20, Batch: 250/793, Loss: 1.0509\n",
      "Epoch: 9/20, Batch: 300/793, Loss: 0.7443\n",
      "Epoch: 9/20, Batch: 350/793, Loss: 1.0749\n",
      "Epoch: 9/20, Batch: 400/793, Loss: 0.9919\n",
      "Epoch: 9/20, Batch: 450/793, Loss: 0.8232\n",
      "Epoch: 9/20, Batch: 500/793, Loss: 1.4928\n",
      "Epoch: 9/20, Batch: 550/793, Loss: 0.9754\n",
      "Epoch: 9/20, Batch: 600/793, Loss: 1.1564\n",
      "Epoch: 9/20, Batch: 650/793, Loss: 0.6847\n",
      "Epoch: 9/20, Batch: 700/793, Loss: 0.9414\n",
      "Epoch: 9/20, Batch: 750/793, Loss: 1.0958\n",
      "Epoch: 9, Train Loss: 0.9652\n",
      "Epoch: 10/20, Batch: 0/793, Loss: 0.7683\n",
      "Epoch: 10/20, Batch: 50/793, Loss: 1.0184\n",
      "Epoch: 10/20, Batch: 100/793, Loss: 0.7352\n",
      "Epoch: 10/20, Batch: 150/793, Loss: 0.8451\n",
      "Epoch: 10/20, Batch: 200/793, Loss: 1.2740\n",
      "Epoch: 10/20, Batch: 250/793, Loss: 0.9135\n",
      "Epoch: 10/20, Batch: 300/793, Loss: 0.7179\n",
      "Epoch: 10/20, Batch: 350/793, Loss: 1.0497\n",
      "Epoch: 10/20, Batch: 400/793, Loss: 0.9899\n",
      "Epoch: 10/20, Batch: 450/793, Loss: 1.2276\n",
      "Epoch: 10/20, Batch: 500/793, Loss: 0.8132\n",
      "Epoch: 10/20, Batch: 550/793, Loss: 0.7598\n",
      "Epoch: 10/20, Batch: 600/793, Loss: 0.7114\n",
      "Epoch: 10/20, Batch: 650/793, Loss: 0.7929\n",
      "Epoch: 10/20, Batch: 700/793, Loss: 0.8282\n",
      "Epoch: 10/20, Batch: 750/793, Loss: 1.3495\n",
      "Epoch: 10, Train Loss: 0.9672\n",
      "Epoch: 11/20, Batch: 0/793, Loss: 0.8213\n",
      "Epoch: 11/20, Batch: 50/793, Loss: 0.8649\n",
      "Epoch: 11/20, Batch: 100/793, Loss: 0.7571\n",
      "Epoch: 11/20, Batch: 150/793, Loss: 0.8108\n",
      "Epoch: 11/20, Batch: 200/793, Loss: 0.7911\n",
      "Epoch: 11/20, Batch: 250/793, Loss: 0.8073\n",
      "Epoch: 11/20, Batch: 300/793, Loss: 0.7907\n",
      "Epoch: 11/20, Batch: 350/793, Loss: 0.9087\n",
      "Epoch: 11/20, Batch: 400/793, Loss: 0.8963\n",
      "Epoch: 11/20, Batch: 450/793, Loss: 1.2311\n",
      "Epoch: 11/20, Batch: 500/793, Loss: 0.9052\n",
      "Epoch: 11/20, Batch: 550/793, Loss: 0.8040\n",
      "Epoch: 11/20, Batch: 600/793, Loss: 0.9481\n",
      "Epoch: 11/20, Batch: 650/793, Loss: 1.3274\n",
      "Epoch: 11/20, Batch: 700/793, Loss: 0.9586\n",
      "Epoch: 11/20, Batch: 750/793, Loss: 0.8399\n",
      "Epoch: 11, Train Loss: 0.9666\n",
      "Epoch: 12/20, Batch: 0/793, Loss: 1.2432\n",
      "Epoch: 12/20, Batch: 50/793, Loss: 1.1072\n",
      "Epoch: 12/20, Batch: 100/793, Loss: 0.8129\n",
      "Epoch: 12/20, Batch: 150/793, Loss: 0.9543\n",
      "Epoch: 12/20, Batch: 200/793, Loss: 0.8818\n",
      "Epoch: 12/20, Batch: 250/793, Loss: 1.0166\n",
      "Epoch: 12/20, Batch: 300/793, Loss: 0.9861\n",
      "Epoch: 12/20, Batch: 350/793, Loss: 0.8732\n",
      "Epoch: 12/20, Batch: 400/793, Loss: 1.1370\n",
      "Epoch: 12/20, Batch: 450/793, Loss: 0.8511\n",
      "Epoch: 12/20, Batch: 500/793, Loss: 1.0272\n",
      "Epoch: 12/20, Batch: 550/793, Loss: 1.2613\n",
      "Epoch: 12/20, Batch: 600/793, Loss: 0.8124\n",
      "Epoch: 12/20, Batch: 650/793, Loss: 0.7660\n",
      "Epoch: 12/20, Batch: 700/793, Loss: 0.8481\n",
      "Epoch: 12/20, Batch: 750/793, Loss: 0.9180\n",
      "Epoch: 12, Train Loss: 0.9668\n",
      "Epoch: 13/20, Batch: 0/793, Loss: 1.0007\n",
      "Epoch: 13/20, Batch: 50/793, Loss: 1.0894\n",
      "Epoch: 13/20, Batch: 100/793, Loss: 0.7520\n",
      "Epoch: 13/20, Batch: 150/793, Loss: 1.1600\n",
      "Epoch: 13/20, Batch: 200/793, Loss: 1.0026\n",
      "Epoch: 13/20, Batch: 250/793, Loss: 0.9203\n",
      "Epoch: 13/20, Batch: 300/793, Loss: 1.0966\n",
      "Epoch: 13/20, Batch: 350/793, Loss: 0.8195\n",
      "Epoch: 13/20, Batch: 400/793, Loss: 1.3336\n",
      "Epoch: 13/20, Batch: 450/793, Loss: 1.0498\n",
      "Epoch: 13/20, Batch: 500/793, Loss: 0.7388\n",
      "Epoch: 13/20, Batch: 550/793, Loss: 0.9484\n",
      "Epoch: 13/20, Batch: 600/793, Loss: 1.0814\n",
      "Epoch: 13/20, Batch: 650/793, Loss: 0.8264\n",
      "Epoch: 13/20, Batch: 700/793, Loss: 1.0021\n",
      "Epoch: 13/20, Batch: 750/793, Loss: 0.7832\n",
      "Epoch: 13, Train Loss: 0.9640\n",
      "Epoch: 14/20, Batch: 0/793, Loss: 0.9582\n",
      "Epoch: 14/20, Batch: 50/793, Loss: 1.0840\n",
      "Epoch: 14/20, Batch: 100/793, Loss: 1.0849\n",
      "Epoch: 14/20, Batch: 150/793, Loss: 0.8898\n",
      "Epoch: 14/20, Batch: 200/793, Loss: 0.9409\n",
      "Epoch: 14/20, Batch: 250/793, Loss: 1.2006\n",
      "Epoch: 14/20, Batch: 300/793, Loss: 1.1413\n",
      "Epoch: 14/20, Batch: 350/793, Loss: 0.8102\n",
      "Epoch: 14/20, Batch: 400/793, Loss: 0.9080\n",
      "Epoch: 14/20, Batch: 450/793, Loss: 1.0787\n",
      "Epoch: 14/20, Batch: 500/793, Loss: 0.9104\n",
      "Epoch: 14/20, Batch: 550/793, Loss: 0.9708\n",
      "Epoch: 14/20, Batch: 600/793, Loss: 0.7554\n",
      "Epoch: 14/20, Batch: 650/793, Loss: 1.0131\n",
      "Epoch: 14/20, Batch: 700/793, Loss: 0.6523\n",
      "Epoch: 14/20, Batch: 750/793, Loss: 1.1936\n",
      "Epoch: 14, Train Loss: 0.9645\n",
      "Epoch: 15/20, Batch: 0/793, Loss: 1.2067\n",
      "Epoch: 15/20, Batch: 50/793, Loss: 1.1169\n",
      "Epoch: 15/20, Batch: 100/793, Loss: 1.0417\n",
      "Epoch: 15/20, Batch: 150/793, Loss: 0.8450\n",
      "Epoch: 15/20, Batch: 200/793, Loss: 0.6973\n",
      "Epoch: 15/20, Batch: 250/793, Loss: 0.8550\n",
      "Epoch: 15/20, Batch: 300/793, Loss: 0.8840\n",
      "Epoch: 15/20, Batch: 350/793, Loss: 0.9015\n",
      "Epoch: 15/20, Batch: 400/793, Loss: 0.9108\n",
      "Epoch: 15/20, Batch: 450/793, Loss: 0.9168\n",
      "Epoch: 15/20, Batch: 500/793, Loss: 0.8359\n",
      "Epoch: 15/20, Batch: 550/793, Loss: 0.8152\n",
      "Epoch: 15/20, Batch: 600/793, Loss: 1.0869\n",
      "Epoch: 15/20, Batch: 650/793, Loss: 1.0159\n",
      "Epoch: 15/20, Batch: 700/793, Loss: 0.8327\n",
      "Epoch: 15/20, Batch: 750/793, Loss: 1.1115\n",
      "Epoch: 15, Train Loss: 0.9603\n",
      "Epoch: 16/20, Batch: 0/793, Loss: 0.7815\n",
      "Epoch: 16/20, Batch: 50/793, Loss: 0.8261\n",
      "Epoch: 16/20, Batch: 100/793, Loss: 0.8944\n",
      "Epoch: 16/20, Batch: 150/793, Loss: 0.9109\n",
      "Epoch: 16/20, Batch: 200/793, Loss: 0.7353\n",
      "Epoch: 16/20, Batch: 250/793, Loss: 1.4583\n",
      "Epoch: 16/20, Batch: 300/793, Loss: 1.2251\n",
      "Epoch: 16/20, Batch: 350/793, Loss: 0.9890\n",
      "Epoch: 16/20, Batch: 400/793, Loss: 0.9817\n",
      "Epoch: 16/20, Batch: 450/793, Loss: 0.9493\n",
      "Epoch: 16/20, Batch: 500/793, Loss: 0.8384\n",
      "Epoch: 16/20, Batch: 550/793, Loss: 0.8016\n",
      "Epoch: 16/20, Batch: 600/793, Loss: 0.8229\n",
      "Epoch: 16/20, Batch: 650/793, Loss: 0.8057\n",
      "Epoch: 16/20, Batch: 700/793, Loss: 0.8271\n",
      "Epoch: 16/20, Batch: 750/793, Loss: 0.9509\n",
      "Epoch: 16, Train Loss: 0.9626\n",
      "Epoch: 17/20, Batch: 0/793, Loss: 1.0459\n",
      "Epoch: 17/20, Batch: 50/793, Loss: 0.8835\n",
      "Epoch: 17/20, Batch: 100/793, Loss: 1.1836\n",
      "Epoch: 17/20, Batch: 150/793, Loss: 0.9472\n",
      "Epoch: 17/20, Batch: 200/793, Loss: 0.7507\n",
      "Epoch: 17/20, Batch: 250/793, Loss: 0.7462\n",
      "Epoch: 17/20, Batch: 300/793, Loss: 1.1020\n",
      "Epoch: 17/20, Batch: 350/793, Loss: 0.7569\n",
      "Epoch: 17/20, Batch: 400/793, Loss: 0.9874\n",
      "Epoch: 17/20, Batch: 450/793, Loss: 0.7422\n",
      "Epoch: 17/20, Batch: 500/793, Loss: 0.7940\n",
      "Epoch: 17/20, Batch: 550/793, Loss: 0.9983\n",
      "Epoch: 17/20, Batch: 600/793, Loss: 1.0859\n",
      "Epoch: 17/20, Batch: 650/793, Loss: 0.8719\n",
      "Epoch: 17/20, Batch: 700/793, Loss: 0.9806\n",
      "Epoch: 17/20, Batch: 750/793, Loss: 0.9859\n",
      "Epoch: 17, Train Loss: 0.9621\n",
      "Epoch: 18/20, Batch: 0/793, Loss: 0.7478\n",
      "Epoch: 18/20, Batch: 50/793, Loss: 0.7809\n",
      "Epoch: 18/20, Batch: 100/793, Loss: 0.6839\n",
      "Epoch: 18/20, Batch: 150/793, Loss: 1.0320\n",
      "Epoch: 18/20, Batch: 200/793, Loss: 1.5132\n",
      "Epoch: 18/20, Batch: 250/793, Loss: 1.3753\n",
      "Epoch: 18/20, Batch: 300/793, Loss: 1.1551\n",
      "Epoch: 18/20, Batch: 350/793, Loss: 1.4975\n",
      "Epoch: 18/20, Batch: 400/793, Loss: 1.2738\n",
      "Epoch: 18/20, Batch: 450/793, Loss: 0.9233\n",
      "Epoch: 18/20, Batch: 500/793, Loss: 1.1357\n",
      "Epoch: 18/20, Batch: 550/793, Loss: 1.3110\n",
      "Epoch: 18/20, Batch: 600/793, Loss: 1.3097\n",
      "Epoch: 18/20, Batch: 650/793, Loss: 1.0581\n",
      "Epoch: 18/20, Batch: 700/793, Loss: 1.0284\n",
      "Epoch: 18/20, Batch: 750/793, Loss: 0.6951\n",
      "Epoch: 18, Train Loss: 0.9619\n",
      "Epoch: 19/20, Batch: 0/793, Loss: 0.8354\n",
      "Epoch: 19/20, Batch: 50/793, Loss: 1.1360\n",
      "Epoch: 19/20, Batch: 100/793, Loss: 0.9956\n",
      "Epoch: 19/20, Batch: 150/793, Loss: 0.7934\n",
      "Epoch: 19/20, Batch: 200/793, Loss: 1.1036\n",
      "Epoch: 19/20, Batch: 250/793, Loss: 0.9784\n",
      "Epoch: 19/20, Batch: 300/793, Loss: 0.9503\n",
      "Epoch: 19/20, Batch: 350/793, Loss: 0.7567\n",
      "Epoch: 19/20, Batch: 400/793, Loss: 1.1292\n",
      "Epoch: 19/20, Batch: 450/793, Loss: 0.7644\n",
      "Epoch: 19/20, Batch: 500/793, Loss: 0.7056\n",
      "Epoch: 19/20, Batch: 550/793, Loss: 0.8815\n",
      "Epoch: 19/20, Batch: 600/793, Loss: 1.5988\n",
      "Epoch: 19/20, Batch: 650/793, Loss: 0.8619\n",
      "Epoch: 19/20, Batch: 700/793, Loss: 0.8132\n",
      "Epoch: 19/20, Batch: 750/793, Loss: 0.6391\n",
      "Epoch: 19, Train Loss: 0.9606\n",
      "Epoch: 20/20, Batch: 0/793, Loss: 0.9915\n",
      "Epoch: 20/20, Batch: 50/793, Loss: 0.7882\n",
      "Epoch: 20/20, Batch: 100/793, Loss: 1.1971\n",
      "Epoch: 20/20, Batch: 150/793, Loss: 0.6697\n",
      "Epoch: 20/20, Batch: 200/793, Loss: 1.1772\n",
      "Epoch: 20/20, Batch: 250/793, Loss: 1.0271\n",
      "Epoch: 20/20, Batch: 300/793, Loss: 0.8830\n",
      "Epoch: 20/20, Batch: 350/793, Loss: 0.6957\n",
      "Epoch: 20/20, Batch: 400/793, Loss: 0.6508\n",
      "Epoch: 20/20, Batch: 450/793, Loss: 0.8486\n",
      "Epoch: 20/20, Batch: 500/793, Loss: 0.9457\n",
      "Epoch: 20/20, Batch: 550/793, Loss: 1.0562\n",
      "Epoch: 20/20, Batch: 600/793, Loss: 0.8343\n",
      "Epoch: 20/20, Batch: 650/793, Loss: 0.8279\n",
      "Epoch: 20/20, Batch: 700/793, Loss: 1.0524\n",
      "Epoch: 20/20, Batch: 750/793, Loss: 0.9470\n",
      "Epoch: 20, Train Loss: 0.9588\n",
      "Training finished!\n"
     ]
    }
   ],
   "source": [
    "total_size = len(X)\n",
    "train_size = int(0.4 * total_size)\n",
    "test_size = total_size - train_size\n",
    "\n",
    "dataset = HeartDiseaseDataset(X, y)\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1024, shuffle=False)\n",
    "\n",
    "input_features = X.shape[1]  \n",
    "\n",
    "weighted_model_3 = HeartDiseaseMLPClassifier(input_size=input_features, class_frequencies=class_frequencies).to(device)\n",
    "\n",
    "optimizer = optim.Adam(weighted_model_3.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)\n",
    "\n",
    "num_epochs = 20\n",
    "weighted_train_losses = []\n",
    "\n",
    "print(\"\\nStarting training...\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    weighted_model_3.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = weighted_model_3(data)\n",
    "    \n",
    "        target = target.float()\n",
    "        output = output.float()    \n",
    "        loss = weighted_model_3.get_weighted_loss(output, target)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if batch_idx % 50 == 0:\n",
    "            print(f'Epoch: {epoch+1}/{num_epochs}, Batch: {batch_idx}/{len(train_loader)}, Loss: {loss.item():.4f}')\n",
    "    \n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "    weighted_train_losses.append(avg_train_loss)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1}, Train Loss: {avg_train_loss:.4f}')\n",
    "\n",
    "print('Training finished!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report (Weighted 3):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.72      0.83    137846\n",
      "         1.0       0.23      0.83      0.36     14362\n",
      "\n",
      "    accuracy                           0.73    152208\n",
      "   macro avg       0.60      0.77      0.60    152208\n",
      "weighted avg       0.91      0.73      0.78    152208\n",
      "\n",
      "\n",
      "Confusion Matrix (Weighted 3):\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "weighted_model_3.eval()\n",
    "\n",
    "y_true_weighted_3 = []\n",
    "y_pred_weighted_3 = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        output = weighted_model_3(data)\n",
    "        \n",
    "        y_true_weighted_3.extend(target.cpu().numpy())\n",
    "        y_pred_weighted_3.extend((output > 0.5).cpu().numpy())\n",
    "\n",
    "y_true_weighted_3 = np.array(y_true_weighted_3)\n",
    "y_pred_weighted_3 = np.array(y_pred_weighted_3)\n",
    "\n",
    "print(\"\\nClassification Report (Weighted 3):\")\n",
    "print(classification_report(y_true_weighted_3, y_pred_weighted_3))\n",
    "print(\"\\nConfusion Matrix (Weighted 3):\")\n",
    "cm_weighted_3 = confusion_matrix(y_true_weighted_3, y_pred_weighted_3)\n",
    "sns.heatmap(cm_weighted_3, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix (Weighted 3)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensional Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train:test = 4:1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "std = StandardScaler()\n",
    "X_train_scaled = std.fit_transform(X_train)\n",
    "X_test_scaled = std.transform(X_test)\n",
    "\n",
    "pca = PCA(n_components=14)\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "train_dataset = HeartDiseaseDataset(X_train_pca, y_train)\n",
    "test_dataset = HeartDiseaseDataset(X_test_pca, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1024, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training...\n",
      "Epoch: 1/20, Batch: 0/1586, Loss: 1.6164\n",
      "Epoch: 1/20, Batch: 50/1586, Loss: 1.0990\n",
      "Epoch: 1/20, Batch: 100/1586, Loss: 0.8676\n",
      "Epoch: 1/20, Batch: 150/1586, Loss: 0.7939\n",
      "Epoch: 1/20, Batch: 200/1586, Loss: 1.0151\n",
      "Epoch: 1/20, Batch: 250/1586, Loss: 0.9033\n",
      "Epoch: 1/20, Batch: 300/1586, Loss: 0.9121\n",
      "Epoch: 1/20, Batch: 350/1586, Loss: 1.1611\n",
      "Epoch: 1/20, Batch: 400/1586, Loss: 1.0669\n",
      "Epoch: 1/20, Batch: 450/1586, Loss: 1.1088\n",
      "Epoch: 1/20, Batch: 500/1586, Loss: 1.0694\n",
      "Epoch: 1/20, Batch: 550/1586, Loss: 0.9989\n",
      "Epoch: 1/20, Batch: 600/1586, Loss: 0.9732\n",
      "Epoch: 1/20, Batch: 650/1586, Loss: 1.0244\n",
      "Epoch: 1/20, Batch: 700/1586, Loss: 0.8831\n",
      "Epoch: 1/20, Batch: 750/1586, Loss: 0.7434\n",
      "Epoch: 1/20, Batch: 800/1586, Loss: 1.2029\n",
      "Epoch: 1/20, Batch: 850/1586, Loss: 0.6659\n",
      "Epoch: 1/20, Batch: 900/1586, Loss: 1.2023\n",
      "Epoch: 1/20, Batch: 950/1586, Loss: 0.8830\n",
      "Epoch: 1/20, Batch: 1000/1586, Loss: 1.0838\n",
      "Epoch: 1/20, Batch: 1050/1586, Loss: 0.6963\n",
      "Epoch: 1/20, Batch: 1100/1586, Loss: 0.8173\n",
      "Epoch: 1/20, Batch: 1150/1586, Loss: 1.1617\n",
      "Epoch: 1/20, Batch: 1200/1586, Loss: 0.9212\n",
      "Epoch: 1/20, Batch: 1250/1586, Loss: 0.8537\n",
      "Epoch: 1/20, Batch: 1300/1586, Loss: 1.1196\n",
      "Epoch: 1/20, Batch: 1350/1586, Loss: 1.2137\n",
      "Epoch: 1/20, Batch: 1400/1586, Loss: 0.7242\n",
      "Epoch: 1/20, Batch: 1450/1586, Loss: 0.8419\n",
      "Epoch: 1/20, Batch: 1500/1586, Loss: 1.0723\n",
      "Epoch: 1/20, Batch: 1550/1586, Loss: 0.7537\n",
      "Epoch: 1, Train Loss: 1.0131\n",
      "Epoch: 2/20, Batch: 0/1586, Loss: 0.9153\n",
      "Epoch: 2/20, Batch: 50/1586, Loss: 0.9648\n",
      "Epoch: 2/20, Batch: 100/1586, Loss: 1.0118\n",
      "Epoch: 2/20, Batch: 150/1586, Loss: 0.8862\n",
      "Epoch: 2/20, Batch: 200/1586, Loss: 1.0102\n",
      "Epoch: 2/20, Batch: 250/1586, Loss: 0.8755\n",
      "Epoch: 2/20, Batch: 300/1586, Loss: 1.0274\n",
      "Epoch: 2/20, Batch: 350/1586, Loss: 0.9183\n",
      "Epoch: 2/20, Batch: 400/1586, Loss: 1.0616\n",
      "Epoch: 2/20, Batch: 450/1586, Loss: 1.0242\n",
      "Epoch: 2/20, Batch: 500/1586, Loss: 0.8273\n",
      "Epoch: 2/20, Batch: 550/1586, Loss: 1.0028\n",
      "Epoch: 2/20, Batch: 600/1586, Loss: 0.9480\n",
      "Epoch: 2/20, Batch: 650/1586, Loss: 0.8767\n",
      "Epoch: 2/20, Batch: 700/1586, Loss: 1.0695\n",
      "Epoch: 2/20, Batch: 750/1586, Loss: 1.2166\n",
      "Epoch: 2/20, Batch: 800/1586, Loss: 1.0070\n",
      "Epoch: 2/20, Batch: 850/1586, Loss: 0.8687\n",
      "Epoch: 2/20, Batch: 900/1586, Loss: 0.6961\n",
      "Epoch: 2/20, Batch: 950/1586, Loss: 0.9312\n",
      "Epoch: 2/20, Batch: 1000/1586, Loss: 1.1767\n",
      "Epoch: 2/20, Batch: 1050/1586, Loss: 0.7716\n",
      "Epoch: 2/20, Batch: 1100/1586, Loss: 0.9938\n",
      "Epoch: 2/20, Batch: 1150/1586, Loss: 0.7061\n",
      "Epoch: 2/20, Batch: 1200/1586, Loss: 1.0479\n",
      "Epoch: 2/20, Batch: 1250/1586, Loss: 1.2042\n",
      "Epoch: 2/20, Batch: 1300/1586, Loss: 0.9687\n",
      "Epoch: 2/20, Batch: 1350/1586, Loss: 1.0020\n",
      "Epoch: 2/20, Batch: 1400/1586, Loss: 0.8664\n",
      "Epoch: 2/20, Batch: 1450/1586, Loss: 0.6698\n",
      "Epoch: 2/20, Batch: 1500/1586, Loss: 0.7204\n",
      "Epoch: 2/20, Batch: 1550/1586, Loss: 1.2072\n",
      "Epoch: 2, Train Loss: 0.9979\n",
      "Epoch: 3/20, Batch: 0/1586, Loss: 0.8017\n",
      "Epoch: 3/20, Batch: 50/1586, Loss: 0.9091\n",
      "Epoch: 3/20, Batch: 100/1586, Loss: 0.6889\n",
      "Epoch: 3/20, Batch: 150/1586, Loss: 1.2011\n",
      "Epoch: 3/20, Batch: 200/1586, Loss: 0.8199\n",
      "Epoch: 3/20, Batch: 250/1586, Loss: 0.7875\n",
      "Epoch: 3/20, Batch: 300/1586, Loss: 1.1157\n",
      "Epoch: 3/20, Batch: 350/1586, Loss: 1.2534\n",
      "Epoch: 3/20, Batch: 400/1586, Loss: 1.0087\n",
      "Epoch: 3/20, Batch: 450/1586, Loss: 1.1872\n",
      "Epoch: 3/20, Batch: 500/1586, Loss: 1.0579\n",
      "Epoch: 3/20, Batch: 550/1586, Loss: 0.8423\n",
      "Epoch: 3/20, Batch: 600/1586, Loss: 0.8751\n",
      "Epoch: 3/20, Batch: 650/1586, Loss: 0.9619\n",
      "Epoch: 3/20, Batch: 700/1586, Loss: 1.4558\n",
      "Epoch: 3/20, Batch: 750/1586, Loss: 0.9364\n",
      "Epoch: 3/20, Batch: 800/1586, Loss: 0.9983\n",
      "Epoch: 3/20, Batch: 850/1586, Loss: 1.4821\n",
      "Epoch: 3/20, Batch: 900/1586, Loss: 0.9824\n",
      "Epoch: 3/20, Batch: 950/1586, Loss: 0.8747\n",
      "Epoch: 3/20, Batch: 1000/1586, Loss: 1.2925\n",
      "Epoch: 3/20, Batch: 1050/1586, Loss: 0.7743\n",
      "Epoch: 3/20, Batch: 1100/1586, Loss: 0.8171\n",
      "Epoch: 3/20, Batch: 1150/1586, Loss: 0.9491\n",
      "Epoch: 3/20, Batch: 1200/1586, Loss: 0.9901\n",
      "Epoch: 3/20, Batch: 1250/1586, Loss: 0.8480\n",
      "Epoch: 3/20, Batch: 1300/1586, Loss: 0.7747\n",
      "Epoch: 3/20, Batch: 1350/1586, Loss: 0.9133\n",
      "Epoch: 3/20, Batch: 1400/1586, Loss: 0.8944\n",
      "Epoch: 3/20, Batch: 1450/1586, Loss: 0.9698\n",
      "Epoch: 3/20, Batch: 1500/1586, Loss: 0.7772\n",
      "Epoch: 3/20, Batch: 1550/1586, Loss: 0.9315\n",
      "Epoch: 3, Train Loss: 0.9931\n",
      "Epoch: 4/20, Batch: 0/1586, Loss: 0.9466\n",
      "Epoch: 4/20, Batch: 50/1586, Loss: 0.7545\n",
      "Epoch: 4/20, Batch: 100/1586, Loss: 0.7203\n",
      "Epoch: 4/20, Batch: 150/1586, Loss: 1.5076\n",
      "Epoch: 4/20, Batch: 200/1586, Loss: 1.4824\n",
      "Epoch: 4/20, Batch: 250/1586, Loss: 0.8568\n",
      "Epoch: 4/20, Batch: 300/1586, Loss: 1.1705\n",
      "Epoch: 4/20, Batch: 350/1586, Loss: 1.0687\n",
      "Epoch: 4/20, Batch: 400/1586, Loss: 0.9356\n",
      "Epoch: 4/20, Batch: 450/1586, Loss: 0.8155\n",
      "Epoch: 4/20, Batch: 500/1586, Loss: 1.1260\n",
      "Epoch: 4/20, Batch: 550/1586, Loss: 1.1032\n",
      "Epoch: 4/20, Batch: 600/1586, Loss: 0.7070\n",
      "Epoch: 4/20, Batch: 650/1586, Loss: 0.8901\n",
      "Epoch: 4/20, Batch: 700/1586, Loss: 1.1151\n",
      "Epoch: 4/20, Batch: 750/1586, Loss: 1.0244\n",
      "Epoch: 4/20, Batch: 800/1586, Loss: 0.7843\n",
      "Epoch: 4/20, Batch: 850/1586, Loss: 0.7435\n",
      "Epoch: 4/20, Batch: 900/1586, Loss: 0.9811\n",
      "Epoch: 4/20, Batch: 950/1586, Loss: 0.8088\n",
      "Epoch: 4/20, Batch: 1000/1586, Loss: 0.9894\n",
      "Epoch: 4/20, Batch: 1050/1586, Loss: 1.1408\n",
      "Epoch: 4/20, Batch: 1100/1586, Loss: 1.0487\n",
      "Epoch: 4/20, Batch: 1150/1586, Loss: 0.8605\n",
      "Epoch: 4/20, Batch: 1200/1586, Loss: 1.0922\n",
      "Epoch: 4/20, Batch: 1250/1586, Loss: 0.7594\n",
      "Epoch: 4/20, Batch: 1300/1586, Loss: 1.0257\n",
      "Epoch: 4/20, Batch: 1350/1586, Loss: 1.0457\n",
      "Epoch: 4/20, Batch: 1400/1586, Loss: 0.8537\n",
      "Epoch: 4/20, Batch: 1450/1586, Loss: 0.7515\n",
      "Epoch: 4/20, Batch: 1500/1586, Loss: 0.8405\n",
      "Epoch: 4/20, Batch: 1550/1586, Loss: 1.0300\n",
      "Epoch: 4, Train Loss: 0.9906\n",
      "Epoch: 5/20, Batch: 0/1586, Loss: 0.9060\n",
      "Epoch: 5/20, Batch: 50/1586, Loss: 0.8445\n",
      "Epoch: 5/20, Batch: 100/1586, Loss: 1.0299\n",
      "Epoch: 5/20, Batch: 150/1586, Loss: 0.6845\n",
      "Epoch: 5/20, Batch: 200/1586, Loss: 1.1081\n",
      "Epoch: 5/20, Batch: 250/1586, Loss: 0.7786\n",
      "Epoch: 5/20, Batch: 300/1586, Loss: 0.7431\n",
      "Epoch: 5/20, Batch: 350/1586, Loss: 1.2191\n",
      "Epoch: 5/20, Batch: 400/1586, Loss: 1.4643\n",
      "Epoch: 5/20, Batch: 450/1586, Loss: 0.7487\n",
      "Epoch: 5/20, Batch: 500/1586, Loss: 1.1058\n",
      "Epoch: 5/20, Batch: 550/1586, Loss: 1.0918\n",
      "Epoch: 5/20, Batch: 600/1586, Loss: 1.1356\n",
      "Epoch: 5/20, Batch: 650/1586, Loss: 0.9647\n",
      "Epoch: 5/20, Batch: 700/1586, Loss: 0.8877\n",
      "Epoch: 5/20, Batch: 750/1586, Loss: 1.3323\n",
      "Epoch: 5/20, Batch: 800/1586, Loss: 1.3019\n",
      "Epoch: 5/20, Batch: 850/1586, Loss: 0.9758\n",
      "Epoch: 5/20, Batch: 900/1586, Loss: 1.2340\n",
      "Epoch: 5/20, Batch: 950/1586, Loss: 0.9846\n",
      "Epoch: 5/20, Batch: 1000/1586, Loss: 0.8850\n",
      "Epoch: 5/20, Batch: 1050/1586, Loss: 0.8649\n",
      "Epoch: 5/20, Batch: 1100/1586, Loss: 0.9002\n",
      "Epoch: 5/20, Batch: 1150/1586, Loss: 1.2342\n",
      "Epoch: 5/20, Batch: 1200/1586, Loss: 1.0000\n",
      "Epoch: 5/20, Batch: 1250/1586, Loss: 0.9621\n",
      "Epoch: 5/20, Batch: 1300/1586, Loss: 0.9853\n",
      "Epoch: 5/20, Batch: 1350/1586, Loss: 0.9376\n",
      "Epoch: 5/20, Batch: 1400/1586, Loss: 1.0992\n",
      "Epoch: 5/20, Batch: 1450/1586, Loss: 0.9970\n",
      "Epoch: 5/20, Batch: 1500/1586, Loss: 1.1594\n",
      "Epoch: 5/20, Batch: 1550/1586, Loss: 1.1471\n",
      "Epoch: 5, Train Loss: 0.9894\n",
      "Epoch: 6/20, Batch: 0/1586, Loss: 0.7537\n",
      "Epoch: 6/20, Batch: 50/1586, Loss: 0.8528\n",
      "Epoch: 6/20, Batch: 100/1586, Loss: 0.9853\n",
      "Epoch: 6/20, Batch: 150/1586, Loss: 1.3371\n",
      "Epoch: 6/20, Batch: 200/1586, Loss: 0.7241\n",
      "Epoch: 6/20, Batch: 250/1586, Loss: 1.2598\n",
      "Epoch: 6/20, Batch: 300/1586, Loss: 1.3750\n",
      "Epoch: 6/20, Batch: 350/1586, Loss: 0.9891\n",
      "Epoch: 6/20, Batch: 400/1586, Loss: 1.0360\n",
      "Epoch: 6/20, Batch: 450/1586, Loss: 0.9462\n",
      "Epoch: 6/20, Batch: 500/1586, Loss: 1.1016\n",
      "Epoch: 6/20, Batch: 550/1586, Loss: 0.8298\n",
      "Epoch: 6/20, Batch: 600/1586, Loss: 1.0934\n",
      "Epoch: 6/20, Batch: 650/1586, Loss: 0.8238\n",
      "Epoch: 6/20, Batch: 700/1586, Loss: 0.9884\n",
      "Epoch: 6/20, Batch: 750/1586, Loss: 0.9337\n",
      "Epoch: 6/20, Batch: 800/1586, Loss: 1.0977\n",
      "Epoch: 6/20, Batch: 850/1586, Loss: 0.9101\n",
      "Epoch: 6/20, Batch: 900/1586, Loss: 1.0782\n",
      "Epoch: 6/20, Batch: 950/1586, Loss: 1.0361\n",
      "Epoch: 6/20, Batch: 1000/1586, Loss: 1.0939\n",
      "Epoch: 6/20, Batch: 1050/1586, Loss: 0.8431\n",
      "Epoch: 6/20, Batch: 1100/1586, Loss: 0.7228\n",
      "Epoch: 6/20, Batch: 1150/1586, Loss: 1.5450\n",
      "Epoch: 6/20, Batch: 1200/1586, Loss: 1.0753\n",
      "Epoch: 6/20, Batch: 1250/1586, Loss: 0.9658\n",
      "Epoch: 6/20, Batch: 1300/1586, Loss: 1.8318\n",
      "Epoch: 6/20, Batch: 1350/1586, Loss: 0.8799\n",
      "Epoch: 6/20, Batch: 1400/1586, Loss: 0.8497\n",
      "Epoch: 6/20, Batch: 1450/1586, Loss: 1.2359\n",
      "Epoch: 6/20, Batch: 1500/1586, Loss: 1.2600\n",
      "Epoch: 6/20, Batch: 1550/1586, Loss: 0.8123\n",
      "Epoch: 6, Train Loss: 0.9889\n",
      "Epoch: 7/20, Batch: 0/1586, Loss: 1.0703\n",
      "Epoch: 7/20, Batch: 50/1586, Loss: 0.9494\n",
      "Epoch: 7/20, Batch: 100/1586, Loss: 1.3946\n",
      "Epoch: 7/20, Batch: 150/1586, Loss: 0.8512\n",
      "Epoch: 7/20, Batch: 200/1586, Loss: 0.7975\n",
      "Epoch: 7/20, Batch: 250/1586, Loss: 1.1170\n",
      "Epoch: 7/20, Batch: 300/1586, Loss: 0.9491\n",
      "Epoch: 7/20, Batch: 350/1586, Loss: 0.8716\n",
      "Epoch: 7/20, Batch: 400/1586, Loss: 0.8874\n",
      "Epoch: 7/20, Batch: 450/1586, Loss: 1.3177\n",
      "Epoch: 7/20, Batch: 500/1586, Loss: 1.2609\n",
      "Epoch: 7/20, Batch: 550/1586, Loss: 0.6751\n",
      "Epoch: 7/20, Batch: 600/1586, Loss: 1.1194\n",
      "Epoch: 7/20, Batch: 650/1586, Loss: 0.9198\n",
      "Epoch: 7/20, Batch: 700/1586, Loss: 0.8662\n",
      "Epoch: 7/20, Batch: 750/1586, Loss: 1.4252\n",
      "Epoch: 7/20, Batch: 800/1586, Loss: 1.0153\n",
      "Epoch: 7/20, Batch: 850/1586, Loss: 0.9937\n",
      "Epoch: 7/20, Batch: 900/1586, Loss: 1.0464\n",
      "Epoch: 7/20, Batch: 950/1586, Loss: 0.8552\n",
      "Epoch: 7/20, Batch: 1000/1586, Loss: 1.1928\n",
      "Epoch: 7/20, Batch: 1050/1586, Loss: 1.0546\n",
      "Epoch: 7/20, Batch: 1100/1586, Loss: 0.9496\n",
      "Epoch: 7/20, Batch: 1150/1586, Loss: 0.7223\n",
      "Epoch: 7/20, Batch: 1200/1586, Loss: 1.0631\n",
      "Epoch: 7/20, Batch: 1250/1586, Loss: 0.9899\n",
      "Epoch: 7/20, Batch: 1300/1586, Loss: 0.8895\n",
      "Epoch: 7/20, Batch: 1350/1586, Loss: 0.9043\n",
      "Epoch: 7/20, Batch: 1400/1586, Loss: 1.2203\n",
      "Epoch: 7/20, Batch: 1450/1586, Loss: 1.1384\n",
      "Epoch: 7/20, Batch: 1500/1586, Loss: 0.9606\n",
      "Epoch: 7/20, Batch: 1550/1586, Loss: 1.0293\n",
      "Epoch: 7, Train Loss: 0.9869\n",
      "Epoch: 8/20, Batch: 0/1586, Loss: 1.0132\n",
      "Epoch: 8/20, Batch: 50/1586, Loss: 0.8106\n",
      "Epoch: 8/20, Batch: 100/1586, Loss: 0.7711\n",
      "Epoch: 8/20, Batch: 150/1586, Loss: 1.2121\n",
      "Epoch: 8/20, Batch: 200/1586, Loss: 0.8158\n",
      "Epoch: 8/20, Batch: 250/1586, Loss: 0.8592\n",
      "Epoch: 8/20, Batch: 300/1586, Loss: 0.7854\n",
      "Epoch: 8/20, Batch: 350/1586, Loss: 0.7710\n",
      "Epoch: 8/20, Batch: 400/1586, Loss: 0.9195\n",
      "Epoch: 8/20, Batch: 450/1586, Loss: 0.8677\n",
      "Epoch: 8/20, Batch: 500/1586, Loss: 0.7561\n",
      "Epoch: 8/20, Batch: 550/1586, Loss: 0.8198\n",
      "Epoch: 8/20, Batch: 600/1586, Loss: 1.1568\n",
      "Epoch: 8/20, Batch: 650/1586, Loss: 0.7923\n",
      "Epoch: 8/20, Batch: 700/1586, Loss: 1.0355\n",
      "Epoch: 8/20, Batch: 750/1586, Loss: 0.7264\n",
      "Epoch: 8/20, Batch: 800/1586, Loss: 1.3901\n",
      "Epoch: 8/20, Batch: 850/1586, Loss: 1.2126\n",
      "Epoch: 8/20, Batch: 900/1586, Loss: 1.1126\n",
      "Epoch: 8/20, Batch: 950/1586, Loss: 0.8600\n",
      "Epoch: 8/20, Batch: 1000/1586, Loss: 1.0077\n",
      "Epoch: 8/20, Batch: 1050/1586, Loss: 0.8346\n",
      "Epoch: 8/20, Batch: 1100/1586, Loss: 0.8779\n",
      "Epoch: 8/20, Batch: 1150/1586, Loss: 0.7941\n",
      "Epoch: 8/20, Batch: 1200/1586, Loss: 1.3960\n",
      "Epoch: 8/20, Batch: 1250/1586, Loss: 1.1390\n",
      "Epoch: 8/20, Batch: 1300/1586, Loss: 0.6562\n",
      "Epoch: 8/20, Batch: 1350/1586, Loss: 0.9099\n",
      "Epoch: 8/20, Batch: 1400/1586, Loss: 0.7574\n",
      "Epoch: 8/20, Batch: 1450/1586, Loss: 0.8166\n",
      "Epoch: 8/20, Batch: 1500/1586, Loss: 1.2889\n",
      "Epoch: 8/20, Batch: 1550/1586, Loss: 0.7847\n",
      "Epoch: 8, Train Loss: 0.9857\n",
      "Epoch: 9/20, Batch: 0/1586, Loss: 1.0841\n",
      "Epoch: 9/20, Batch: 50/1586, Loss: 0.8971\n",
      "Epoch: 9/20, Batch: 100/1586, Loss: 1.5357\n",
      "Epoch: 9/20, Batch: 150/1586, Loss: 1.3258\n",
      "Epoch: 9/20, Batch: 200/1586, Loss: 0.6928\n",
      "Epoch: 9/20, Batch: 250/1586, Loss: 0.9718\n",
      "Epoch: 9/20, Batch: 300/1586, Loss: 0.8478\n",
      "Epoch: 9/20, Batch: 350/1586, Loss: 0.7787\n",
      "Epoch: 9/20, Batch: 400/1586, Loss: 0.7764\n",
      "Epoch: 9/20, Batch: 450/1586, Loss: 0.9786\n",
      "Epoch: 9/20, Batch: 500/1586, Loss: 0.8741\n",
      "Epoch: 9/20, Batch: 550/1586, Loss: 0.8378\n",
      "Epoch: 9/20, Batch: 600/1586, Loss: 0.8877\n",
      "Epoch: 9/20, Batch: 650/1586, Loss: 1.2095\n",
      "Epoch: 9/20, Batch: 700/1586, Loss: 1.3232\n",
      "Epoch: 9/20, Batch: 750/1586, Loss: 1.0705\n",
      "Epoch: 9/20, Batch: 800/1586, Loss: 0.9156\n",
      "Epoch: 9/20, Batch: 850/1586, Loss: 0.7882\n",
      "Epoch: 9/20, Batch: 900/1586, Loss: 1.0125\n",
      "Epoch: 9/20, Batch: 950/1586, Loss: 1.4821\n",
      "Epoch: 9/20, Batch: 1000/1586, Loss: 0.7967\n",
      "Epoch: 9/20, Batch: 1050/1586, Loss: 0.9996\n",
      "Epoch: 9/20, Batch: 1100/1586, Loss: 1.0797\n",
      "Epoch: 9/20, Batch: 1150/1586, Loss: 0.8296\n",
      "Epoch: 9/20, Batch: 1200/1586, Loss: 0.8188\n",
      "Epoch: 9/20, Batch: 1250/1586, Loss: 0.9202\n",
      "Epoch: 9/20, Batch: 1300/1586, Loss: 0.9111\n",
      "Epoch: 9/20, Batch: 1350/1586, Loss: 1.0923\n",
      "Epoch: 9/20, Batch: 1400/1586, Loss: 0.8021\n",
      "Epoch: 9/20, Batch: 1450/1586, Loss: 1.9082\n",
      "Epoch: 9/20, Batch: 1500/1586, Loss: 1.1019\n",
      "Epoch: 9/20, Batch: 1550/1586, Loss: 0.7739\n",
      "Epoch: 9, Train Loss: 0.9836\n",
      "Epoch: 10/20, Batch: 0/1586, Loss: 0.7375\n",
      "Epoch: 10/20, Batch: 50/1586, Loss: 1.1989\n",
      "Epoch: 10/20, Batch: 100/1586, Loss: 1.1161\n",
      "Epoch: 10/20, Batch: 150/1586, Loss: 0.9992\n",
      "Epoch: 10/20, Batch: 200/1586, Loss: 1.1916\n",
      "Epoch: 10/20, Batch: 250/1586, Loss: 0.8218\n",
      "Epoch: 10/20, Batch: 300/1586, Loss: 0.8557\n",
      "Epoch: 10/20, Batch: 350/1586, Loss: 1.0387\n",
      "Epoch: 10/20, Batch: 400/1586, Loss: 0.7995\n",
      "Epoch: 10/20, Batch: 450/1586, Loss: 0.6350\n",
      "Epoch: 10/20, Batch: 500/1586, Loss: 0.9491\n",
      "Epoch: 10/20, Batch: 550/1586, Loss: 0.7305\n",
      "Epoch: 10/20, Batch: 600/1586, Loss: 0.8025\n",
      "Epoch: 10/20, Batch: 650/1586, Loss: 1.0061\n",
      "Epoch: 10/20, Batch: 700/1586, Loss: 0.9169\n",
      "Epoch: 10/20, Batch: 750/1586, Loss: 0.7552\n",
      "Epoch: 10/20, Batch: 800/1586, Loss: 0.8568\n",
      "Epoch: 10/20, Batch: 850/1586, Loss: 1.1661\n",
      "Epoch: 10/20, Batch: 900/1586, Loss: 1.2206\n",
      "Epoch: 10/20, Batch: 950/1586, Loss: 1.3534\n",
      "Epoch: 10/20, Batch: 1000/1586, Loss: 1.1798\n",
      "Epoch: 10/20, Batch: 1050/1586, Loss: 0.7185\n",
      "Epoch: 10/20, Batch: 1100/1586, Loss: 1.2546\n",
      "Epoch: 10/20, Batch: 1150/1586, Loss: 0.8063\n",
      "Epoch: 10/20, Batch: 1200/1586, Loss: 1.2438\n",
      "Epoch: 10/20, Batch: 1250/1586, Loss: 0.8380\n",
      "Epoch: 10/20, Batch: 1300/1586, Loss: 0.8037\n",
      "Epoch: 10/20, Batch: 1350/1586, Loss: 1.1587\n",
      "Epoch: 10/20, Batch: 1400/1586, Loss: 1.0890\n",
      "Epoch: 10/20, Batch: 1450/1586, Loss: 1.0053\n",
      "Epoch: 10/20, Batch: 1500/1586, Loss: 1.1034\n",
      "Epoch: 10/20, Batch: 1550/1586, Loss: 1.0432\n",
      "Epoch: 10, Train Loss: 0.9830\n",
      "Epoch: 11/20, Batch: 0/1586, Loss: 1.0028\n",
      "Epoch: 11/20, Batch: 50/1586, Loss: 1.1529\n",
      "Epoch: 11/20, Batch: 100/1586, Loss: 0.7782\n",
      "Epoch: 11/20, Batch: 150/1586, Loss: 0.8500\n",
      "Epoch: 11/20, Batch: 200/1586, Loss: 1.0868\n",
      "Epoch: 11/20, Batch: 250/1586, Loss: 0.8671\n",
      "Epoch: 11/20, Batch: 300/1586, Loss: 1.0632\n",
      "Epoch: 11/20, Batch: 350/1586, Loss: 0.9019\n",
      "Epoch: 11/20, Batch: 400/1586, Loss: 0.8492\n",
      "Epoch: 11/20, Batch: 450/1586, Loss: 0.7944\n",
      "Epoch: 11/20, Batch: 500/1586, Loss: 1.2385\n",
      "Epoch: 11/20, Batch: 550/1586, Loss: 0.9109\n",
      "Epoch: 11/20, Batch: 600/1586, Loss: 0.7467\n",
      "Epoch: 11/20, Batch: 650/1586, Loss: 1.0548\n",
      "Epoch: 11/20, Batch: 700/1586, Loss: 1.0342\n",
      "Epoch: 11/20, Batch: 750/1586, Loss: 0.7997\n",
      "Epoch: 11/20, Batch: 800/1586, Loss: 0.8084\n",
      "Epoch: 11/20, Batch: 850/1586, Loss: 0.9873\n",
      "Epoch: 11/20, Batch: 900/1586, Loss: 0.7977\n",
      "Epoch: 11/20, Batch: 950/1586, Loss: 0.9530\n",
      "Epoch: 11/20, Batch: 1000/1586, Loss: 0.8169\n",
      "Epoch: 11/20, Batch: 1050/1586, Loss: 1.5103\n",
      "Epoch: 11/20, Batch: 1100/1586, Loss: 1.0094\n",
      "Epoch: 11/20, Batch: 1150/1586, Loss: 1.0716\n",
      "Epoch: 11/20, Batch: 1200/1586, Loss: 0.8868\n",
      "Epoch: 11/20, Batch: 1250/1586, Loss: 0.7443\n",
      "Epoch: 11/20, Batch: 1300/1586, Loss: 1.0052\n",
      "Epoch: 11/20, Batch: 1350/1586, Loss: 1.2167\n",
      "Epoch: 11/20, Batch: 1400/1586, Loss: 1.2358\n",
      "Epoch: 11/20, Batch: 1450/1586, Loss: 1.0407\n",
      "Epoch: 11/20, Batch: 1500/1586, Loss: 0.8203\n",
      "Epoch: 11/20, Batch: 1550/1586, Loss: 1.1494\n",
      "Epoch: 11, Train Loss: 0.9832\n",
      "Epoch: 12/20, Batch: 0/1586, Loss: 1.0603\n",
      "Epoch: 12/20, Batch: 50/1586, Loss: 1.1692\n",
      "Epoch: 12/20, Batch: 100/1586, Loss: 0.8628\n",
      "Epoch: 12/20, Batch: 150/1586, Loss: 0.6886\n",
      "Epoch: 12/20, Batch: 200/1586, Loss: 0.9446\n",
      "Epoch: 12/20, Batch: 250/1586, Loss: 0.8320\n",
      "Epoch: 12/20, Batch: 300/1586, Loss: 0.8369\n",
      "Epoch: 12/20, Batch: 350/1586, Loss: 1.2338\n",
      "Epoch: 12/20, Batch: 400/1586, Loss: 0.7844\n",
      "Epoch: 12/20, Batch: 450/1586, Loss: 0.9036\n",
      "Epoch: 12/20, Batch: 500/1586, Loss: 1.1833\n",
      "Epoch: 12/20, Batch: 550/1586, Loss: 1.1259\n",
      "Epoch: 12/20, Batch: 600/1586, Loss: 1.0610\n",
      "Epoch: 12/20, Batch: 650/1586, Loss: 0.9807\n",
      "Epoch: 12/20, Batch: 700/1586, Loss: 0.8312\n",
      "Epoch: 12/20, Batch: 750/1586, Loss: 0.9002\n",
      "Epoch: 12/20, Batch: 800/1586, Loss: 0.9215\n",
      "Epoch: 12/20, Batch: 850/1586, Loss: 1.1559\n",
      "Epoch: 12/20, Batch: 900/1586, Loss: 0.9014\n",
      "Epoch: 12/20, Batch: 950/1586, Loss: 0.8955\n",
      "Epoch: 12/20, Batch: 1000/1586, Loss: 1.1201\n",
      "Epoch: 12/20, Batch: 1050/1586, Loss: 1.0423\n",
      "Epoch: 12/20, Batch: 1100/1586, Loss: 0.9628\n",
      "Epoch: 12/20, Batch: 1150/1586, Loss: 0.8589\n",
      "Epoch: 12/20, Batch: 1200/1586, Loss: 0.7404\n",
      "Epoch: 12/20, Batch: 1250/1586, Loss: 0.9604\n",
      "Epoch: 12/20, Batch: 1300/1586, Loss: 0.9538\n",
      "Epoch: 12/20, Batch: 1350/1586, Loss: 0.8465\n",
      "Epoch: 12/20, Batch: 1400/1586, Loss: 1.0695\n",
      "Epoch: 12/20, Batch: 1450/1586, Loss: 0.8641\n",
      "Epoch: 12/20, Batch: 1500/1586, Loss: 0.8885\n",
      "Epoch: 12/20, Batch: 1550/1586, Loss: 0.9670\n",
      "Epoch: 12, Train Loss: 0.9833\n",
      "Epoch: 13/20, Batch: 0/1586, Loss: 1.2769\n",
      "Epoch: 13/20, Batch: 50/1586, Loss: 0.7343\n",
      "Epoch: 13/20, Batch: 100/1586, Loss: 1.1087\n",
      "Epoch: 13/20, Batch: 150/1586, Loss: 0.9485\n",
      "Epoch: 13/20, Batch: 200/1586, Loss: 0.8781\n",
      "Epoch: 13/20, Batch: 250/1586, Loss: 0.8268\n",
      "Epoch: 13/20, Batch: 300/1586, Loss: 0.9189\n",
      "Epoch: 13/20, Batch: 350/1586, Loss: 0.7348\n",
      "Epoch: 13/20, Batch: 400/1586, Loss: 0.8275\n",
      "Epoch: 13/20, Batch: 450/1586, Loss: 0.8466\n",
      "Epoch: 13/20, Batch: 500/1586, Loss: 1.1022\n",
      "Epoch: 13/20, Batch: 550/1586, Loss: 0.9009\n",
      "Epoch: 13/20, Batch: 600/1586, Loss: 1.0094\n",
      "Epoch: 13/20, Batch: 650/1586, Loss: 1.1347\n",
      "Epoch: 13/20, Batch: 700/1586, Loss: 1.0502\n",
      "Epoch: 13/20, Batch: 750/1586, Loss: 0.8089\n",
      "Epoch: 13/20, Batch: 800/1586, Loss: 0.7031\n",
      "Epoch: 13/20, Batch: 850/1586, Loss: 0.6404\n",
      "Epoch: 13/20, Batch: 900/1586, Loss: 0.9232\n",
      "Epoch: 13/20, Batch: 950/1586, Loss: 0.8615\n",
      "Epoch: 13/20, Batch: 1000/1586, Loss: 1.5707\n",
      "Epoch: 13/20, Batch: 1050/1586, Loss: 1.0235\n",
      "Epoch: 13/20, Batch: 1100/1586, Loss: 0.9106\n",
      "Epoch: 13/20, Batch: 1150/1586, Loss: 1.0063\n",
      "Epoch: 13/20, Batch: 1200/1586, Loss: 0.7098\n",
      "Epoch: 13/20, Batch: 1250/1586, Loss: 0.8864\n",
      "Epoch: 13/20, Batch: 1300/1586, Loss: 1.0484\n",
      "Epoch: 13/20, Batch: 1350/1586, Loss: 1.1391\n",
      "Epoch: 13/20, Batch: 1400/1586, Loss: 0.6902\n",
      "Epoch: 13/20, Batch: 1450/1586, Loss: 0.8543\n",
      "Epoch: 13/20, Batch: 1500/1586, Loss: 1.0759\n",
      "Epoch: 13/20, Batch: 1550/1586, Loss: 0.9632\n",
      "Epoch: 13, Train Loss: 0.9818\n",
      "Epoch: 14/20, Batch: 0/1586, Loss: 1.3048\n",
      "Epoch: 14/20, Batch: 50/1586, Loss: 1.2061\n",
      "Epoch: 14/20, Batch: 100/1586, Loss: 1.0759\n",
      "Epoch: 14/20, Batch: 150/1586, Loss: 0.8416\n",
      "Epoch: 14/20, Batch: 200/1586, Loss: 0.8928\n",
      "Epoch: 14/20, Batch: 250/1586, Loss: 0.9060\n",
      "Epoch: 14/20, Batch: 300/1586, Loss: 0.9678\n",
      "Epoch: 14/20, Batch: 350/1586, Loss: 0.8921\n",
      "Epoch: 14/20, Batch: 400/1586, Loss: 0.8675\n",
      "Epoch: 14/20, Batch: 450/1586, Loss: 1.1104\n",
      "Epoch: 14/20, Batch: 500/1586, Loss: 0.7329\n",
      "Epoch: 14/20, Batch: 550/1586, Loss: 0.8251\n",
      "Epoch: 14/20, Batch: 600/1586, Loss: 1.1605\n",
      "Epoch: 14/20, Batch: 650/1586, Loss: 0.8764\n",
      "Epoch: 14/20, Batch: 700/1586, Loss: 0.8283\n",
      "Epoch: 14/20, Batch: 750/1586, Loss: 0.7668\n",
      "Epoch: 14/20, Batch: 800/1586, Loss: 0.8287\n",
      "Epoch: 14/20, Batch: 850/1586, Loss: 1.1976\n",
      "Epoch: 14/20, Batch: 900/1586, Loss: 0.8685\n",
      "Epoch: 14/20, Batch: 950/1586, Loss: 1.0524\n",
      "Epoch: 14/20, Batch: 1000/1586, Loss: 1.0711\n",
      "Epoch: 14/20, Batch: 1050/1586, Loss: 1.0775\n",
      "Epoch: 14/20, Batch: 1100/1586, Loss: 0.6930\n",
      "Epoch: 14/20, Batch: 1150/1586, Loss: 1.2525\n",
      "Epoch: 14/20, Batch: 1200/1586, Loss: 0.8186\n",
      "Epoch: 14/20, Batch: 1250/1586, Loss: 1.2620\n",
      "Epoch: 14/20, Batch: 1300/1586, Loss: 1.1497\n",
      "Epoch: 14/20, Batch: 1350/1586, Loss: 1.1276\n",
      "Epoch: 14/20, Batch: 1400/1586, Loss: 1.2416\n",
      "Epoch: 14/20, Batch: 1450/1586, Loss: 0.9188\n",
      "Epoch: 14/20, Batch: 1500/1586, Loss: 1.0156\n",
      "Epoch: 14/20, Batch: 1550/1586, Loss: 0.7699\n",
      "Epoch: 14, Train Loss: 0.9820\n",
      "Epoch: 15/20, Batch: 0/1586, Loss: 1.1166\n",
      "Epoch: 15/20, Batch: 50/1586, Loss: 1.0009\n",
      "Epoch: 15/20, Batch: 100/1586, Loss: 1.1546\n",
      "Epoch: 15/20, Batch: 150/1586, Loss: 0.8218\n",
      "Epoch: 15/20, Batch: 200/1586, Loss: 1.1053\n",
      "Epoch: 15/20, Batch: 250/1586, Loss: 0.7567\n",
      "Epoch: 15/20, Batch: 300/1586, Loss: 1.1009\n",
      "Epoch: 15/20, Batch: 350/1586, Loss: 0.8505\n",
      "Epoch: 15/20, Batch: 400/1586, Loss: 1.3550\n",
      "Epoch: 15/20, Batch: 450/1586, Loss: 1.3380\n",
      "Epoch: 15/20, Batch: 500/1586, Loss: 0.8053\n",
      "Epoch: 15/20, Batch: 550/1586, Loss: 0.7569\n",
      "Epoch: 15/20, Batch: 600/1586, Loss: 1.0622\n",
      "Epoch: 15/20, Batch: 650/1586, Loss: 1.0022\n",
      "Epoch: 15/20, Batch: 700/1586, Loss: 0.9997\n",
      "Epoch: 15/20, Batch: 750/1586, Loss: 1.0176\n",
      "Epoch: 15/20, Batch: 800/1586, Loss: 1.0792\n",
      "Epoch: 15/20, Batch: 850/1586, Loss: 1.0403\n",
      "Epoch: 15/20, Batch: 900/1586, Loss: 0.9152\n",
      "Epoch: 15/20, Batch: 950/1586, Loss: 0.6732\n",
      "Epoch: 15/20, Batch: 1000/1586, Loss: 0.9209\n",
      "Epoch: 15/20, Batch: 1050/1586, Loss: 0.8150\n",
      "Epoch: 15/20, Batch: 1100/1586, Loss: 0.8433\n",
      "Epoch: 15/20, Batch: 1150/1586, Loss: 0.8804\n",
      "Epoch: 15/20, Batch: 1200/1586, Loss: 0.7910\n",
      "Epoch: 15/20, Batch: 1250/1586, Loss: 0.9422\n",
      "Epoch: 15/20, Batch: 1300/1586, Loss: 0.8844\n",
      "Epoch: 15/20, Batch: 1350/1586, Loss: 0.6795\n",
      "Epoch: 15/20, Batch: 1400/1586, Loss: 1.0243\n",
      "Epoch: 15/20, Batch: 1450/1586, Loss: 0.9019\n",
      "Epoch: 15/20, Batch: 1500/1586, Loss: 0.8270\n",
      "Epoch: 15/20, Batch: 1550/1586, Loss: 1.0626\n",
      "Epoch: 15, Train Loss: 0.9817\n",
      "Epoch: 16/20, Batch: 0/1586, Loss: 0.8874\n",
      "Epoch: 16/20, Batch: 50/1586, Loss: 0.9538\n",
      "Epoch: 16/20, Batch: 100/1586, Loss: 1.2300\n",
      "Epoch: 16/20, Batch: 150/1586, Loss: 0.8634\n",
      "Epoch: 16/20, Batch: 200/1586, Loss: 0.9260\n",
      "Epoch: 16/20, Batch: 250/1586, Loss: 1.0352\n",
      "Epoch: 16/20, Batch: 300/1586, Loss: 1.1162\n",
      "Epoch: 16/20, Batch: 350/1586, Loss: 0.7462\n",
      "Epoch: 16/20, Batch: 400/1586, Loss: 1.3039\n",
      "Epoch: 16/20, Batch: 450/1586, Loss: 0.6519\n",
      "Epoch: 16/20, Batch: 500/1586, Loss: 1.3480\n",
      "Epoch: 16/20, Batch: 550/1586, Loss: 0.9316\n",
      "Epoch: 16/20, Batch: 600/1586, Loss: 0.9912\n",
      "Epoch: 16/20, Batch: 650/1586, Loss: 1.1543\n",
      "Epoch: 16/20, Batch: 700/1586, Loss: 1.0520\n",
      "Epoch: 16/20, Batch: 750/1586, Loss: 1.0096\n",
      "Epoch: 16/20, Batch: 800/1586, Loss: 0.8264\n",
      "Epoch: 16/20, Batch: 850/1586, Loss: 0.8805\n",
      "Epoch: 16/20, Batch: 900/1586, Loss: 0.7160\n",
      "Epoch: 16/20, Batch: 950/1586, Loss: 1.0193\n",
      "Epoch: 16/20, Batch: 1000/1586, Loss: 1.0966\n",
      "Epoch: 16/20, Batch: 1050/1586, Loss: 0.8971\n",
      "Epoch: 16/20, Batch: 1100/1586, Loss: 0.8366\n",
      "Epoch: 16/20, Batch: 1150/1586, Loss: 0.9542\n",
      "Epoch: 16/20, Batch: 1200/1586, Loss: 1.0912\n",
      "Epoch: 16/20, Batch: 1250/1586, Loss: 0.8750\n",
      "Epoch: 16/20, Batch: 1300/1586, Loss: 0.8653\n",
      "Epoch: 16/20, Batch: 1350/1586, Loss: 0.9400\n",
      "Epoch: 16/20, Batch: 1400/1586, Loss: 1.0037\n",
      "Epoch: 16/20, Batch: 1450/1586, Loss: 0.7416\n",
      "Epoch: 16/20, Batch: 1500/1586, Loss: 0.8545\n",
      "Epoch: 16/20, Batch: 1550/1586, Loss: 0.8313\n",
      "Epoch: 16, Train Loss: 0.9812\n",
      "Epoch: 17/20, Batch: 0/1586, Loss: 1.0687\n",
      "Epoch: 17/20, Batch: 50/1586, Loss: 1.0415\n",
      "Epoch: 17/20, Batch: 100/1586, Loss: 0.9551\n",
      "Epoch: 17/20, Batch: 150/1586, Loss: 1.2002\n",
      "Epoch: 17/20, Batch: 200/1586, Loss: 0.9257\n",
      "Epoch: 17/20, Batch: 250/1586, Loss: 0.8660\n",
      "Epoch: 17/20, Batch: 300/1586, Loss: 1.0616\n",
      "Epoch: 17/20, Batch: 350/1586, Loss: 0.8660\n",
      "Epoch: 17/20, Batch: 400/1586, Loss: 0.6787\n",
      "Epoch: 17/20, Batch: 450/1586, Loss: 0.8440\n",
      "Epoch: 17/20, Batch: 500/1586, Loss: 0.7729\n",
      "Epoch: 17/20, Batch: 550/1586, Loss: 0.8907\n",
      "Epoch: 17/20, Batch: 600/1586, Loss: 1.0753\n",
      "Epoch: 17/20, Batch: 650/1586, Loss: 0.7889\n",
      "Epoch: 17/20, Batch: 700/1586, Loss: 1.0161\n",
      "Epoch: 17/20, Batch: 750/1586, Loss: 0.9419\n",
      "Epoch: 17/20, Batch: 800/1586, Loss: 0.9405\n",
      "Epoch: 17/20, Batch: 850/1586, Loss: 0.7691\n",
      "Epoch: 17/20, Batch: 900/1586, Loss: 1.2520\n",
      "Epoch: 17/20, Batch: 950/1586, Loss: 0.9902\n",
      "Epoch: 17/20, Batch: 1000/1586, Loss: 0.8172\n",
      "Epoch: 17/20, Batch: 1050/1586, Loss: 1.1817\n",
      "Epoch: 17/20, Batch: 1100/1586, Loss: 0.8599\n",
      "Epoch: 17/20, Batch: 1150/1586, Loss: 1.2839\n",
      "Epoch: 17/20, Batch: 1200/1586, Loss: 1.1489\n",
      "Epoch: 17/20, Batch: 1250/1586, Loss: 1.3446\n",
      "Epoch: 17/20, Batch: 1300/1586, Loss: 0.9212\n",
      "Epoch: 17/20, Batch: 1350/1586, Loss: 0.8596\n",
      "Epoch: 17/20, Batch: 1400/1586, Loss: 0.9996\n",
      "Epoch: 17/20, Batch: 1450/1586, Loss: 1.2053\n",
      "Epoch: 17/20, Batch: 1500/1586, Loss: 1.2593\n",
      "Epoch: 17/20, Batch: 1550/1586, Loss: 1.0601\n",
      "Epoch: 17, Train Loss: 0.9803\n",
      "Epoch: 18/20, Batch: 0/1586, Loss: 0.9038\n",
      "Epoch: 18/20, Batch: 50/1586, Loss: 1.3629\n",
      "Epoch: 18/20, Batch: 100/1586, Loss: 1.0224\n",
      "Epoch: 18/20, Batch: 150/1586, Loss: 0.9457\n",
      "Epoch: 18/20, Batch: 200/1586, Loss: 0.9478\n",
      "Epoch: 18/20, Batch: 250/1586, Loss: 0.9573\n",
      "Epoch: 18/20, Batch: 300/1586, Loss: 0.8253\n",
      "Epoch: 18/20, Batch: 350/1586, Loss: 0.8398\n",
      "Epoch: 18/20, Batch: 400/1586, Loss: 0.7491\n",
      "Epoch: 18/20, Batch: 450/1586, Loss: 0.9368\n",
      "Epoch: 18/20, Batch: 500/1586, Loss: 0.7350\n",
      "Epoch: 18/20, Batch: 550/1586, Loss: 1.0538\n",
      "Epoch: 18/20, Batch: 600/1586, Loss: 1.2949\n",
      "Epoch: 18/20, Batch: 650/1586, Loss: 1.1096\n",
      "Epoch: 18/20, Batch: 700/1586, Loss: 1.0518\n",
      "Epoch: 18/20, Batch: 750/1586, Loss: 0.8584\n",
      "Epoch: 18/20, Batch: 800/1586, Loss: 1.0441\n",
      "Epoch: 18/20, Batch: 850/1586, Loss: 1.5270\n",
      "Epoch: 18/20, Batch: 900/1586, Loss: 0.9061\n",
      "Epoch: 18/20, Batch: 950/1586, Loss: 1.4703\n",
      "Epoch: 18/20, Batch: 1000/1586, Loss: 0.9523\n",
      "Epoch: 18/20, Batch: 1050/1586, Loss: 1.1200\n",
      "Epoch: 18/20, Batch: 1100/1586, Loss: 0.8331\n",
      "Epoch: 18/20, Batch: 1150/1586, Loss: 1.7017\n",
      "Epoch: 18/20, Batch: 1200/1586, Loss: 0.9964\n",
      "Epoch: 18/20, Batch: 1250/1586, Loss: 1.1780\n",
      "Epoch: 18/20, Batch: 1300/1586, Loss: 1.7042\n",
      "Epoch: 18/20, Batch: 1350/1586, Loss: 1.0384\n",
      "Epoch: 18/20, Batch: 1400/1586, Loss: 1.0005\n",
      "Epoch: 18/20, Batch: 1450/1586, Loss: 0.9809\n",
      "Epoch: 18/20, Batch: 1500/1586, Loss: 1.6547\n",
      "Epoch: 18/20, Batch: 1550/1586, Loss: 0.8913\n",
      "Epoch: 18, Train Loss: 0.9785\n",
      "Epoch: 19/20, Batch: 0/1586, Loss: 0.9497\n",
      "Epoch: 19/20, Batch: 50/1586, Loss: 1.1705\n",
      "Epoch: 19/20, Batch: 100/1586, Loss: 1.0559\n",
      "Epoch: 19/20, Batch: 150/1586, Loss: 0.8201\n",
      "Epoch: 19/20, Batch: 200/1586, Loss: 0.8918\n",
      "Epoch: 19/20, Batch: 250/1586, Loss: 0.9356\n",
      "Epoch: 19/20, Batch: 300/1586, Loss: 1.0562\n",
      "Epoch: 19/20, Batch: 350/1586, Loss: 0.9117\n",
      "Epoch: 19/20, Batch: 400/1586, Loss: 1.0880\n",
      "Epoch: 19/20, Batch: 450/1586, Loss: 0.9183\n",
      "Epoch: 19/20, Batch: 500/1586, Loss: 0.7664\n",
      "Epoch: 19/20, Batch: 550/1586, Loss: 0.7888\n",
      "Epoch: 19/20, Batch: 600/1586, Loss: 1.2054\n",
      "Epoch: 19/20, Batch: 650/1586, Loss: 0.9178\n",
      "Epoch: 19/20, Batch: 700/1586, Loss: 0.8289\n",
      "Epoch: 19/20, Batch: 750/1586, Loss: 0.7584\n",
      "Epoch: 19/20, Batch: 800/1586, Loss: 0.7958\n",
      "Epoch: 19/20, Batch: 850/1586, Loss: 1.2763\n",
      "Epoch: 19/20, Batch: 900/1586, Loss: 0.8640\n",
      "Epoch: 19/20, Batch: 950/1586, Loss: 1.0441\n",
      "Epoch: 19/20, Batch: 1000/1586, Loss: 0.8338\n",
      "Epoch: 19/20, Batch: 1050/1586, Loss: 0.8413\n",
      "Epoch: 19/20, Batch: 1100/1586, Loss: 1.1737\n",
      "Epoch: 19/20, Batch: 1150/1586, Loss: 1.0394\n",
      "Epoch: 19/20, Batch: 1200/1586, Loss: 0.8086\n",
      "Epoch: 19/20, Batch: 1250/1586, Loss: 0.9341\n",
      "Epoch: 19/20, Batch: 1300/1586, Loss: 0.7355\n",
      "Epoch: 19/20, Batch: 1350/1586, Loss: 0.7074\n",
      "Epoch: 19/20, Batch: 1400/1586, Loss: 1.0638\n",
      "Epoch: 19/20, Batch: 1450/1586, Loss: 1.1703\n",
      "Epoch: 19/20, Batch: 1500/1586, Loss: 0.9805\n",
      "Epoch: 19/20, Batch: 1550/1586, Loss: 0.7762\n",
      "Epoch: 19, Train Loss: 0.9797\n",
      "Epoch: 20/20, Batch: 0/1586, Loss: 1.3433\n",
      "Epoch: 20/20, Batch: 50/1586, Loss: 0.7873\n",
      "Epoch: 20/20, Batch: 100/1586, Loss: 1.0819\n",
      "Epoch: 20/20, Batch: 150/1586, Loss: 1.0141\n",
      "Epoch: 20/20, Batch: 200/1586, Loss: 1.0244\n",
      "Epoch: 20/20, Batch: 250/1586, Loss: 0.7344\n",
      "Epoch: 20/20, Batch: 300/1586, Loss: 0.8169\n",
      "Epoch: 20/20, Batch: 350/1586, Loss: 0.9709\n",
      "Epoch: 20/20, Batch: 400/1586, Loss: 1.0093\n",
      "Epoch: 20/20, Batch: 450/1586, Loss: 1.0295\n",
      "Epoch: 20/20, Batch: 500/1586, Loss: 1.0593\n",
      "Epoch: 20/20, Batch: 550/1586, Loss: 1.0187\n",
      "Epoch: 20/20, Batch: 600/1586, Loss: 1.7417\n",
      "Epoch: 20/20, Batch: 650/1586, Loss: 0.9046\n",
      "Epoch: 20/20, Batch: 700/1586, Loss: 0.7878\n",
      "Epoch: 20/20, Batch: 750/1586, Loss: 0.7803\n",
      "Epoch: 20/20, Batch: 800/1586, Loss: 0.6984\n",
      "Epoch: 20/20, Batch: 850/1586, Loss: 0.8624\n",
      "Epoch: 20/20, Batch: 900/1586, Loss: 0.8554\n",
      "Epoch: 20/20, Batch: 950/1586, Loss: 0.7184\n",
      "Epoch: 20/20, Batch: 1000/1586, Loss: 0.9047\n",
      "Epoch: 20/20, Batch: 1050/1586, Loss: 0.9332\n",
      "Epoch: 20/20, Batch: 1100/1586, Loss: 0.8344\n",
      "Epoch: 20/20, Batch: 1150/1586, Loss: 1.1064\n",
      "Epoch: 20/20, Batch: 1200/1586, Loss: 1.0556\n",
      "Epoch: 20/20, Batch: 1250/1586, Loss: 1.1158\n",
      "Epoch: 20/20, Batch: 1300/1586, Loss: 0.8968\n",
      "Epoch: 20/20, Batch: 1350/1586, Loss: 0.9469\n",
      "Epoch: 20/20, Batch: 1400/1586, Loss: 1.1129\n",
      "Epoch: 20/20, Batch: 1450/1586, Loss: 1.1003\n",
      "Epoch: 20/20, Batch: 1500/1586, Loss: 0.7089\n",
      "Epoch: 20/20, Batch: 1550/1586, Loss: 0.6665\n",
      "Epoch: 20, Train Loss: 0.9797\n",
      "Training finished!\n"
     ]
    }
   ],
   "source": [
    "input_features = X_train_pca.shape[1]  \n",
    "\n",
    "weighted_model = HeartDiseaseMLPClassifier(input_size=input_features, class_frequencies=class_frequencies).to(device)\n",
    "\n",
    "optimizer = optim.Adam(weighted_model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)\n",
    "\n",
    "num_epochs = 20\n",
    "weighted_train_losses = []\n",
    "\n",
    "print(\"\\nStarting training...\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    weighted_model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = weighted_model(data)\n",
    "    \n",
    "        target = target.float()\n",
    "        output = output.float()    \n",
    "        loss = weighted_model.get_weighted_loss(output, target)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if batch_idx % 50 == 0:\n",
    "            print(f'Epoch: {epoch+1}/{num_epochs}, Batch: {batch_idx}/{len(train_loader)}, Loss: {loss.item():.4f}')\n",
    "    \n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "    weighted_train_losses.append(avg_train_loss)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1}, Train Loss: {avg_train_loss:.4f}')\n",
    "\n",
    "print('Training finished!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report (Weighted):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.72      0.83     45957\n",
      "         1.0       0.23      0.82      0.36      4779\n",
      "\n",
      "    accuracy                           0.73     50736\n",
      "   macro avg       0.60      0.77      0.60     50736\n",
      "weighted avg       0.90      0.73      0.78     50736\n",
      "\n",
      "\n",
      "Confusion Matrix (Weighted):\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "weighted_model.eval()\n",
    "y_true_weighted = []\n",
    "y_pred_weighted = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        output = weighted_model(data)\n",
    "        \n",
    "        y_true_weighted.extend(target.cpu().numpy())\n",
    "        y_pred_weighted.extend((output > 0.5).cpu().numpy())\n",
    "\n",
    "y_true_weighted = np.array(y_true_weighted)\n",
    "y_pred_weighted = np.array(y_pred_weighted)\n",
    "\n",
    "print(\"\\nClassification Report (Weighted):\")\n",
    "print(classification_report(y_true_weighted, y_pred_weighted))\n",
    "print(\"\\nConfusion Matrix (Weighted):\")\n",
    "cm_weighted = confusion_matrix(y_true_weighted, y_pred_weighted)\n",
    "sns.heatmap(cm_weighted, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix (Weighted)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train:test = 7:3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training...\n",
      "Epoch: 1/20, Batch: 0/1388, Loss: 1.6448\n",
      "Epoch: 1/20, Batch: 50/1388, Loss: 0.8332\n",
      "Epoch: 1/20, Batch: 100/1388, Loss: 0.9874\n",
      "Epoch: 1/20, Batch: 150/1388, Loss: 0.9608\n",
      "Epoch: 1/20, Batch: 200/1388, Loss: 1.2987\n",
      "Epoch: 1/20, Batch: 250/1388, Loss: 1.1182\n",
      "Epoch: 1/20, Batch: 300/1388, Loss: 1.3456\n",
      "Epoch: 1/20, Batch: 350/1388, Loss: 1.1925\n",
      "Epoch: 1/20, Batch: 400/1388, Loss: 0.7892\n",
      "Epoch: 1/20, Batch: 450/1388, Loss: 0.9442\n",
      "Epoch: 1/20, Batch: 500/1388, Loss: 1.3185\n",
      "Epoch: 1/20, Batch: 550/1388, Loss: 0.8292\n",
      "Epoch: 1/20, Batch: 600/1388, Loss: 1.2458\n",
      "Epoch: 1/20, Batch: 650/1388, Loss: 1.1959\n",
      "Epoch: 1/20, Batch: 700/1388, Loss: 1.1307\n",
      "Epoch: 1/20, Batch: 750/1388, Loss: 1.2695\n",
      "Epoch: 1/20, Batch: 800/1388, Loss: 0.8949\n",
      "Epoch: 1/20, Batch: 850/1388, Loss: 0.9881\n",
      "Epoch: 1/20, Batch: 900/1388, Loss: 0.8534\n",
      "Epoch: 1/20, Batch: 950/1388, Loss: 1.3633\n",
      "Epoch: 1/20, Batch: 1000/1388, Loss: 0.8087\n",
      "Epoch: 1/20, Batch: 1050/1388, Loss: 0.9534\n",
      "Epoch: 1/20, Batch: 1100/1388, Loss: 1.1307\n",
      "Epoch: 1/20, Batch: 1150/1388, Loss: 0.7220\n",
      "Epoch: 1/20, Batch: 1200/1388, Loss: 1.3020\n",
      "Epoch: 1/20, Batch: 1250/1388, Loss: 0.9793\n",
      "Epoch: 1/20, Batch: 1300/1388, Loss: 1.4013\n",
      "Epoch: 1/20, Batch: 1350/1388, Loss: 0.9700\n",
      "Epoch: 1, Train Loss: 1.0159\n",
      "Epoch: 2/20, Batch: 0/1388, Loss: 1.0733\n",
      "Epoch: 2/20, Batch: 50/1388, Loss: 0.8782\n",
      "Epoch: 2/20, Batch: 100/1388, Loss: 0.8378\n",
      "Epoch: 2/20, Batch: 150/1388, Loss: 0.6827\n",
      "Epoch: 2/20, Batch: 200/1388, Loss: 1.0116\n",
      "Epoch: 2/20, Batch: 250/1388, Loss: 0.7568\n",
      "Epoch: 2/20, Batch: 300/1388, Loss: 0.9335\n",
      "Epoch: 2/20, Batch: 350/1388, Loss: 1.1097\n",
      "Epoch: 2/20, Batch: 400/1388, Loss: 0.8505\n",
      "Epoch: 2/20, Batch: 450/1388, Loss: 0.8455\n",
      "Epoch: 2/20, Batch: 500/1388, Loss: 1.4043\n",
      "Epoch: 2/20, Batch: 550/1388, Loss: 1.0000\n",
      "Epoch: 2/20, Batch: 600/1388, Loss: 1.0293\n",
      "Epoch: 2/20, Batch: 650/1388, Loss: 1.4089\n",
      "Epoch: 2/20, Batch: 700/1388, Loss: 0.8153\n",
      "Epoch: 2/20, Batch: 750/1388, Loss: 1.1308\n",
      "Epoch: 2/20, Batch: 800/1388, Loss: 0.7732\n",
      "Epoch: 2/20, Batch: 850/1388, Loss: 0.9637\n",
      "Epoch: 2/20, Batch: 900/1388, Loss: 1.2494\n",
      "Epoch: 2/20, Batch: 950/1388, Loss: 1.3393\n",
      "Epoch: 2/20, Batch: 1000/1388, Loss: 1.1701\n",
      "Epoch: 2/20, Batch: 1050/1388, Loss: 1.0306\n",
      "Epoch: 2/20, Batch: 1100/1388, Loss: 1.2637\n",
      "Epoch: 2/20, Batch: 1150/1388, Loss: 1.0329\n",
      "Epoch: 2/20, Batch: 1200/1388, Loss: 0.9974\n",
      "Epoch: 2/20, Batch: 1250/1388, Loss: 1.2639\n",
      "Epoch: 2/20, Batch: 1300/1388, Loss: 0.8841\n",
      "Epoch: 2/20, Batch: 1350/1388, Loss: 0.9361\n",
      "Epoch: 2, Train Loss: 0.9967\n",
      "Epoch: 3/20, Batch: 0/1388, Loss: 0.8955\n",
      "Epoch: 3/20, Batch: 50/1388, Loss: 0.9907\n",
      "Epoch: 3/20, Batch: 100/1388, Loss: 0.9804\n",
      "Epoch: 3/20, Batch: 150/1388, Loss: 0.7999\n",
      "Epoch: 3/20, Batch: 200/1388, Loss: 1.0650\n",
      "Epoch: 3/20, Batch: 250/1388, Loss: 0.9969\n",
      "Epoch: 3/20, Batch: 300/1388, Loss: 0.8092\n",
      "Epoch: 3/20, Batch: 350/1388, Loss: 1.2870\n",
      "Epoch: 3/20, Batch: 400/1388, Loss: 1.0405\n",
      "Epoch: 3/20, Batch: 450/1388, Loss: 1.2334\n",
      "Epoch: 3/20, Batch: 500/1388, Loss: 1.2836\n",
      "Epoch: 3/20, Batch: 550/1388, Loss: 0.9704\n",
      "Epoch: 3/20, Batch: 600/1388, Loss: 0.9759\n",
      "Epoch: 3/20, Batch: 650/1388, Loss: 1.0753\n",
      "Epoch: 3/20, Batch: 700/1388, Loss: 0.9779\n",
      "Epoch: 3/20, Batch: 750/1388, Loss: 1.1036\n",
      "Epoch: 3/20, Batch: 800/1388, Loss: 0.9557\n",
      "Epoch: 3/20, Batch: 850/1388, Loss: 1.1618\n",
      "Epoch: 3/20, Batch: 900/1388, Loss: 1.2783\n",
      "Epoch: 3/20, Batch: 950/1388, Loss: 1.3340\n",
      "Epoch: 3/20, Batch: 1000/1388, Loss: 1.0844\n",
      "Epoch: 3/20, Batch: 1050/1388, Loss: 0.9655\n",
      "Epoch: 3/20, Batch: 1100/1388, Loss: 0.9766\n",
      "Epoch: 3/20, Batch: 1150/1388, Loss: 1.0154\n",
      "Epoch: 3/20, Batch: 1200/1388, Loss: 0.8817\n",
      "Epoch: 3/20, Batch: 1250/1388, Loss: 1.1692\n",
      "Epoch: 3/20, Batch: 1300/1388, Loss: 1.0416\n",
      "Epoch: 3/20, Batch: 1350/1388, Loss: 1.0064\n",
      "Epoch: 3, Train Loss: 0.9915\n",
      "Epoch: 4/20, Batch: 0/1388, Loss: 1.1918\n",
      "Epoch: 4/20, Batch: 50/1388, Loss: 0.7755\n",
      "Epoch: 4/20, Batch: 100/1388, Loss: 1.7315\n",
      "Epoch: 4/20, Batch: 150/1388, Loss: 0.8879\n",
      "Epoch: 4/20, Batch: 200/1388, Loss: 0.8371\n",
      "Epoch: 4/20, Batch: 250/1388, Loss: 0.8660\n",
      "Epoch: 4/20, Batch: 300/1388, Loss: 0.9515\n",
      "Epoch: 4/20, Batch: 350/1388, Loss: 0.9777\n",
      "Epoch: 4/20, Batch: 400/1388, Loss: 1.6219\n",
      "Epoch: 4/20, Batch: 450/1388, Loss: 0.8073\n",
      "Epoch: 4/20, Batch: 500/1388, Loss: 0.8690\n",
      "Epoch: 4/20, Batch: 550/1388, Loss: 1.3859\n",
      "Epoch: 4/20, Batch: 600/1388, Loss: 1.1070\n",
      "Epoch: 4/20, Batch: 650/1388, Loss: 0.9718\n",
      "Epoch: 4/20, Batch: 700/1388, Loss: 0.9173\n",
      "Epoch: 4/20, Batch: 750/1388, Loss: 0.8288\n",
      "Epoch: 4/20, Batch: 800/1388, Loss: 1.0267\n",
      "Epoch: 4/20, Batch: 850/1388, Loss: 0.8321\n",
      "Epoch: 4/20, Batch: 900/1388, Loss: 0.7083\n",
      "Epoch: 4/20, Batch: 950/1388, Loss: 0.8945\n",
      "Epoch: 4/20, Batch: 1000/1388, Loss: 1.0141\n",
      "Epoch: 4/20, Batch: 1050/1388, Loss: 0.9046\n",
      "Epoch: 4/20, Batch: 1100/1388, Loss: 0.7776\n",
      "Epoch: 4/20, Batch: 1150/1388, Loss: 0.8515\n",
      "Epoch: 4/20, Batch: 1200/1388, Loss: 0.7883\n",
      "Epoch: 4/20, Batch: 1250/1388, Loss: 0.9160\n",
      "Epoch: 4/20, Batch: 1300/1388, Loss: 0.9060\n",
      "Epoch: 4/20, Batch: 1350/1388, Loss: 1.0213\n",
      "Epoch: 4, Train Loss: 0.9879\n",
      "Epoch: 5/20, Batch: 0/1388, Loss: 1.2317\n",
      "Epoch: 5/20, Batch: 50/1388, Loss: 1.0301\n",
      "Epoch: 5/20, Batch: 100/1388, Loss: 1.2128\n",
      "Epoch: 5/20, Batch: 150/1388, Loss: 1.0154\n",
      "Epoch: 5/20, Batch: 200/1388, Loss: 1.1689\n",
      "Epoch: 5/20, Batch: 250/1388, Loss: 0.8983\n",
      "Epoch: 5/20, Batch: 300/1388, Loss: 0.7528\n",
      "Epoch: 5/20, Batch: 350/1388, Loss: 0.7910\n",
      "Epoch: 5/20, Batch: 400/1388, Loss: 1.0279\n",
      "Epoch: 5/20, Batch: 450/1388, Loss: 1.0560\n",
      "Epoch: 5/20, Batch: 500/1388, Loss: 0.8023\n",
      "Epoch: 5/20, Batch: 550/1388, Loss: 0.7577\n",
      "Epoch: 5/20, Batch: 600/1388, Loss: 0.7796\n",
      "Epoch: 5/20, Batch: 650/1388, Loss: 1.0396\n",
      "Epoch: 5/20, Batch: 700/1388, Loss: 0.7871\n",
      "Epoch: 5/20, Batch: 750/1388, Loss: 0.8948\n",
      "Epoch: 5/20, Batch: 800/1388, Loss: 0.7045\n",
      "Epoch: 5/20, Batch: 850/1388, Loss: 1.2226\n",
      "Epoch: 5/20, Batch: 900/1388, Loss: 0.8005\n",
      "Epoch: 5/20, Batch: 950/1388, Loss: 1.2385\n",
      "Epoch: 5/20, Batch: 1000/1388, Loss: 0.7529\n",
      "Epoch: 5/20, Batch: 1050/1388, Loss: 1.0896\n",
      "Epoch: 5/20, Batch: 1100/1388, Loss: 0.9355\n",
      "Epoch: 5/20, Batch: 1150/1388, Loss: 0.8213\n",
      "Epoch: 5/20, Batch: 1200/1388, Loss: 0.8559\n",
      "Epoch: 5/20, Batch: 1250/1388, Loss: 1.0083\n",
      "Epoch: 5/20, Batch: 1300/1388, Loss: 1.6427\n",
      "Epoch: 5/20, Batch: 1350/1388, Loss: 0.7435\n",
      "Epoch: 5, Train Loss: 0.9902\n",
      "Epoch: 6/20, Batch: 0/1388, Loss: 0.8763\n",
      "Epoch: 6/20, Batch: 50/1388, Loss: 1.2690\n",
      "Epoch: 6/20, Batch: 100/1388, Loss: 0.7873\n",
      "Epoch: 6/20, Batch: 150/1388, Loss: 1.3365\n",
      "Epoch: 6/20, Batch: 200/1388, Loss: 1.0406\n",
      "Epoch: 6/20, Batch: 250/1388, Loss: 1.1068\n",
      "Epoch: 6/20, Batch: 300/1388, Loss: 0.8790\n",
      "Epoch: 6/20, Batch: 350/1388, Loss: 1.2886\n",
      "Epoch: 6/20, Batch: 400/1388, Loss: 1.3070\n",
      "Epoch: 6/20, Batch: 450/1388, Loss: 0.9750\n",
      "Epoch: 6/20, Batch: 500/1388, Loss: 0.9052\n",
      "Epoch: 6/20, Batch: 550/1388, Loss: 1.1063\n",
      "Epoch: 6/20, Batch: 600/1388, Loss: 0.9400\n",
      "Epoch: 6/20, Batch: 650/1388, Loss: 0.9322\n",
      "Epoch: 6/20, Batch: 700/1388, Loss: 0.9518\n",
      "Epoch: 6/20, Batch: 750/1388, Loss: 0.7986\n",
      "Epoch: 6/20, Batch: 800/1388, Loss: 0.8503\n",
      "Epoch: 6/20, Batch: 850/1388, Loss: 1.1240\n",
      "Epoch: 6/20, Batch: 900/1388, Loss: 0.8282\n",
      "Epoch: 6/20, Batch: 950/1388, Loss: 1.3438\n",
      "Epoch: 6/20, Batch: 1000/1388, Loss: 1.4729\n",
      "Epoch: 6/20, Batch: 1050/1388, Loss: 0.9999\n",
      "Epoch: 6/20, Batch: 1100/1388, Loss: 1.0748\n",
      "Epoch: 6/20, Batch: 1150/1388, Loss: 1.0890\n",
      "Epoch: 6/20, Batch: 1200/1388, Loss: 1.0909\n",
      "Epoch: 6/20, Batch: 1250/1388, Loss: 0.9941\n",
      "Epoch: 6/20, Batch: 1300/1388, Loss: 0.8068\n",
      "Epoch: 6/20, Batch: 1350/1388, Loss: 0.8830\n",
      "Epoch: 6, Train Loss: 0.9867\n",
      "Epoch: 7/20, Batch: 0/1388, Loss: 1.0766\n",
      "Epoch: 7/20, Batch: 50/1388, Loss: 1.2789\n",
      "Epoch: 7/20, Batch: 100/1388, Loss: 0.7283\n",
      "Epoch: 7/20, Batch: 150/1388, Loss: 0.9985\n",
      "Epoch: 7/20, Batch: 200/1388, Loss: 1.3247\n",
      "Epoch: 7/20, Batch: 250/1388, Loss: 1.0423\n",
      "Epoch: 7/20, Batch: 300/1388, Loss: 0.8462\n",
      "Epoch: 7/20, Batch: 350/1388, Loss: 1.0229\n",
      "Epoch: 7/20, Batch: 400/1388, Loss: 1.0316\n",
      "Epoch: 7/20, Batch: 450/1388, Loss: 0.8880\n",
      "Epoch: 7/20, Batch: 500/1388, Loss: 1.0237\n",
      "Epoch: 7/20, Batch: 550/1388, Loss: 0.8972\n",
      "Epoch: 7/20, Batch: 600/1388, Loss: 1.4300\n",
      "Epoch: 7/20, Batch: 650/1388, Loss: 0.8471\n",
      "Epoch: 7/20, Batch: 700/1388, Loss: 0.9315\n",
      "Epoch: 7/20, Batch: 750/1388, Loss: 0.8184\n",
      "Epoch: 7/20, Batch: 800/1388, Loss: 0.7767\n",
      "Epoch: 7/20, Batch: 850/1388, Loss: 0.9521\n",
      "Epoch: 7/20, Batch: 900/1388, Loss: 0.9455\n",
      "Epoch: 7/20, Batch: 950/1388, Loss: 1.1832\n",
      "Epoch: 7/20, Batch: 1000/1388, Loss: 0.7434\n",
      "Epoch: 7/20, Batch: 1050/1388, Loss: 1.3952\n",
      "Epoch: 7/20, Batch: 1100/1388, Loss: 0.6971\n",
      "Epoch: 7/20, Batch: 1150/1388, Loss: 0.8676\n",
      "Epoch: 7/20, Batch: 1200/1388, Loss: 1.2316\n",
      "Epoch: 7/20, Batch: 1250/1388, Loss: 0.8342\n",
      "Epoch: 7/20, Batch: 1300/1388, Loss: 0.9760\n",
      "Epoch: 7/20, Batch: 1350/1388, Loss: 0.8481\n",
      "Epoch: 7, Train Loss: 0.9850\n",
      "Epoch: 8/20, Batch: 0/1388, Loss: 1.3382\n",
      "Epoch: 8/20, Batch: 50/1388, Loss: 0.8195\n",
      "Epoch: 8/20, Batch: 100/1388, Loss: 1.2372\n",
      "Epoch: 8/20, Batch: 150/1388, Loss: 0.7370\n",
      "Epoch: 8/20, Batch: 200/1388, Loss: 0.8464\n",
      "Epoch: 8/20, Batch: 250/1388, Loss: 0.8038\n",
      "Epoch: 8/20, Batch: 300/1388, Loss: 0.9825\n",
      "Epoch: 8/20, Batch: 350/1388, Loss: 0.7888\n",
      "Epoch: 8/20, Batch: 400/1388, Loss: 0.8654\n",
      "Epoch: 8/20, Batch: 450/1388, Loss: 0.7395\n",
      "Epoch: 8/20, Batch: 500/1388, Loss: 0.9236\n",
      "Epoch: 8/20, Batch: 550/1388, Loss: 1.1454\n",
      "Epoch: 8/20, Batch: 600/1388, Loss: 1.1352\n",
      "Epoch: 8/20, Batch: 650/1388, Loss: 1.1292\n",
      "Epoch: 8/20, Batch: 700/1388, Loss: 1.2695\n",
      "Epoch: 8/20, Batch: 750/1388, Loss: 1.2236\n",
      "Epoch: 8/20, Batch: 800/1388, Loss: 1.0968\n",
      "Epoch: 8/20, Batch: 850/1388, Loss: 1.0942\n",
      "Epoch: 8/20, Batch: 900/1388, Loss: 0.8150\n",
      "Epoch: 8/20, Batch: 950/1388, Loss: 0.7611\n",
      "Epoch: 8/20, Batch: 1000/1388, Loss: 0.7736\n",
      "Epoch: 8/20, Batch: 1050/1388, Loss: 0.9711\n",
      "Epoch: 8/20, Batch: 1100/1388, Loss: 1.1703\n",
      "Epoch: 8/20, Batch: 1150/1388, Loss: 0.9455\n",
      "Epoch: 8/20, Batch: 1200/1388, Loss: 0.7342\n",
      "Epoch: 8/20, Batch: 1250/1388, Loss: 0.9345\n",
      "Epoch: 8/20, Batch: 1300/1388, Loss: 0.8704\n",
      "Epoch: 8/20, Batch: 1350/1388, Loss: 1.1736\n",
      "Epoch: 8, Train Loss: 0.9828\n",
      "Epoch: 9/20, Batch: 0/1388, Loss: 1.0085\n",
      "Epoch: 9/20, Batch: 50/1388, Loss: 1.1718\n",
      "Epoch: 9/20, Batch: 100/1388, Loss: 0.9884\n",
      "Epoch: 9/20, Batch: 150/1388, Loss: 1.5587\n",
      "Epoch: 9/20, Batch: 200/1388, Loss: 0.8838\n",
      "Epoch: 9/20, Batch: 250/1388, Loss: 0.9826\n",
      "Epoch: 9/20, Batch: 300/1388, Loss: 0.8069\n",
      "Epoch: 9/20, Batch: 350/1388, Loss: 1.0503\n",
      "Epoch: 9/20, Batch: 400/1388, Loss: 1.0509\n",
      "Epoch: 9/20, Batch: 450/1388, Loss: 0.8666\n",
      "Epoch: 9/20, Batch: 500/1388, Loss: 1.1010\n",
      "Epoch: 9/20, Batch: 550/1388, Loss: 0.7747\n",
      "Epoch: 9/20, Batch: 600/1388, Loss: 0.9778\n",
      "Epoch: 9/20, Batch: 650/1388, Loss: 1.1696\n",
      "Epoch: 9/20, Batch: 700/1388, Loss: 1.3322\n",
      "Epoch: 9/20, Batch: 750/1388, Loss: 1.1538\n",
      "Epoch: 9/20, Batch: 800/1388, Loss: 0.8013\n",
      "Epoch: 9/20, Batch: 850/1388, Loss: 0.8170\n",
      "Epoch: 9/20, Batch: 900/1388, Loss: 0.7365\n",
      "Epoch: 9/20, Batch: 950/1388, Loss: 0.9366\n",
      "Epoch: 9/20, Batch: 1000/1388, Loss: 1.0056\n",
      "Epoch: 9/20, Batch: 1050/1388, Loss: 0.8056\n",
      "Epoch: 9/20, Batch: 1100/1388, Loss: 0.8963\n",
      "Epoch: 9/20, Batch: 1150/1388, Loss: 1.4262\n",
      "Epoch: 9/20, Batch: 1200/1388, Loss: 1.0347\n",
      "Epoch: 9/20, Batch: 1250/1388, Loss: 0.9695\n",
      "Epoch: 9/20, Batch: 1300/1388, Loss: 0.9089\n",
      "Epoch: 9/20, Batch: 1350/1388, Loss: 0.9520\n",
      "Epoch: 9, Train Loss: 0.9830\n",
      "Epoch: 10/20, Batch: 0/1388, Loss: 1.0394\n",
      "Epoch: 10/20, Batch: 50/1388, Loss: 0.9558\n",
      "Epoch: 10/20, Batch: 100/1388, Loss: 0.9412\n",
      "Epoch: 10/20, Batch: 150/1388, Loss: 0.6800\n",
      "Epoch: 10/20, Batch: 200/1388, Loss: 0.8990\n",
      "Epoch: 10/20, Batch: 250/1388, Loss: 0.8394\n",
      "Epoch: 10/20, Batch: 300/1388, Loss: 0.7946\n",
      "Epoch: 10/20, Batch: 350/1388, Loss: 1.0110\n",
      "Epoch: 10/20, Batch: 400/1388, Loss: 0.9208\n",
      "Epoch: 10/20, Batch: 450/1388, Loss: 1.0895\n",
      "Epoch: 10/20, Batch: 500/1388, Loss: 0.9280\n",
      "Epoch: 10/20, Batch: 550/1388, Loss: 0.8712\n",
      "Epoch: 10/20, Batch: 600/1388, Loss: 0.8773\n",
      "Epoch: 10/20, Batch: 650/1388, Loss: 1.1379\n",
      "Epoch: 10/20, Batch: 700/1388, Loss: 0.8915\n",
      "Epoch: 10/20, Batch: 750/1388, Loss: 1.1863\n",
      "Epoch: 10/20, Batch: 800/1388, Loss: 0.7242\n",
      "Epoch: 10/20, Batch: 850/1388, Loss: 1.0862\n",
      "Epoch: 10/20, Batch: 900/1388, Loss: 1.0210\n",
      "Epoch: 10/20, Batch: 950/1388, Loss: 0.9934\n",
      "Epoch: 10/20, Batch: 1000/1388, Loss: 0.9285\n",
      "Epoch: 10/20, Batch: 1050/1388, Loss: 0.8452\n",
      "Epoch: 10/20, Batch: 1100/1388, Loss: 1.0441\n",
      "Epoch: 10/20, Batch: 1150/1388, Loss: 1.1886\n",
      "Epoch: 10/20, Batch: 1200/1388, Loss: 0.9476\n",
      "Epoch: 10/20, Batch: 1250/1388, Loss: 1.4482\n",
      "Epoch: 10/20, Batch: 1300/1388, Loss: 0.8531\n",
      "Epoch: 10/20, Batch: 1350/1388, Loss: 0.7134\n",
      "Epoch: 10, Train Loss: 0.9840\n",
      "Epoch: 11/20, Batch: 0/1388, Loss: 0.8466\n",
      "Epoch: 11/20, Batch: 50/1388, Loss: 0.9669\n",
      "Epoch: 11/20, Batch: 100/1388, Loss: 0.9061\n",
      "Epoch: 11/20, Batch: 150/1388, Loss: 0.7859\n",
      "Epoch: 11/20, Batch: 200/1388, Loss: 1.2697\n",
      "Epoch: 11/20, Batch: 250/1388, Loss: 0.9571\n",
      "Epoch: 11/20, Batch: 300/1388, Loss: 0.9291\n",
      "Epoch: 11/20, Batch: 350/1388, Loss: 1.1054\n",
      "Epoch: 11/20, Batch: 400/1388, Loss: 1.0064\n",
      "Epoch: 11/20, Batch: 450/1388, Loss: 0.7318\n",
      "Epoch: 11/20, Batch: 500/1388, Loss: 1.0108\n",
      "Epoch: 11/20, Batch: 550/1388, Loss: 1.3261\n",
      "Epoch: 11/20, Batch: 600/1388, Loss: 0.8581\n",
      "Epoch: 11/20, Batch: 650/1388, Loss: 0.7430\n",
      "Epoch: 11/20, Batch: 700/1388, Loss: 0.8903\n",
      "Epoch: 11/20, Batch: 750/1388, Loss: 1.0408\n",
      "Epoch: 11/20, Batch: 800/1388, Loss: 1.1591\n",
      "Epoch: 11/20, Batch: 850/1388, Loss: 1.0623\n",
      "Epoch: 11/20, Batch: 900/1388, Loss: 1.5011\n",
      "Epoch: 11/20, Batch: 950/1388, Loss: 1.3061\n",
      "Epoch: 11/20, Batch: 1000/1388, Loss: 0.8807\n",
      "Epoch: 11/20, Batch: 1050/1388, Loss: 1.3733\n",
      "Epoch: 11/20, Batch: 1100/1388, Loss: 0.8372\n",
      "Epoch: 11/20, Batch: 1150/1388, Loss: 0.7109\n",
      "Epoch: 11/20, Batch: 1200/1388, Loss: 1.0294\n",
      "Epoch: 11/20, Batch: 1250/1388, Loss: 0.7882\n",
      "Epoch: 11/20, Batch: 1300/1388, Loss: 0.6291\n",
      "Epoch: 11/20, Batch: 1350/1388, Loss: 1.0784\n",
      "Epoch: 11, Train Loss: 0.9813\n",
      "Epoch: 12/20, Batch: 0/1388, Loss: 0.9494\n",
      "Epoch: 12/20, Batch: 50/1388, Loss: 1.0273\n",
      "Epoch: 12/20, Batch: 100/1388, Loss: 0.6788\n",
      "Epoch: 12/20, Batch: 150/1388, Loss: 0.8979\n",
      "Epoch: 12/20, Batch: 200/1388, Loss: 0.7824\n",
      "Epoch: 12/20, Batch: 250/1388, Loss: 0.9291\n",
      "Epoch: 12/20, Batch: 300/1388, Loss: 1.1080\n",
      "Epoch: 12/20, Batch: 350/1388, Loss: 1.0135\n",
      "Epoch: 12/20, Batch: 400/1388, Loss: 0.8456\n",
      "Epoch: 12/20, Batch: 450/1388, Loss: 1.3342\n",
      "Epoch: 12/20, Batch: 500/1388, Loss: 1.1894\n",
      "Epoch: 12/20, Batch: 550/1388, Loss: 0.8687\n",
      "Epoch: 12/20, Batch: 600/1388, Loss: 0.9664\n",
      "Epoch: 12/20, Batch: 650/1388, Loss: 0.9671\n",
      "Epoch: 12/20, Batch: 700/1388, Loss: 1.0984\n",
      "Epoch: 12/20, Batch: 750/1388, Loss: 0.7339\n",
      "Epoch: 12/20, Batch: 800/1388, Loss: 1.0033\n",
      "Epoch: 12/20, Batch: 850/1388, Loss: 0.8658\n",
      "Epoch: 12/20, Batch: 900/1388, Loss: 0.7874\n",
      "Epoch: 12/20, Batch: 950/1388, Loss: 1.1384\n",
      "Epoch: 12/20, Batch: 1000/1388, Loss: 0.8301\n",
      "Epoch: 12/20, Batch: 1050/1388, Loss: 0.9528\n",
      "Epoch: 12/20, Batch: 1100/1388, Loss: 1.2703\n",
      "Epoch: 12/20, Batch: 1150/1388, Loss: 1.0063\n",
      "Epoch: 12/20, Batch: 1200/1388, Loss: 0.7558\n",
      "Epoch: 12/20, Batch: 1250/1388, Loss: 1.2576\n",
      "Epoch: 12/20, Batch: 1300/1388, Loss: 1.1567\n",
      "Epoch: 12/20, Batch: 1350/1388, Loss: 0.8158\n",
      "Epoch: 12, Train Loss: 0.9812\n",
      "Epoch: 13/20, Batch: 0/1388, Loss: 0.9686\n",
      "Epoch: 13/20, Batch: 50/1388, Loss: 0.7947\n",
      "Epoch: 13/20, Batch: 100/1388, Loss: 0.9830\n",
      "Epoch: 13/20, Batch: 150/1388, Loss: 0.9160\n",
      "Epoch: 13/20, Batch: 200/1388, Loss: 0.9095\n",
      "Epoch: 13/20, Batch: 250/1388, Loss: 0.9695\n",
      "Epoch: 13/20, Batch: 300/1388, Loss: 0.8906\n",
      "Epoch: 13/20, Batch: 350/1388, Loss: 1.2028\n",
      "Epoch: 13/20, Batch: 400/1388, Loss: 0.8003\n",
      "Epoch: 13/20, Batch: 450/1388, Loss: 0.8056\n",
      "Epoch: 13/20, Batch: 500/1388, Loss: 1.3356\n",
      "Epoch: 13/20, Batch: 550/1388, Loss: 0.9442\n",
      "Epoch: 13/20, Batch: 600/1388, Loss: 0.9602\n",
      "Epoch: 13/20, Batch: 650/1388, Loss: 0.8404\n",
      "Epoch: 13/20, Batch: 700/1388, Loss: 0.7760\n",
      "Epoch: 13/20, Batch: 750/1388, Loss: 1.0013\n",
      "Epoch: 13/20, Batch: 800/1388, Loss: 0.8033\n",
      "Epoch: 13/20, Batch: 850/1388, Loss: 0.9785\n",
      "Epoch: 13/20, Batch: 900/1388, Loss: 1.0510\n",
      "Epoch: 13/20, Batch: 950/1388, Loss: 1.0925\n",
      "Epoch: 13/20, Batch: 1000/1388, Loss: 0.6317\n",
      "Epoch: 13/20, Batch: 1050/1388, Loss: 1.0001\n",
      "Epoch: 13/20, Batch: 1100/1388, Loss: 0.8475\n",
      "Epoch: 13/20, Batch: 1150/1388, Loss: 1.2475\n",
      "Epoch: 13/20, Batch: 1200/1388, Loss: 1.2909\n",
      "Epoch: 13/20, Batch: 1250/1388, Loss: 0.7743\n",
      "Epoch: 13/20, Batch: 1300/1388, Loss: 0.9481\n",
      "Epoch: 13/20, Batch: 1350/1388, Loss: 0.9868\n",
      "Epoch: 13, Train Loss: 0.9807\n",
      "Epoch: 14/20, Batch: 0/1388, Loss: 0.7636\n",
      "Epoch: 14/20, Batch: 50/1388, Loss: 0.7959\n",
      "Epoch: 14/20, Batch: 100/1388, Loss: 0.8549\n",
      "Epoch: 14/20, Batch: 150/1388, Loss: 0.6616\n",
      "Epoch: 14/20, Batch: 200/1388, Loss: 1.3122\n",
      "Epoch: 14/20, Batch: 250/1388, Loss: 0.9842\n",
      "Epoch: 14/20, Batch: 300/1388, Loss: 0.8082\n",
      "Epoch: 14/20, Batch: 350/1388, Loss: 0.9953\n",
      "Epoch: 14/20, Batch: 400/1388, Loss: 0.8849\n",
      "Epoch: 14/20, Batch: 450/1388, Loss: 0.7492\n",
      "Epoch: 14/20, Batch: 500/1388, Loss: 0.8496\n",
      "Epoch: 14/20, Batch: 550/1388, Loss: 0.8001\n",
      "Epoch: 14/20, Batch: 600/1388, Loss: 0.8734\n",
      "Epoch: 14/20, Batch: 650/1388, Loss: 0.7742\n",
      "Epoch: 14/20, Batch: 700/1388, Loss: 1.2927\n",
      "Epoch: 14/20, Batch: 750/1388, Loss: 1.1108\n",
      "Epoch: 14/20, Batch: 800/1388, Loss: 1.0095\n",
      "Epoch: 14/20, Batch: 850/1388, Loss: 1.0489\n",
      "Epoch: 14/20, Batch: 900/1388, Loss: 1.0580\n",
      "Epoch: 14/20, Batch: 950/1388, Loss: 0.9769\n",
      "Epoch: 14/20, Batch: 1000/1388, Loss: 0.9993\n",
      "Epoch: 14/20, Batch: 1050/1388, Loss: 0.7433\n",
      "Epoch: 14/20, Batch: 1100/1388, Loss: 0.8905\n",
      "Epoch: 14/20, Batch: 1150/1388, Loss: 0.8744\n",
      "Epoch: 14/20, Batch: 1200/1388, Loss: 0.7742\n",
      "Epoch: 14/20, Batch: 1250/1388, Loss: 0.8082\n",
      "Epoch: 14/20, Batch: 1300/1388, Loss: 1.1521\n",
      "Epoch: 14/20, Batch: 1350/1388, Loss: 0.7463\n",
      "Epoch: 14, Train Loss: 0.9785\n",
      "Epoch: 15/20, Batch: 0/1388, Loss: 1.3938\n",
      "Epoch: 15/20, Batch: 50/1388, Loss: 0.9732\n",
      "Epoch: 15/20, Batch: 100/1388, Loss: 0.8865\n",
      "Epoch: 15/20, Batch: 150/1388, Loss: 1.0680\n",
      "Epoch: 15/20, Batch: 200/1388, Loss: 0.8270\n",
      "Epoch: 15/20, Batch: 250/1388, Loss: 0.6586\n",
      "Epoch: 15/20, Batch: 300/1388, Loss: 0.7531\n",
      "Epoch: 15/20, Batch: 350/1388, Loss: 0.8726\n",
      "Epoch: 15/20, Batch: 400/1388, Loss: 0.8995\n",
      "Epoch: 15/20, Batch: 450/1388, Loss: 0.8425\n",
      "Epoch: 15/20, Batch: 500/1388, Loss: 0.9722\n",
      "Epoch: 15/20, Batch: 550/1388, Loss: 0.9196\n",
      "Epoch: 15/20, Batch: 600/1388, Loss: 1.0738\n",
      "Epoch: 15/20, Batch: 650/1388, Loss: 1.3139\n",
      "Epoch: 15/20, Batch: 700/1388, Loss: 0.8619\n",
      "Epoch: 15/20, Batch: 750/1388, Loss: 0.9368\n",
      "Epoch: 15/20, Batch: 800/1388, Loss: 0.8089\n",
      "Epoch: 15/20, Batch: 850/1388, Loss: 0.9428\n",
      "Epoch: 15/20, Batch: 900/1388, Loss: 0.7655\n",
      "Epoch: 15/20, Batch: 950/1388, Loss: 0.8167\n",
      "Epoch: 15/20, Batch: 1000/1388, Loss: 0.8150\n",
      "Epoch: 15/20, Batch: 1050/1388, Loss: 1.0836\n",
      "Epoch: 15/20, Batch: 1100/1388, Loss: 1.2637\n",
      "Epoch: 15/20, Batch: 1150/1388, Loss: 0.9033\n",
      "Epoch: 15/20, Batch: 1200/1388, Loss: 1.4104\n",
      "Epoch: 15/20, Batch: 1250/1388, Loss: 0.8313\n",
      "Epoch: 15/20, Batch: 1300/1388, Loss: 1.0066\n",
      "Epoch: 15/20, Batch: 1350/1388, Loss: 1.0142\n",
      "Epoch: 15, Train Loss: 0.9811\n",
      "Epoch: 16/20, Batch: 0/1388, Loss: 0.8527\n",
      "Epoch: 16/20, Batch: 50/1388, Loss: 0.6866\n",
      "Epoch: 16/20, Batch: 100/1388, Loss: 1.1788\n",
      "Epoch: 16/20, Batch: 150/1388, Loss: 1.0022\n",
      "Epoch: 16/20, Batch: 200/1388, Loss: 0.7955\n",
      "Epoch: 16/20, Batch: 250/1388, Loss: 1.1514\n",
      "Epoch: 16/20, Batch: 300/1388, Loss: 1.3419\n",
      "Epoch: 16/20, Batch: 350/1388, Loss: 1.0101\n",
      "Epoch: 16/20, Batch: 400/1388, Loss: 0.9416\n",
      "Epoch: 16/20, Batch: 450/1388, Loss: 0.7470\n",
      "Epoch: 16/20, Batch: 500/1388, Loss: 1.0194\n",
      "Epoch: 16/20, Batch: 550/1388, Loss: 0.7677\n",
      "Epoch: 16/20, Batch: 600/1388, Loss: 1.0684\n",
      "Epoch: 16/20, Batch: 650/1388, Loss: 1.0469\n",
      "Epoch: 16/20, Batch: 700/1388, Loss: 1.1629\n",
      "Epoch: 16/20, Batch: 750/1388, Loss: 1.0183\n",
      "Epoch: 16/20, Batch: 800/1388, Loss: 1.0341\n",
      "Epoch: 16/20, Batch: 850/1388, Loss: 1.2213\n",
      "Epoch: 16/20, Batch: 900/1388, Loss: 0.9778\n",
      "Epoch: 16/20, Batch: 950/1388, Loss: 1.3020\n",
      "Epoch: 16/20, Batch: 1000/1388, Loss: 0.8188\n",
      "Epoch: 16/20, Batch: 1050/1388, Loss: 0.8591\n",
      "Epoch: 16/20, Batch: 1100/1388, Loss: 0.7939\n",
      "Epoch: 16/20, Batch: 1150/1388, Loss: 1.0197\n",
      "Epoch: 16/20, Batch: 1200/1388, Loss: 1.2497\n",
      "Epoch: 16/20, Batch: 1250/1388, Loss: 1.4253\n",
      "Epoch: 16/20, Batch: 1300/1388, Loss: 0.7626\n",
      "Epoch: 16/20, Batch: 1350/1388, Loss: 1.0541\n",
      "Epoch: 16, Train Loss: 0.9795\n",
      "Epoch: 17/20, Batch: 0/1388, Loss: 0.6343\n",
      "Epoch: 17/20, Batch: 50/1388, Loss: 1.0011\n",
      "Epoch: 17/20, Batch: 100/1388, Loss: 0.9895\n",
      "Epoch: 17/20, Batch: 150/1388, Loss: 0.8032\n",
      "Epoch: 17/20, Batch: 200/1388, Loss: 1.4317\n",
      "Epoch: 17/20, Batch: 250/1388, Loss: 1.0107\n",
      "Epoch: 17/20, Batch: 300/1388, Loss: 0.9551\n",
      "Epoch: 17/20, Batch: 350/1388, Loss: 1.3278\n",
      "Epoch: 17/20, Batch: 400/1388, Loss: 0.9514\n",
      "Epoch: 17/20, Batch: 450/1388, Loss: 0.7700\n",
      "Epoch: 17/20, Batch: 500/1388, Loss: 1.2505\n",
      "Epoch: 17/20, Batch: 550/1388, Loss: 0.8983\n",
      "Epoch: 17/20, Batch: 600/1388, Loss: 0.9077\n",
      "Epoch: 17/20, Batch: 650/1388, Loss: 1.4018\n",
      "Epoch: 17/20, Batch: 700/1388, Loss: 1.0900\n",
      "Epoch: 17/20, Batch: 750/1388, Loss: 0.9572\n",
      "Epoch: 17/20, Batch: 800/1388, Loss: 0.8620\n",
      "Epoch: 17/20, Batch: 850/1388, Loss: 1.4489\n",
      "Epoch: 17/20, Batch: 900/1388, Loss: 0.8895\n",
      "Epoch: 17/20, Batch: 950/1388, Loss: 1.1358\n",
      "Epoch: 17/20, Batch: 1000/1388, Loss: 0.9184\n",
      "Epoch: 17/20, Batch: 1050/1388, Loss: 1.2794\n",
      "Epoch: 17/20, Batch: 1100/1388, Loss: 0.9654\n",
      "Epoch: 17/20, Batch: 1150/1388, Loss: 1.1343\n",
      "Epoch: 17/20, Batch: 1200/1388, Loss: 1.1009\n",
      "Epoch: 17/20, Batch: 1250/1388, Loss: 0.8473\n",
      "Epoch: 17/20, Batch: 1300/1388, Loss: 0.9402\n",
      "Epoch: 17/20, Batch: 1350/1388, Loss: 0.9056\n",
      "Epoch: 17, Train Loss: 0.9797\n",
      "Epoch: 18/20, Batch: 0/1388, Loss: 0.8493\n",
      "Epoch: 18/20, Batch: 50/1388, Loss: 0.7809\n",
      "Epoch: 18/20, Batch: 100/1388, Loss: 0.7622\n",
      "Epoch: 18/20, Batch: 150/1388, Loss: 0.8929\n",
      "Epoch: 18/20, Batch: 200/1388, Loss: 0.9413\n",
      "Epoch: 18/20, Batch: 250/1388, Loss: 0.8261\n",
      "Epoch: 18/20, Batch: 300/1388, Loss: 0.9423\n",
      "Epoch: 18/20, Batch: 350/1388, Loss: 1.0177\n",
      "Epoch: 18/20, Batch: 400/1388, Loss: 1.0214\n",
      "Epoch: 18/20, Batch: 450/1388, Loss: 0.8586\n",
      "Epoch: 18/20, Batch: 500/1388, Loss: 1.2385\n",
      "Epoch: 18/20, Batch: 550/1388, Loss: 0.8732\n",
      "Epoch: 18/20, Batch: 600/1388, Loss: 1.0687\n",
      "Epoch: 18/20, Batch: 650/1388, Loss: 0.8256\n",
      "Epoch: 18/20, Batch: 700/1388, Loss: 0.9877\n",
      "Epoch: 18/20, Batch: 750/1388, Loss: 0.6840\n",
      "Epoch: 18/20, Batch: 800/1388, Loss: 1.1158\n",
      "Epoch: 18/20, Batch: 850/1388, Loss: 0.9837\n",
      "Epoch: 18/20, Batch: 900/1388, Loss: 0.9492\n",
      "Epoch: 18/20, Batch: 950/1388, Loss: 0.7993\n",
      "Epoch: 18/20, Batch: 1000/1388, Loss: 0.9327\n",
      "Epoch: 18/20, Batch: 1050/1388, Loss: 0.7378\n",
      "Epoch: 18/20, Batch: 1100/1388, Loss: 0.9082\n",
      "Epoch: 18/20, Batch: 1150/1388, Loss: 0.8807\n",
      "Epoch: 18/20, Batch: 1200/1388, Loss: 1.3098\n",
      "Epoch: 18/20, Batch: 1250/1388, Loss: 0.8646\n",
      "Epoch: 18/20, Batch: 1300/1388, Loss: 1.0085\n",
      "Epoch: 18/20, Batch: 1350/1388, Loss: 0.8093\n",
      "Epoch: 18, Train Loss: 0.9800\n",
      "Epoch: 19/20, Batch: 0/1388, Loss: 0.9344\n",
      "Epoch: 19/20, Batch: 50/1388, Loss: 1.0297\n",
      "Epoch: 19/20, Batch: 100/1388, Loss: 1.1362\n",
      "Epoch: 19/20, Batch: 150/1388, Loss: 1.0444\n",
      "Epoch: 19/20, Batch: 200/1388, Loss: 1.3458\n",
      "Epoch: 19/20, Batch: 250/1388, Loss: 1.0844\n",
      "Epoch: 19/20, Batch: 300/1388, Loss: 0.8033\n",
      "Epoch: 19/20, Batch: 350/1388, Loss: 1.3593\n",
      "Epoch: 19/20, Batch: 400/1388, Loss: 0.7467\n",
      "Epoch: 19/20, Batch: 450/1388, Loss: 0.9978\n",
      "Epoch: 19/20, Batch: 500/1388, Loss: 0.8473\n",
      "Epoch: 19/20, Batch: 550/1388, Loss: 0.9198\n",
      "Epoch: 19/20, Batch: 600/1388, Loss: 1.1645\n",
      "Epoch: 19/20, Batch: 650/1388, Loss: 1.1172\n",
      "Epoch: 19/20, Batch: 700/1388, Loss: 0.8663\n",
      "Epoch: 19/20, Batch: 750/1388, Loss: 0.8753\n",
      "Epoch: 19/20, Batch: 800/1388, Loss: 1.0172\n",
      "Epoch: 19/20, Batch: 850/1388, Loss: 0.8850\n",
      "Epoch: 19/20, Batch: 900/1388, Loss: 1.0935\n",
      "Epoch: 19/20, Batch: 950/1388, Loss: 0.8583\n",
      "Epoch: 19/20, Batch: 1000/1388, Loss: 0.8863\n",
      "Epoch: 19/20, Batch: 1050/1388, Loss: 0.8286\n",
      "Epoch: 19/20, Batch: 1100/1388, Loss: 1.1064\n",
      "Epoch: 19/20, Batch: 1150/1388, Loss: 1.2355\n",
      "Epoch: 19/20, Batch: 1200/1388, Loss: 1.0822\n",
      "Epoch: 19/20, Batch: 1250/1388, Loss: 0.7524\n",
      "Epoch: 19/20, Batch: 1300/1388, Loss: 1.1870\n",
      "Epoch: 19/20, Batch: 1350/1388, Loss: 1.1030\n",
      "Epoch: 19, Train Loss: 0.9783\n",
      "Epoch: 20/20, Batch: 0/1388, Loss: 0.9028\n",
      "Epoch: 20/20, Batch: 50/1388, Loss: 0.7256\n",
      "Epoch: 20/20, Batch: 100/1388, Loss: 0.8342\n",
      "Epoch: 20/20, Batch: 150/1388, Loss: 0.9135\n",
      "Epoch: 20/20, Batch: 200/1388, Loss: 1.1114\n",
      "Epoch: 20/20, Batch: 250/1388, Loss: 0.8070\n",
      "Epoch: 20/20, Batch: 300/1388, Loss: 0.7664\n",
      "Epoch: 20/20, Batch: 350/1388, Loss: 0.7991\n",
      "Epoch: 20/20, Batch: 400/1388, Loss: 0.8228\n",
      "Epoch: 20/20, Batch: 450/1388, Loss: 0.8525\n",
      "Epoch: 20/20, Batch: 500/1388, Loss: 0.7848\n",
      "Epoch: 20/20, Batch: 550/1388, Loss: 0.9864\n",
      "Epoch: 20/20, Batch: 600/1388, Loss: 0.8717\n",
      "Epoch: 20/20, Batch: 650/1388, Loss: 1.4162\n",
      "Epoch: 20/20, Batch: 700/1388, Loss: 1.4614\n",
      "Epoch: 20/20, Batch: 750/1388, Loss: 0.7778\n",
      "Epoch: 20/20, Batch: 800/1388, Loss: 0.6986\n",
      "Epoch: 20/20, Batch: 850/1388, Loss: 0.8175\n",
      "Epoch: 20/20, Batch: 900/1388, Loss: 1.1244\n",
      "Epoch: 20/20, Batch: 950/1388, Loss: 0.8240\n",
      "Epoch: 20/20, Batch: 1000/1388, Loss: 0.8407\n",
      "Epoch: 20/20, Batch: 1050/1388, Loss: 0.7718\n",
      "Epoch: 20/20, Batch: 1100/1388, Loss: 1.1494\n",
      "Epoch: 20/20, Batch: 1150/1388, Loss: 0.7999\n",
      "Epoch: 20/20, Batch: 1200/1388, Loss: 0.7425\n",
      "Epoch: 20/20, Batch: 1250/1388, Loss: 0.7960\n",
      "Epoch: 20/20, Batch: 1300/1388, Loss: 0.8531\n",
      "Epoch: 20/20, Batch: 1350/1388, Loss: 1.2047\n",
      "Epoch: 20, Train Loss: 0.9789\n",
      "Training finished!\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "std = StandardScaler()\n",
    "X_train_scaled = std.fit_transform(X_train)\n",
    "X_test_scaled = std.transform(X_test)\n",
    "\n",
    "pca = PCA(n_components=14)\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "train_dataset = HeartDiseaseDataset(X_train_pca, y_train)\n",
    "test_dataset = HeartDiseaseDataset(X_test_pca, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1024, shuffle=False)\n",
    "\n",
    "input_features = X_train_pca.shape[1]  \n",
    "\n",
    "weighted_model = HeartDiseaseMLPClassifier(input_size=input_features, class_frequencies=class_frequencies).to(device)\n",
    "\n",
    "optimizer = optim.Adam(weighted_model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)\n",
    "\n",
    "num_epochs = 20\n",
    "weighted_train_losses = []\n",
    "\n",
    "print(\"\\nStarting training...\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    weighted_model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = weighted_model(data)\n",
    "    \n",
    "        target = target.float()\n",
    "        output = output.float()    \n",
    "        loss = weighted_model.get_weighted_loss(output, target)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if batch_idx % 50 == 0:\n",
    "            print(f'Epoch: {epoch+1}/{num_epochs}, Batch: {batch_idx}/{len(train_loader)}, Loss: {loss.item():.4f}')\n",
    "    \n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "    weighted_train_losses.append(avg_train_loss)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1}, Train Loss: {avg_train_loss:.4f}')\n",
    "\n",
    "print('Training finished!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report (Weighted):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.70      0.81     68936\n",
      "         1.0       0.22      0.83      0.35      7168\n",
      "\n",
      "    accuracy                           0.71     76104\n",
      "   macro avg       0.60      0.77      0.58     76104\n",
      "weighted avg       0.91      0.71      0.77     76104\n",
      "\n",
      "\n",
      "Confusion Matrix (Weighted):\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "weighted_model.eval()\n",
    "y_true_weighted = []\n",
    "y_pred_weighted = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        output = weighted_model(data)\n",
    "        \n",
    "        y_true_weighted.extend(target.cpu().numpy())\n",
    "        y_pred_weighted.extend((output > 0.5).cpu().numpy())\n",
    "\n",
    "y_true_weighted = np.array(y_true_weighted)\n",
    "y_pred_weighted = np.array(y_pred_weighted)\n",
    "\n",
    "print(\"\\nClassification Report (Weighted):\")\n",
    "print(classification_report(y_true_weighted, y_pred_weighted))\n",
    "print(\"\\nConfusion Matrix (Weighted):\")\n",
    "cm_weighted = confusion_matrix(y_true_weighted, y_pred_weighted)\n",
    "sns.heatmap(cm_weighted, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix (Weighted)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train:test = 6:4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training...\n",
      "Epoch: 1/20, Batch: 0/1190, Loss: 1.3539\n",
      "Epoch: 1/20, Batch: 50/1190, Loss: 1.1146\n",
      "Epoch: 1/20, Batch: 100/1190, Loss: 0.9146\n",
      "Epoch: 1/20, Batch: 150/1190, Loss: 1.0458\n",
      "Epoch: 1/20, Batch: 200/1190, Loss: 0.9952\n",
      "Epoch: 1/20, Batch: 250/1190, Loss: 0.8527\n",
      "Epoch: 1/20, Batch: 300/1190, Loss: 1.0107\n",
      "Epoch: 1/20, Batch: 350/1190, Loss: 1.4752\n",
      "Epoch: 1/20, Batch: 400/1190, Loss: 0.9263\n",
      "Epoch: 1/20, Batch: 450/1190, Loss: 0.8552\n",
      "Epoch: 1/20, Batch: 500/1190, Loss: 0.6298\n",
      "Epoch: 1/20, Batch: 550/1190, Loss: 1.0326\n",
      "Epoch: 1/20, Batch: 600/1190, Loss: 0.7615\n",
      "Epoch: 1/20, Batch: 650/1190, Loss: 1.3143\n",
      "Epoch: 1/20, Batch: 700/1190, Loss: 0.7878\n",
      "Epoch: 1/20, Batch: 750/1190, Loss: 0.7185\n",
      "Epoch: 1/20, Batch: 800/1190, Loss: 1.0536\n",
      "Epoch: 1/20, Batch: 850/1190, Loss: 1.1834\n",
      "Epoch: 1/20, Batch: 900/1190, Loss: 0.9659\n",
      "Epoch: 1/20, Batch: 950/1190, Loss: 0.9442\n",
      "Epoch: 1/20, Batch: 1000/1190, Loss: 1.2166\n",
      "Epoch: 1/20, Batch: 1050/1190, Loss: 0.8207\n",
      "Epoch: 1/20, Batch: 1100/1190, Loss: 1.2993\n",
      "Epoch: 1/20, Batch: 1150/1190, Loss: 1.2707\n",
      "Epoch: 1, Train Loss: 1.0213\n",
      "Epoch: 2/20, Batch: 0/1190, Loss: 0.9645\n",
      "Epoch: 2/20, Batch: 50/1190, Loss: 0.9701\n",
      "Epoch: 2/20, Batch: 100/1190, Loss: 1.2566\n",
      "Epoch: 2/20, Batch: 150/1190, Loss: 1.0954\n",
      "Epoch: 2/20, Batch: 200/1190, Loss: 1.0938\n",
      "Epoch: 2/20, Batch: 250/1190, Loss: 0.9603\n",
      "Epoch: 2/20, Batch: 300/1190, Loss: 1.9030\n",
      "Epoch: 2/20, Batch: 350/1190, Loss: 0.9338\n",
      "Epoch: 2/20, Batch: 400/1190, Loss: 0.8165\n",
      "Epoch: 2/20, Batch: 450/1190, Loss: 1.0097\n",
      "Epoch: 2/20, Batch: 500/1190, Loss: 0.7766\n",
      "Epoch: 2/20, Batch: 550/1190, Loss: 0.9175\n",
      "Epoch: 2/20, Batch: 600/1190, Loss: 0.6889\n",
      "Epoch: 2/20, Batch: 650/1190, Loss: 1.0038\n",
      "Epoch: 2/20, Batch: 700/1190, Loss: 1.0384\n",
      "Epoch: 2/20, Batch: 750/1190, Loss: 0.9213\n",
      "Epoch: 2/20, Batch: 800/1190, Loss: 0.7051\n",
      "Epoch: 2/20, Batch: 850/1190, Loss: 1.1166\n",
      "Epoch: 2/20, Batch: 900/1190, Loss: 0.9216\n",
      "Epoch: 2/20, Batch: 950/1190, Loss: 0.6690\n",
      "Epoch: 2/20, Batch: 1000/1190, Loss: 0.6639\n",
      "Epoch: 2/20, Batch: 1050/1190, Loss: 1.2434\n",
      "Epoch: 2/20, Batch: 1100/1190, Loss: 1.2588\n",
      "Epoch: 2/20, Batch: 1150/1190, Loss: 1.3806\n",
      "Epoch: 2, Train Loss: 1.0014\n",
      "Epoch: 3/20, Batch: 0/1190, Loss: 1.3959\n",
      "Epoch: 3/20, Batch: 50/1190, Loss: 1.0943\n",
      "Epoch: 3/20, Batch: 100/1190, Loss: 1.0620\n",
      "Epoch: 3/20, Batch: 150/1190, Loss: 1.0872\n",
      "Epoch: 3/20, Batch: 200/1190, Loss: 0.8681\n",
      "Epoch: 3/20, Batch: 250/1190, Loss: 0.7954\n",
      "Epoch: 3/20, Batch: 300/1190, Loss: 0.9261\n",
      "Epoch: 3/20, Batch: 350/1190, Loss: 0.7862\n",
      "Epoch: 3/20, Batch: 400/1190, Loss: 0.8635\n",
      "Epoch: 3/20, Batch: 450/1190, Loss: 1.1840\n",
      "Epoch: 3/20, Batch: 500/1190, Loss: 1.0186\n",
      "Epoch: 3/20, Batch: 550/1190, Loss: 1.0254\n",
      "Epoch: 3/20, Batch: 600/1190, Loss: 1.0650\n",
      "Epoch: 3/20, Batch: 650/1190, Loss: 1.0559\n",
      "Epoch: 3/20, Batch: 700/1190, Loss: 1.1736\n",
      "Epoch: 3/20, Batch: 750/1190, Loss: 0.9215\n",
      "Epoch: 3/20, Batch: 800/1190, Loss: 1.1428\n",
      "Epoch: 3/20, Batch: 850/1190, Loss: 0.8857\n",
      "Epoch: 3/20, Batch: 900/1190, Loss: 1.0586\n",
      "Epoch: 3/20, Batch: 950/1190, Loss: 1.1310\n",
      "Epoch: 3/20, Batch: 1000/1190, Loss: 1.0562\n",
      "Epoch: 3/20, Batch: 1050/1190, Loss: 0.8945\n",
      "Epoch: 3/20, Batch: 1100/1190, Loss: 1.1275\n",
      "Epoch: 3/20, Batch: 1150/1190, Loss: 0.8061\n",
      "Epoch: 3, Train Loss: 0.9945\n",
      "Epoch: 4/20, Batch: 0/1190, Loss: 0.9633\n",
      "Epoch: 4/20, Batch: 50/1190, Loss: 1.4662\n",
      "Epoch: 4/20, Batch: 100/1190, Loss: 0.7685\n",
      "Epoch: 4/20, Batch: 150/1190, Loss: 1.2017\n",
      "Epoch: 4/20, Batch: 200/1190, Loss: 1.0419\n",
      "Epoch: 4/20, Batch: 250/1190, Loss: 0.6863\n",
      "Epoch: 4/20, Batch: 300/1190, Loss: 0.9555\n",
      "Epoch: 4/20, Batch: 350/1190, Loss: 0.8795\n",
      "Epoch: 4/20, Batch: 400/1190, Loss: 0.8921\n",
      "Epoch: 4/20, Batch: 450/1190, Loss: 0.6870\n",
      "Epoch: 4/20, Batch: 500/1190, Loss: 1.0097\n",
      "Epoch: 4/20, Batch: 550/1190, Loss: 0.8679\n",
      "Epoch: 4/20, Batch: 600/1190, Loss: 1.4932\n",
      "Epoch: 4/20, Batch: 650/1190, Loss: 0.7677\n",
      "Epoch: 4/20, Batch: 700/1190, Loss: 1.0612\n",
      "Epoch: 4/20, Batch: 750/1190, Loss: 0.9412\n",
      "Epoch: 4/20, Batch: 800/1190, Loss: 1.2053\n",
      "Epoch: 4/20, Batch: 850/1190, Loss: 1.1425\n",
      "Epoch: 4/20, Batch: 900/1190, Loss: 1.3010\n",
      "Epoch: 4/20, Batch: 950/1190, Loss: 1.2818\n",
      "Epoch: 4/20, Batch: 1000/1190, Loss: 0.9864\n",
      "Epoch: 4/20, Batch: 1050/1190, Loss: 0.8943\n",
      "Epoch: 4/20, Batch: 1100/1190, Loss: 0.7627\n",
      "Epoch: 4/20, Batch: 1150/1190, Loss: 0.9057\n",
      "Epoch: 4, Train Loss: 0.9968\n",
      "Epoch: 5/20, Batch: 0/1190, Loss: 0.8567\n",
      "Epoch: 5/20, Batch: 50/1190, Loss: 0.9141\n",
      "Epoch: 5/20, Batch: 100/1190, Loss: 0.7641\n",
      "Epoch: 5/20, Batch: 150/1190, Loss: 1.3277\n",
      "Epoch: 5/20, Batch: 200/1190, Loss: 0.8857\n",
      "Epoch: 5/20, Batch: 250/1190, Loss: 0.8124\n",
      "Epoch: 5/20, Batch: 300/1190, Loss: 0.9407\n",
      "Epoch: 5/20, Batch: 350/1190, Loss: 0.8161\n",
      "Epoch: 5/20, Batch: 400/1190, Loss: 0.9298\n",
      "Epoch: 5/20, Batch: 450/1190, Loss: 1.2831\n",
      "Epoch: 5/20, Batch: 500/1190, Loss: 1.0180\n",
      "Epoch: 5/20, Batch: 550/1190, Loss: 1.1041\n",
      "Epoch: 5/20, Batch: 600/1190, Loss: 0.9413\n",
      "Epoch: 5/20, Batch: 650/1190, Loss: 1.1184\n",
      "Epoch: 5/20, Batch: 700/1190, Loss: 0.7994\n",
      "Epoch: 5/20, Batch: 750/1190, Loss: 0.9983\n",
      "Epoch: 5/20, Batch: 800/1190, Loss: 0.8519\n",
      "Epoch: 5/20, Batch: 850/1190, Loss: 1.2670\n",
      "Epoch: 5/20, Batch: 900/1190, Loss: 0.9316\n",
      "Epoch: 5/20, Batch: 950/1190, Loss: 0.8036\n",
      "Epoch: 5/20, Batch: 1000/1190, Loss: 0.7520\n",
      "Epoch: 5/20, Batch: 1050/1190, Loss: 1.1007\n",
      "Epoch: 5/20, Batch: 1100/1190, Loss: 0.7917\n",
      "Epoch: 5/20, Batch: 1150/1190, Loss: 1.4617\n",
      "Epoch: 5, Train Loss: 0.9904\n",
      "Epoch: 6/20, Batch: 0/1190, Loss: 1.0865\n",
      "Epoch: 6/20, Batch: 50/1190, Loss: 0.7949\n",
      "Epoch: 6/20, Batch: 100/1190, Loss: 0.9365\n",
      "Epoch: 6/20, Batch: 150/1190, Loss: 0.9796\n",
      "Epoch: 6/20, Batch: 200/1190, Loss: 1.2540\n",
      "Epoch: 6/20, Batch: 250/1190, Loss: 0.9651\n",
      "Epoch: 6/20, Batch: 300/1190, Loss: 0.7582\n",
      "Epoch: 6/20, Batch: 350/1190, Loss: 0.6185\n",
      "Epoch: 6/20, Batch: 400/1190, Loss: 1.0382\n",
      "Epoch: 6/20, Batch: 450/1190, Loss: 0.8081\n",
      "Epoch: 6/20, Batch: 500/1190, Loss: 0.9372\n",
      "Epoch: 6/20, Batch: 550/1190, Loss: 1.0915\n",
      "Epoch: 6/20, Batch: 600/1190, Loss: 0.9263\n",
      "Epoch: 6/20, Batch: 650/1190, Loss: 0.7442\n",
      "Epoch: 6/20, Batch: 700/1190, Loss: 0.7674\n",
      "Epoch: 6/20, Batch: 750/1190, Loss: 0.9305\n",
      "Epoch: 6/20, Batch: 800/1190, Loss: 1.3349\n",
      "Epoch: 6/20, Batch: 850/1190, Loss: 0.8553\n",
      "Epoch: 6/20, Batch: 900/1190, Loss: 0.8754\n",
      "Epoch: 6/20, Batch: 950/1190, Loss: 1.4256\n",
      "Epoch: 6/20, Batch: 1000/1190, Loss: 0.7959\n",
      "Epoch: 6/20, Batch: 1050/1190, Loss: 1.0070\n",
      "Epoch: 6/20, Batch: 1100/1190, Loss: 0.9650\n",
      "Epoch: 6/20, Batch: 1150/1190, Loss: 0.8294\n",
      "Epoch: 6, Train Loss: 0.9890\n",
      "Epoch: 7/20, Batch: 0/1190, Loss: 1.0737\n",
      "Epoch: 7/20, Batch: 50/1190, Loss: 0.9890\n",
      "Epoch: 7/20, Batch: 100/1190, Loss: 1.1541\n",
      "Epoch: 7/20, Batch: 150/1190, Loss: 1.2780\n",
      "Epoch: 7/20, Batch: 200/1190, Loss: 0.8225\n",
      "Epoch: 7/20, Batch: 250/1190, Loss: 1.0812\n",
      "Epoch: 7/20, Batch: 300/1190, Loss: 0.7333\n",
      "Epoch: 7/20, Batch: 350/1190, Loss: 0.8336\n",
      "Epoch: 7/20, Batch: 400/1190, Loss: 1.3455\n",
      "Epoch: 7/20, Batch: 450/1190, Loss: 1.4003\n",
      "Epoch: 7/20, Batch: 500/1190, Loss: 0.8783\n",
      "Epoch: 7/20, Batch: 550/1190, Loss: 0.8862\n",
      "Epoch: 7/20, Batch: 600/1190, Loss: 1.1093\n",
      "Epoch: 7/20, Batch: 650/1190, Loss: 0.9120\n",
      "Epoch: 7/20, Batch: 700/1190, Loss: 1.3079\n",
      "Epoch: 7/20, Batch: 750/1190, Loss: 1.3430\n",
      "Epoch: 7/20, Batch: 800/1190, Loss: 0.7081\n",
      "Epoch: 7/20, Batch: 850/1190, Loss: 0.9193\n",
      "Epoch: 7/20, Batch: 900/1190, Loss: 0.8976\n",
      "Epoch: 7/20, Batch: 950/1190, Loss: 1.0080\n",
      "Epoch: 7/20, Batch: 1000/1190, Loss: 1.2322\n",
      "Epoch: 7/20, Batch: 1050/1190, Loss: 1.0100\n",
      "Epoch: 7/20, Batch: 1100/1190, Loss: 0.7662\n",
      "Epoch: 7/20, Batch: 1150/1190, Loss: 0.9922\n",
      "Epoch: 7, Train Loss: 0.9885\n",
      "Epoch: 8/20, Batch: 0/1190, Loss: 0.9866\n",
      "Epoch: 8/20, Batch: 50/1190, Loss: 0.7992\n",
      "Epoch: 8/20, Batch: 100/1190, Loss: 1.0123\n",
      "Epoch: 8/20, Batch: 150/1190, Loss: 0.8938\n",
      "Epoch: 8/20, Batch: 200/1190, Loss: 0.9838\n",
      "Epoch: 8/20, Batch: 250/1190, Loss: 1.3233\n",
      "Epoch: 8/20, Batch: 300/1190, Loss: 0.7752\n",
      "Epoch: 8/20, Batch: 350/1190, Loss: 0.8941\n",
      "Epoch: 8/20, Batch: 400/1190, Loss: 1.0205\n",
      "Epoch: 8/20, Batch: 450/1190, Loss: 1.3652\n",
      "Epoch: 8/20, Batch: 500/1190, Loss: 0.9421\n",
      "Epoch: 8/20, Batch: 550/1190, Loss: 1.5508\n",
      "Epoch: 8/20, Batch: 600/1190, Loss: 1.6797\n",
      "Epoch: 8/20, Batch: 650/1190, Loss: 1.0015\n",
      "Epoch: 8/20, Batch: 700/1190, Loss: 0.9474\n",
      "Epoch: 8/20, Batch: 750/1190, Loss: 0.9133\n",
      "Epoch: 8/20, Batch: 800/1190, Loss: 0.9772\n",
      "Epoch: 8/20, Batch: 850/1190, Loss: 0.8488\n",
      "Epoch: 8/20, Batch: 900/1190, Loss: 0.8549\n",
      "Epoch: 8/20, Batch: 950/1190, Loss: 0.7919\n",
      "Epoch: 8/20, Batch: 1000/1190, Loss: 0.7416\n",
      "Epoch: 8/20, Batch: 1050/1190, Loss: 0.9902\n",
      "Epoch: 8/20, Batch: 1100/1190, Loss: 0.7733\n",
      "Epoch: 8/20, Batch: 1150/1190, Loss: 1.0173\n",
      "Epoch: 8, Train Loss: 0.9880\n",
      "Epoch: 9/20, Batch: 0/1190, Loss: 1.2339\n",
      "Epoch: 9/20, Batch: 50/1190, Loss: 1.0877\n",
      "Epoch: 9/20, Batch: 100/1190, Loss: 0.7255\n",
      "Epoch: 9/20, Batch: 150/1190, Loss: 0.8458\n",
      "Epoch: 9/20, Batch: 200/1190, Loss: 1.1992\n",
      "Epoch: 9/20, Batch: 250/1190, Loss: 0.7178\n",
      "Epoch: 9/20, Batch: 300/1190, Loss: 0.9112\n",
      "Epoch: 9/20, Batch: 350/1190, Loss: 0.7902\n",
      "Epoch: 9/20, Batch: 400/1190, Loss: 0.7636\n",
      "Epoch: 9/20, Batch: 450/1190, Loss: 0.7716\n",
      "Epoch: 9/20, Batch: 500/1190, Loss: 0.9188\n",
      "Epoch: 9/20, Batch: 550/1190, Loss: 0.8651\n",
      "Epoch: 9/20, Batch: 600/1190, Loss: 0.6770\n",
      "Epoch: 9/20, Batch: 650/1190, Loss: 0.6648\n",
      "Epoch: 9/20, Batch: 700/1190, Loss: 1.0155\n",
      "Epoch: 9/20, Batch: 750/1190, Loss: 0.7565\n",
      "Epoch: 9/20, Batch: 800/1190, Loss: 0.8283\n",
      "Epoch: 9/20, Batch: 850/1190, Loss: 1.0630\n",
      "Epoch: 9/20, Batch: 900/1190, Loss: 1.1956\n",
      "Epoch: 9/20, Batch: 950/1190, Loss: 0.9419\n",
      "Epoch: 9/20, Batch: 1000/1190, Loss: 1.0657\n",
      "Epoch: 9/20, Batch: 1050/1190, Loss: 0.7291\n",
      "Epoch: 9/20, Batch: 1100/1190, Loss: 1.2119\n",
      "Epoch: 9/20, Batch: 1150/1190, Loss: 0.9462\n",
      "Epoch: 9, Train Loss: 0.9865\n",
      "Epoch: 10/20, Batch: 0/1190, Loss: 0.8883\n",
      "Epoch: 10/20, Batch: 50/1190, Loss: 0.8549\n",
      "Epoch: 10/20, Batch: 100/1190, Loss: 0.9571\n",
      "Epoch: 10/20, Batch: 150/1190, Loss: 0.8506\n",
      "Epoch: 10/20, Batch: 200/1190, Loss: 1.1912\n",
      "Epoch: 10/20, Batch: 250/1190, Loss: 1.0947\n",
      "Epoch: 10/20, Batch: 300/1190, Loss: 0.8303\n",
      "Epoch: 10/20, Batch: 350/1190, Loss: 1.0904\n",
      "Epoch: 10/20, Batch: 400/1190, Loss: 0.9225\n",
      "Epoch: 10/20, Batch: 450/1190, Loss: 0.8072\n",
      "Epoch: 10/20, Batch: 500/1190, Loss: 0.9975\n",
      "Epoch: 10/20, Batch: 550/1190, Loss: 0.9242\n",
      "Epoch: 10/20, Batch: 600/1190, Loss: 1.1585\n",
      "Epoch: 10/20, Batch: 650/1190, Loss: 0.9759\n",
      "Epoch: 10/20, Batch: 700/1190, Loss: 1.0678\n",
      "Epoch: 10/20, Batch: 750/1190, Loss: 1.2756\n",
      "Epoch: 10/20, Batch: 800/1190, Loss: 0.8915\n",
      "Epoch: 10/20, Batch: 850/1190, Loss: 0.9549\n",
      "Epoch: 10/20, Batch: 900/1190, Loss: 0.8750\n",
      "Epoch: 10/20, Batch: 950/1190, Loss: 0.8149\n",
      "Epoch: 10/20, Batch: 1000/1190, Loss: 0.9420\n",
      "Epoch: 10/20, Batch: 1050/1190, Loss: 0.7482\n",
      "Epoch: 10/20, Batch: 1100/1190, Loss: 1.1196\n",
      "Epoch: 10/20, Batch: 1150/1190, Loss: 1.1979\n",
      "Epoch: 10, Train Loss: 0.9854\n",
      "Epoch: 11/20, Batch: 0/1190, Loss: 0.7867\n",
      "Epoch: 11/20, Batch: 50/1190, Loss: 0.9666\n",
      "Epoch: 11/20, Batch: 100/1190, Loss: 1.1159\n",
      "Epoch: 11/20, Batch: 150/1190, Loss: 1.2584\n",
      "Epoch: 11/20, Batch: 200/1190, Loss: 1.2152\n",
      "Epoch: 11/20, Batch: 250/1190, Loss: 0.8498\n",
      "Epoch: 11/20, Batch: 300/1190, Loss: 1.6999\n",
      "Epoch: 11/20, Batch: 350/1190, Loss: 1.1989\n",
      "Epoch: 11/20, Batch: 400/1190, Loss: 1.0635\n",
      "Epoch: 11/20, Batch: 450/1190, Loss: 0.7453\n",
      "Epoch: 11/20, Batch: 500/1190, Loss: 0.7021\n",
      "Epoch: 11/20, Batch: 550/1190, Loss: 0.9758\n",
      "Epoch: 11/20, Batch: 600/1190, Loss: 1.4261\n",
      "Epoch: 11/20, Batch: 650/1190, Loss: 0.9712\n",
      "Epoch: 11/20, Batch: 700/1190, Loss: 1.2075\n",
      "Epoch: 11/20, Batch: 750/1190, Loss: 0.9003\n",
      "Epoch: 11/20, Batch: 800/1190, Loss: 1.1390\n",
      "Epoch: 11/20, Batch: 850/1190, Loss: 0.9762\n",
      "Epoch: 11/20, Batch: 900/1190, Loss: 1.0443\n",
      "Epoch: 11/20, Batch: 950/1190, Loss: 0.9274\n",
      "Epoch: 11/20, Batch: 1000/1190, Loss: 1.5393\n",
      "Epoch: 11/20, Batch: 1050/1190, Loss: 0.9315\n",
      "Epoch: 11/20, Batch: 1100/1190, Loss: 1.4695\n",
      "Epoch: 11/20, Batch: 1150/1190, Loss: 0.9614\n",
      "Epoch: 11, Train Loss: 0.9849\n",
      "Epoch: 12/20, Batch: 0/1190, Loss: 0.9911\n",
      "Epoch: 12/20, Batch: 50/1190, Loss: 0.7808\n",
      "Epoch: 12/20, Batch: 100/1190, Loss: 0.9605\n",
      "Epoch: 12/20, Batch: 150/1190, Loss: 1.1860\n",
      "Epoch: 12/20, Batch: 200/1190, Loss: 0.8151\n",
      "Epoch: 12/20, Batch: 250/1190, Loss: 0.8805\n",
      "Epoch: 12/20, Batch: 300/1190, Loss: 0.8214\n",
      "Epoch: 12/20, Batch: 350/1190, Loss: 0.8292\n",
      "Epoch: 12/20, Batch: 400/1190, Loss: 0.8485\n",
      "Epoch: 12/20, Batch: 450/1190, Loss: 0.9128\n",
      "Epoch: 12/20, Batch: 500/1190, Loss: 1.0164\n",
      "Epoch: 12/20, Batch: 550/1190, Loss: 0.8662\n",
      "Epoch: 12/20, Batch: 600/1190, Loss: 0.7953\n",
      "Epoch: 12/20, Batch: 650/1190, Loss: 1.1132\n",
      "Epoch: 12/20, Batch: 700/1190, Loss: 0.8650\n",
      "Epoch: 12/20, Batch: 750/1190, Loss: 1.0388\n",
      "Epoch: 12/20, Batch: 800/1190, Loss: 1.0406\n",
      "Epoch: 12/20, Batch: 850/1190, Loss: 0.9585\n",
      "Epoch: 12/20, Batch: 900/1190, Loss: 1.1323\n",
      "Epoch: 12/20, Batch: 950/1190, Loss: 1.1467\n",
      "Epoch: 12/20, Batch: 1000/1190, Loss: 1.2132\n",
      "Epoch: 12/20, Batch: 1050/1190, Loss: 1.1316\n",
      "Epoch: 12/20, Batch: 1100/1190, Loss: 1.0311\n",
      "Epoch: 12/20, Batch: 1150/1190, Loss: 0.7751\n",
      "Epoch: 12, Train Loss: 0.9859\n",
      "Epoch: 13/20, Batch: 0/1190, Loss: 1.0146\n",
      "Epoch: 13/20, Batch: 50/1190, Loss: 0.8120\n",
      "Epoch: 13/20, Batch: 100/1190, Loss: 0.8999\n",
      "Epoch: 13/20, Batch: 150/1190, Loss: 1.2083\n",
      "Epoch: 13/20, Batch: 200/1190, Loss: 1.1392\n",
      "Epoch: 13/20, Batch: 250/1190, Loss: 1.4073\n",
      "Epoch: 13/20, Batch: 300/1190, Loss: 0.9441\n",
      "Epoch: 13/20, Batch: 350/1190, Loss: 0.7550\n",
      "Epoch: 13/20, Batch: 400/1190, Loss: 1.1880\n",
      "Epoch: 13/20, Batch: 450/1190, Loss: 0.7962\n",
      "Epoch: 13/20, Batch: 500/1190, Loss: 1.4115\n",
      "Epoch: 13/20, Batch: 550/1190, Loss: 0.9303\n",
      "Epoch: 13/20, Batch: 600/1190, Loss: 1.0074\n",
      "Epoch: 13/20, Batch: 650/1190, Loss: 0.8265\n",
      "Epoch: 13/20, Batch: 700/1190, Loss: 1.1579\n",
      "Epoch: 13/20, Batch: 750/1190, Loss: 1.0541\n",
      "Epoch: 13/20, Batch: 800/1190, Loss: 0.7166\n",
      "Epoch: 13/20, Batch: 850/1190, Loss: 0.8256\n",
      "Epoch: 13/20, Batch: 900/1190, Loss: 0.7776\n",
      "Epoch: 13/20, Batch: 950/1190, Loss: 1.0789\n",
      "Epoch: 13/20, Batch: 1000/1190, Loss: 1.2777\n",
      "Epoch: 13/20, Batch: 1050/1190, Loss: 1.0486\n",
      "Epoch: 13/20, Batch: 1100/1190, Loss: 0.8386\n",
      "Epoch: 13/20, Batch: 1150/1190, Loss: 1.0120\n",
      "Epoch: 13, Train Loss: 0.9843\n",
      "Epoch: 14/20, Batch: 0/1190, Loss: 0.8887\n",
      "Epoch: 14/20, Batch: 50/1190, Loss: 0.8505\n",
      "Epoch: 14/20, Batch: 100/1190, Loss: 1.0197\n",
      "Epoch: 14/20, Batch: 150/1190, Loss: 1.0126\n",
      "Epoch: 14/20, Batch: 200/1190, Loss: 1.4816\n",
      "Epoch: 14/20, Batch: 250/1190, Loss: 0.9679\n",
      "Epoch: 14/20, Batch: 300/1190, Loss: 0.7893\n",
      "Epoch: 14/20, Batch: 350/1190, Loss: 1.0556\n",
      "Epoch: 14/20, Batch: 400/1190, Loss: 0.7030\n",
      "Epoch: 14/20, Batch: 450/1190, Loss: 0.8180\n",
      "Epoch: 14/20, Batch: 500/1190, Loss: 0.9165\n",
      "Epoch: 14/20, Batch: 550/1190, Loss: 1.3121\n",
      "Epoch: 14/20, Batch: 600/1190, Loss: 0.9775\n",
      "Epoch: 14/20, Batch: 650/1190, Loss: 0.8211\n",
      "Epoch: 14/20, Batch: 700/1190, Loss: 1.1224\n",
      "Epoch: 14/20, Batch: 750/1190, Loss: 1.0479\n",
      "Epoch: 14/20, Batch: 800/1190, Loss: 1.0672\n",
      "Epoch: 14/20, Batch: 850/1190, Loss: 1.0677\n",
      "Epoch: 14/20, Batch: 900/1190, Loss: 1.0439\n",
      "Epoch: 14/20, Batch: 950/1190, Loss: 1.0860\n",
      "Epoch: 14/20, Batch: 1000/1190, Loss: 0.8221\n",
      "Epoch: 14/20, Batch: 1050/1190, Loss: 1.2390\n",
      "Epoch: 14/20, Batch: 1100/1190, Loss: 1.0240\n",
      "Epoch: 14/20, Batch: 1150/1190, Loss: 0.9475\n",
      "Epoch: 14, Train Loss: 0.9846\n",
      "Epoch: 15/20, Batch: 0/1190, Loss: 0.9589\n",
      "Epoch: 15/20, Batch: 50/1190, Loss: 1.3716\n",
      "Epoch: 15/20, Batch: 100/1190, Loss: 0.9408\n",
      "Epoch: 15/20, Batch: 150/1190, Loss: 1.2557\n",
      "Epoch: 15/20, Batch: 200/1190, Loss: 0.7200\n",
      "Epoch: 15/20, Batch: 250/1190, Loss: 0.9592\n",
      "Epoch: 15/20, Batch: 300/1190, Loss: 1.2074\n",
      "Epoch: 15/20, Batch: 350/1190, Loss: 1.2420\n",
      "Epoch: 15/20, Batch: 400/1190, Loss: 0.9535\n",
      "Epoch: 15/20, Batch: 450/1190, Loss: 0.8129\n",
      "Epoch: 15/20, Batch: 500/1190, Loss: 1.1620\n",
      "Epoch: 15/20, Batch: 550/1190, Loss: 0.6820\n",
      "Epoch: 15/20, Batch: 600/1190, Loss: 0.8802\n",
      "Epoch: 15/20, Batch: 650/1190, Loss: 1.1884\n",
      "Epoch: 15/20, Batch: 700/1190, Loss: 1.5101\n",
      "Epoch: 15/20, Batch: 750/1190, Loss: 0.7187\n",
      "Epoch: 15/20, Batch: 800/1190, Loss: 1.0002\n",
      "Epoch: 15/20, Batch: 850/1190, Loss: 1.0620\n",
      "Epoch: 15/20, Batch: 900/1190, Loss: 0.7333\n",
      "Epoch: 15/20, Batch: 950/1190, Loss: 0.9544\n",
      "Epoch: 15/20, Batch: 1000/1190, Loss: 0.8143\n",
      "Epoch: 15/20, Batch: 1050/1190, Loss: 0.9718\n",
      "Epoch: 15/20, Batch: 1100/1190, Loss: 1.0433\n",
      "Epoch: 15/20, Batch: 1150/1190, Loss: 1.1923\n",
      "Epoch: 15, Train Loss: 0.9839\n",
      "Epoch: 16/20, Batch: 0/1190, Loss: 1.1525\n",
      "Epoch: 16/20, Batch: 50/1190, Loss: 0.9546\n",
      "Epoch: 16/20, Batch: 100/1190, Loss: 1.0949\n",
      "Epoch: 16/20, Batch: 150/1190, Loss: 0.6763\n",
      "Epoch: 16/20, Batch: 200/1190, Loss: 0.9146\n",
      "Epoch: 16/20, Batch: 250/1190, Loss: 1.3724\n",
      "Epoch: 16/20, Batch: 300/1190, Loss: 1.1546\n",
      "Epoch: 16/20, Batch: 350/1190, Loss: 0.8245\n",
      "Epoch: 16/20, Batch: 400/1190, Loss: 0.9663\n",
      "Epoch: 16/20, Batch: 450/1190, Loss: 1.0940\n",
      "Epoch: 16/20, Batch: 500/1190, Loss: 1.0451\n",
      "Epoch: 16/20, Batch: 550/1190, Loss: 0.9332\n",
      "Epoch: 16/20, Batch: 600/1190, Loss: 0.9630\n",
      "Epoch: 16/20, Batch: 650/1190, Loss: 0.7551\n",
      "Epoch: 16/20, Batch: 700/1190, Loss: 0.9341\n",
      "Epoch: 16/20, Batch: 750/1190, Loss: 1.1144\n",
      "Epoch: 16/20, Batch: 800/1190, Loss: 0.6909\n",
      "Epoch: 16/20, Batch: 850/1190, Loss: 0.7852\n",
      "Epoch: 16/20, Batch: 900/1190, Loss: 1.0509\n",
      "Epoch: 16/20, Batch: 950/1190, Loss: 0.6605\n",
      "Epoch: 16/20, Batch: 1000/1190, Loss: 0.8955\n",
      "Epoch: 16/20, Batch: 1050/1190, Loss: 1.1661\n",
      "Epoch: 16/20, Batch: 1100/1190, Loss: 0.7989\n",
      "Epoch: 16/20, Batch: 1150/1190, Loss: 0.9342\n",
      "Epoch: 16, Train Loss: 0.9815\n",
      "Epoch: 17/20, Batch: 0/1190, Loss: 0.9234\n",
      "Epoch: 17/20, Batch: 50/1190, Loss: 0.8678\n",
      "Epoch: 17/20, Batch: 100/1190, Loss: 1.0221\n",
      "Epoch: 17/20, Batch: 150/1190, Loss: 1.3114\n",
      "Epoch: 17/20, Batch: 200/1190, Loss: 1.0371\n",
      "Epoch: 17/20, Batch: 250/1190, Loss: 1.0093\n",
      "Epoch: 17/20, Batch: 300/1190, Loss: 0.8211\n",
      "Epoch: 17/20, Batch: 350/1190, Loss: 1.3923\n",
      "Epoch: 17/20, Batch: 400/1190, Loss: 0.9973\n",
      "Epoch: 17/20, Batch: 450/1190, Loss: 1.2057\n",
      "Epoch: 17/20, Batch: 500/1190, Loss: 1.1427\n",
      "Epoch: 17/20, Batch: 550/1190, Loss: 1.4175\n",
      "Epoch: 17/20, Batch: 600/1190, Loss: 0.8679\n",
      "Epoch: 17/20, Batch: 650/1190, Loss: 1.1154\n",
      "Epoch: 17/20, Batch: 700/1190, Loss: 1.0327\n",
      "Epoch: 17/20, Batch: 750/1190, Loss: 1.2184\n",
      "Epoch: 17/20, Batch: 800/1190, Loss: 1.1909\n",
      "Epoch: 17/20, Batch: 850/1190, Loss: 1.2785\n",
      "Epoch: 17/20, Batch: 900/1190, Loss: 0.9860\n",
      "Epoch: 17/20, Batch: 950/1190, Loss: 0.9438\n",
      "Epoch: 17/20, Batch: 1000/1190, Loss: 0.8628\n",
      "Epoch: 17/20, Batch: 1050/1190, Loss: 0.7468\n",
      "Epoch: 17/20, Batch: 1100/1190, Loss: 0.9406\n",
      "Epoch: 17/20, Batch: 1150/1190, Loss: 1.1059\n",
      "Epoch: 17, Train Loss: 0.9827\n",
      "Epoch: 18/20, Batch: 0/1190, Loss: 0.7551\n",
      "Epoch: 18/20, Batch: 50/1190, Loss: 1.0675\n",
      "Epoch: 18/20, Batch: 100/1190, Loss: 0.7318\n",
      "Epoch: 18/20, Batch: 150/1190, Loss: 0.7862\n",
      "Epoch: 18/20, Batch: 200/1190, Loss: 1.2611\n",
      "Epoch: 18/20, Batch: 250/1190, Loss: 1.0671\n",
      "Epoch: 18/20, Batch: 300/1190, Loss: 0.9522\n",
      "Epoch: 18/20, Batch: 350/1190, Loss: 0.8992\n",
      "Epoch: 18/20, Batch: 400/1190, Loss: 0.9897\n",
      "Epoch: 18/20, Batch: 450/1190, Loss: 1.3603\n",
      "Epoch: 18/20, Batch: 500/1190, Loss: 1.1874\n",
      "Epoch: 18/20, Batch: 550/1190, Loss: 0.7664\n",
      "Epoch: 18/20, Batch: 600/1190, Loss: 0.8618\n",
      "Epoch: 18/20, Batch: 650/1190, Loss: 0.9647\n",
      "Epoch: 18/20, Batch: 700/1190, Loss: 0.9127\n",
      "Epoch: 18/20, Batch: 750/1190, Loss: 0.9210\n",
      "Epoch: 18/20, Batch: 800/1190, Loss: 0.8678\n",
      "Epoch: 18/20, Batch: 850/1190, Loss: 0.8413\n",
      "Epoch: 18/20, Batch: 900/1190, Loss: 1.2359\n",
      "Epoch: 18/20, Batch: 950/1190, Loss: 0.9238\n",
      "Epoch: 18/20, Batch: 1000/1190, Loss: 0.7996\n",
      "Epoch: 18/20, Batch: 1050/1190, Loss: 1.0578\n",
      "Epoch: 18/20, Batch: 1100/1190, Loss: 1.0450\n",
      "Epoch: 18/20, Batch: 1150/1190, Loss: 0.9144\n",
      "Epoch: 18, Train Loss: 0.9811\n",
      "Epoch: 19/20, Batch: 0/1190, Loss: 1.0943\n",
      "Epoch: 19/20, Batch: 50/1190, Loss: 0.9956\n",
      "Epoch: 19/20, Batch: 100/1190, Loss: 0.9300\n",
      "Epoch: 19/20, Batch: 150/1190, Loss: 0.7672\n",
      "Epoch: 19/20, Batch: 200/1190, Loss: 0.8583\n",
      "Epoch: 19/20, Batch: 250/1190, Loss: 1.2747\n",
      "Epoch: 19/20, Batch: 300/1190, Loss: 0.9369\n",
      "Epoch: 19/20, Batch: 350/1190, Loss: 0.9782\n",
      "Epoch: 19/20, Batch: 400/1190, Loss: 0.8576\n",
      "Epoch: 19/20, Batch: 450/1190, Loss: 0.8702\n",
      "Epoch: 19/20, Batch: 500/1190, Loss: 0.8496\n",
      "Epoch: 19/20, Batch: 550/1190, Loss: 0.9361\n",
      "Epoch: 19/20, Batch: 600/1190, Loss: 0.8215\n",
      "Epoch: 19/20, Batch: 650/1190, Loss: 1.0966\n",
      "Epoch: 19/20, Batch: 700/1190, Loss: 1.1719\n",
      "Epoch: 19/20, Batch: 750/1190, Loss: 0.9186\n",
      "Epoch: 19/20, Batch: 800/1190, Loss: 1.3957\n",
      "Epoch: 19/20, Batch: 850/1190, Loss: 0.8648\n",
      "Epoch: 19/20, Batch: 900/1190, Loss: 0.8059\n",
      "Epoch: 19/20, Batch: 950/1190, Loss: 0.9402\n",
      "Epoch: 19/20, Batch: 1000/1190, Loss: 1.0357\n",
      "Epoch: 19/20, Batch: 1050/1190, Loss: 0.8624\n",
      "Epoch: 19/20, Batch: 1100/1190, Loss: 1.0085\n",
      "Epoch: 19/20, Batch: 1150/1190, Loss: 0.7995\n",
      "Epoch: 19, Train Loss: 0.9806\n",
      "Epoch: 20/20, Batch: 0/1190, Loss: 1.0762\n",
      "Epoch: 20/20, Batch: 50/1190, Loss: 1.4125\n",
      "Epoch: 20/20, Batch: 100/1190, Loss: 1.3417\n",
      "Epoch: 20/20, Batch: 150/1190, Loss: 0.8609\n",
      "Epoch: 20/20, Batch: 200/1190, Loss: 0.9742\n",
      "Epoch: 20/20, Batch: 250/1190, Loss: 0.9345\n",
      "Epoch: 20/20, Batch: 300/1190, Loss: 1.1492\n",
      "Epoch: 20/20, Batch: 350/1190, Loss: 0.7859\n",
      "Epoch: 20/20, Batch: 400/1190, Loss: 1.0883\n",
      "Epoch: 20/20, Batch: 450/1190, Loss: 0.8716\n",
      "Epoch: 20/20, Batch: 500/1190, Loss: 1.0570\n",
      "Epoch: 20/20, Batch: 550/1190, Loss: 1.0682\n",
      "Epoch: 20/20, Batch: 600/1190, Loss: 1.0144\n",
      "Epoch: 20/20, Batch: 650/1190, Loss: 1.5057\n",
      "Epoch: 20/20, Batch: 700/1190, Loss: 0.8786\n",
      "Epoch: 20/20, Batch: 750/1190, Loss: 1.1093\n",
      "Epoch: 20/20, Batch: 800/1190, Loss: 0.9419\n",
      "Epoch: 20/20, Batch: 850/1190, Loss: 1.0464\n",
      "Epoch: 20/20, Batch: 900/1190, Loss: 1.2048\n",
      "Epoch: 20/20, Batch: 950/1190, Loss: 1.6624\n",
      "Epoch: 20/20, Batch: 1000/1190, Loss: 0.8473\n",
      "Epoch: 20/20, Batch: 1050/1190, Loss: 0.8930\n",
      "Epoch: 20/20, Batch: 1100/1190, Loss: 0.8800\n",
      "Epoch: 20/20, Batch: 1150/1190, Loss: 0.8403\n",
      "Epoch: 20, Train Loss: 0.9830\n",
      "Training finished!\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42, stratify=y)\n",
    "\n",
    "std = StandardScaler()\n",
    "X_train_scaled = std.fit_transform(X_train)\n",
    "X_test_scaled = std.transform(X_test)\n",
    "\n",
    "pca = PCA(n_components=14)\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "train_dataset = HeartDiseaseDataset(X_train_pca, y_train)\n",
    "test_dataset = HeartDiseaseDataset(X_test_pca, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1024, shuffle=False)\n",
    "\n",
    "input_features = X_train_pca.shape[1]  \n",
    "\n",
    "weighted_model = HeartDiseaseMLPClassifier(input_size=input_features, class_frequencies=class_frequencies).to(device)\n",
    "\n",
    "optimizer = optim.Adam(weighted_model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)\n",
    "\n",
    "num_epochs = 20\n",
    "weighted_train_losses = []\n",
    "\n",
    "print(\"\\nStarting training...\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    weighted_model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = weighted_model(data)\n",
    "    \n",
    "        target = target.float()\n",
    "        output = output.float()    \n",
    "        loss = weighted_model.get_weighted_loss(output, target)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if batch_idx % 50 == 0:\n",
    "            print(f'Epoch: {epoch+1}/{num_epochs}, Batch: {batch_idx}/{len(train_loader)}, Loss: {loss.item():.4f}')\n",
    "    \n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "    weighted_train_losses.append(avg_train_loss)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1}, Train Loss: {avg_train_loss:.4f}')\n",
    "\n",
    "print('Training finished!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report (Weighted):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.69      0.81     91915\n",
      "         1.0       0.22      0.85      0.35      9557\n",
      "\n",
      "    accuracy                           0.70    101472\n",
      "   macro avg       0.60      0.77      0.58    101472\n",
      "weighted avg       0.91      0.70      0.76    101472\n",
      "\n",
      "\n",
      "Confusion Matrix (Weighted):\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "weighted_model.eval()\n",
    "y_true_weighted = []\n",
    "y_pred_weighted = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        output = weighted_model(data)\n",
    "        \n",
    "        y_true_weighted.extend(target.cpu().numpy())\n",
    "        y_pred_weighted.extend((output > 0.5).cpu().numpy())\n",
    "\n",
    "y_true_weighted = np.array(y_true_weighted)\n",
    "y_pred_weighted = np.array(y_pred_weighted)\n",
    "\n",
    "print(\"\\nClassification Report (Weighted):\")\n",
    "print(classification_report(y_true_weighted, y_pred_weighted))\n",
    "print(\"\\nConfusion Matrix (Weighted):\")\n",
    "cm_weighted = confusion_matrix(y_true_weighted, y_pred_weighted)\n",
    "sns.heatmap(cm_weighted, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix (Weighted)')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
